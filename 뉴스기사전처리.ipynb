{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtJ-cjhpy7Ve"
      },
      "source": [
        "# 패키지 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967
        },
        "id": "TPmcqrKxygrE",
        "outputId": "ac08c2da-7f0b-473c-f141-7b01a325e9ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
            "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /tmp/pip-req-build-6m_zdgu9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-req-build-6m_zdgu9\n",
            "  Resolved https://github.com/haven-jeon/PyKoSpacing.git to commit b32a889cbd10b006d2f4aba118f0cd5b677e2979\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.16.2 in /usr/local/lib/python3.10/dist-packages (from pykospacing==0.5) (2.17.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from pykospacing==0.5) (3.11.0)\n",
            "Collecting argparse>=1.1.0 (from pykospacing==0.5)\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py>=3.10.0->pykospacing==0.5) (1.26.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.16.2->pykospacing==0.5) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.16.2->pykospacing==0.5) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.16.2->pykospacing==0.5) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.16.2->pykospacing==0.5) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.16.2->pykospacing==0.5) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16.2->pykospacing==0.5) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16.2->pykospacing==0.5) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16.2->pykospacing==0.5) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16.2->pykospacing==0.5) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.16.2->pykospacing==0.5) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.16.2->pykospacing==0.5) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.16.2->pykospacing==0.5) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow>=2.16.2->pykospacing==0.5) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.16.2->pykospacing==0.5) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.16.2->pykospacing==0.5) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow>=2.16.2->pykospacing==0.5) (0.1.2)\n",
            "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              },
              "id": "b2b1aeda14ae4b95929e178a7020403f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#띄어쓰기\n",
        "# 이거 돌려서 오류나면 본인 컴에 GIT을 설치해야함\n",
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPCDYdxGzGOj",
        "outputId": "e05a8d9a-e8c5-4dcb-c49e-72f9cfa0e7af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.10/dist-packages (6.7.7)\n",
            "Requirement already satisfied: editdistpy>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from symspellpy) (0.1.4)\n"
          ]
        }
      ],
      "source": [
        "#오타교정\n",
        "!pip install symspellpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-iAkOJ-0eUp",
        "outputId": "bcd53025-bbfb-4054-cad0-34f221bbf121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hanziconv in /usr/local/lib/python3.10/dist-packages (0.3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install hanziconv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckfqg3p6ddn7",
        "outputId": "23fddc8d-a1ee-4ee3-b181-b3c59035201b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hanja\n",
            "  Using cached hanja-0.15.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting pyyaml==6.0.1 (from hanja)\n",
            "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from hanja) (7.4.4)\n",
            "Collecting pytest-cov (from hanja)\n",
            "  Using cached pytest_cov-5.0.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting coveralls (from hanja)\n",
            "  Using cached coveralls-4.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting coverage!=6.0.*,!=6.1,!=6.1.1,<8.0,>=5.0 (from coverage[toml]!=6.0.*,!=6.1,!=6.1.1,<8.0,>=5.0->coveralls->hanja)\n",
            "  Using cached coverage-7.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Collecting docopt<0.7.0,>=0.6.1 (from coveralls->hanja)\n",
            "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from coveralls->hanja) (2.32.3)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->hanja) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->hanja) (24.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->hanja) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->hanja) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->hanja) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=1.0.0->coveralls->hanja) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=1.0.0->coveralls->hanja) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=1.0.0->coveralls->hanja) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=1.0.0->coveralls->hanja) (2024.7.4)\n",
            "Using cached hanja-0.15.1-py3-none-any.whl (124 kB)\n",
            "Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coveralls-4.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading pytest_cov-5.0.0-py3-none-any.whl (21 kB)\n",
            "Downloading coverage-7.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (234 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.7/234.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=166683d8204aebc6ce02c295ed8d2b0731861e235bbb76a05e250a2f1d7ca1d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, pyyaml, coverage, pytest-cov, coveralls, hanja\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "Successfully installed coverage-7.6.1 coveralls-4.0.1 docopt-0.6.2 hanja-0.15.1 pytest-cov-5.0.0 pyyaml-6.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install hanja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5pIuImZk0Uo",
        "outputId": "b7bd74ef-0870-4cb3-89c2-af7f0563a201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/ssut/py-hanspell.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvpD3IXrQtrZ",
        "outputId": "ec721110-df14-47b7-e52b-4468f5131bdd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/ssut/py-hanspell.git\n",
            "  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-_zxtyqzs\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-_zxtyqzs\n",
            "  Resolved https://github.com/ssut/py-hanspell.git to commit fdc6ca50c19f1c85971437a072d89d4e5ce024b8\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from py-hanspell==1.1) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->py-hanspell==1.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->py-hanspell==1.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->py-hanspell==1.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->py-hanspell==1.1) (2024.7.4)\n",
            "Building wheels for collected packages: py-hanspell\n",
            "  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-hanspell: filename=py_hanspell-1.1-py3-none-any.whl size=4810 sha256=f1369d8bd127e965ae49ed5d372176f8d2d3d94c51333422b9f52770184926a1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yo7jnyuc/wheels/2e/43/cc/753c9e1d91affb9ea40e186cea5654fb9231deb454da6724e5\n",
            "Successfully built py-hanspell\n",
            "Installing collected packages: py-hanspell\n",
            "Successfully installed py-hanspell-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uKC8-5Z6ygCm"
      },
      "outputs": [],
      "source": [
        "#띄어쓰기\n",
        "from pykospacing import Spacing\n",
        "#오타교정\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "from konlpy.tag import Okt\n",
        "import hanja\n",
        "from hanspell import spell_checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WqBlk8XpO6Bg"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak42gQ_6-ZRu"
      },
      "source": [
        "### 나중에 20개년 파일 하나의 csv로 합칠 때 사용할 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK_sG5j75BQq"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import glob\n",
        "\n",
        "# # 파일 경로 패턴 정의\n",
        "# file_pattern = \"*_한국경제.csv\"\n",
        "\n",
        "# # 파일 목록 가져오기\n",
        "# file_list = glob.glob(file_pattern)\n",
        "\n",
        "# # 파일 읽기 및 데이터프레임 리스트 생성\n",
        "# df_list = [pd.read_csv(file) for file in file_list]\n",
        "\n",
        "# # 데이터프레임 수직으로 결합\n",
        "# combined_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# # 결과를 새로운 CSV 파일로 저장\n",
        "# combined_df.to_csv('combined_한국경제.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "# print(\"모든 파일이 성공적으로 결합되었습니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNivPOYNCPFb"
      },
      "source": [
        "### 합치지 않고 csv파일들을 리스트로 모아서 관리 (아근데 연도별로 분담해서 돌려야하면 필요없음)\n",
        "\n",
        "\n",
        "전처리 할 때마다 계속해서 for문으로 연도별 데이터프레임을 불러와야하기 때문에\n",
        "\n",
        "한국경제와 매일경제 리스트를 만들었음.\n",
        "\n",
        "인덱스 0 에는 2005년 데이터 프레임부터 순서대로 들어가있음\n",
        "\n",
        "* df_list_ky : 한국경제 데이터프레임 리스트\n",
        "* df_list_mye : 매일경제 데이터프레임 리스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asGWUNweC_DU"
      },
      "outputs": [],
      "source": [
        "# # 연도 범위 설정\n",
        "# start_year = 2005\n",
        "# end_year = 2024\n",
        "\n",
        "# # 데이터프레임 리스트 생성\n",
        "# df_list_ky = []  # 한국경제 데이터프레임 리스트\n",
        "# df_list_mye = []  # 매일경제 데이터프레임 리스트\n",
        "\n",
        "# # 한국경제와 매일경제 파일 처리\n",
        "# for year in range(start_year, end_year + 1):\n",
        "#     for source in ['한국경제', '매일경제']:\n",
        "#         filename = f\"{year}_{source}.csv\"\n",
        "\n",
        "#         if os.path.exists(filename):\n",
        "#             try:\n",
        "#                 # CSV 파일을 데이터프레임으로 읽기\n",
        "#                 df = pd.read_csv(filename)\n",
        "\n",
        "#                 # null 값 및 중복 제거\n",
        "#                 df.dropna(inplace=True)\n",
        "#                 df.drop_duplicates(inplace=True)\n",
        "\n",
        "#                 # 데이터프레임 리스트에 추가\n",
        "#                 if source == '한국경제':\n",
        "#                     df_list_ky.append(df)\n",
        "#                 elif source == '매일경제':\n",
        "#                     df_list_mye.append(df)\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"{filename} 파일 읽기 중 오류 발생: {e}\")\n",
        "#         else:\n",
        "#             print(f\"파일을 찾을 수 없습니다: {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxrHJS-5KVLc"
      },
      "source": [
        "\n",
        "# 전처리\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lJH4bzn1SUJM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "#이때 한글 불용어 사용하려면 korean파일이 있어야함. 노션 참고해서 생성하시오."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bBC2DHV7tNY",
        "outputId": "017e6d44-adff-49b9-cfb0-27f4979a1aaa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #지연) 구글 코랩으로 작성해서 직접 생성한 korean파일을 옮기는 코드가 필요했음.. 주피터로 하면 필요없을듯!\n",
        "# import shutil\n",
        "\n",
        "# # 파일 경로\n",
        "# src_path = '/content/korean'\n",
        "# dest_path = '/root/nltk_data/corpora/stopwords'\n",
        "\n",
        "# # 파일 이동\n",
        "# shutil.move(src_path, dest_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4giWQjA4c1t_",
        "outputId": "740d8693-d8cc-4104-9210-3093e8149c4e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/nltk_data/corpora/stopwords/korean'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SGCjbCgCJsI4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 데이터 클렌징 함수 정의\n",
        "def clean_text(text):\n",
        "\n",
        "    # 1. 한자를 한글로 변환\n",
        "    text = re.sub(r'\\([^()]*\\)', '', text)  # 괄호 안의 한자는 삭제\n",
        "    text = hanja.translate(text, 'substitution')  # 그 외 한자는 한글로 변환\n",
        "\n",
        "    # 2. 제거할 단어 및 패턴 목록\n",
        "    patterns = [\n",
        "        r'삼성홀',          # '삼성홀' 패턴\n",
        "        r'\\[포토\\]',        # '[포토]' 패턴\n",
        "        r'\\[기자\\]',        # '[기자]' 패턴\n",
        "        r'\\[저작권\\]',      # '[저작권]' 패턴\n",
        "        r'\\[퀴즈\\]',        # '[퀴즈]' 패턴\n",
        "        r'<표>'            # '<표>' 패턴\n",
        "        ]\n",
        "\n",
        "    # 단어 및 패턴을 제거하는 함수\n",
        "    def remove_patterns(text):\n",
        "      for pattern in patterns:\n",
        "        text = re.sub(pattern, '', text)\n",
        "      return text.strip()  # 텍스트의 양쪽 공백 제거\n",
        "\n",
        "    # 패턴 제거\n",
        "    text = remove_patterns(text)\n",
        "\n",
        "    # 3. 특수 문자 제거\n",
        "    text = re.sub(r'[^가-힣0-9\\s]', '', text)  # 한글과 숫자, 공백만 남김\n",
        "\n",
        "    # 4. 띄어쓰기 교정\n",
        "    spacing = Spacing()\n",
        "    text = spacing(text)\n",
        "\n",
        "    # 5. 오타 수정\n",
        "\n",
        "\n",
        "    # 6. 불용어 제거\n",
        "    stop_words = set(stopwords.words('korean'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_ZxMc0nnf1w"
      },
      "source": [
        "\n",
        "\n",
        "### 하나의 csv 파일만 돌린다고 가정 (for문 x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "O3Oq8qEAngP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53550032-19c4-4f7d-cbf2-96958ebb5256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                body  \\\n",
            "0  포스코는 신년 1월 1일에 '혁신기획실'을 신설했다.혁신기획실에는 혁신전략팀,6시그...   \n",
            "1  '벤처부활은 우리가 이끈다.'부품·소재관련 사업을 하는 벤처기업들이 닭띠해의 '뜨는...   \n",
            "2  2005년 통신시장은 △위성DMB(이동멀티미디어방송)와 지상파DMB 등 이른바 '휴...   \n",
            "3  올해 휴대폰 시장의 트렌드는 다양한 디지털기기의 기능을 하나로 묶은 '융ㆍ복합화의 ...   \n",
            "4  기술적 분석가들은 새해 증시에 낙관론을 펴고있다.지난해 폐장주가가 급등하며 장기 골...   \n",
            "\n",
            "                                        cleaned_body  \n",
            "0  포스코는 신년 1월 1일에 혁신기획실을 신설했다 혁신기획실에는 혁신전략팀 6시그마팀...  \n",
            "1  벤처부활은 우리가 끈다 부품 소재 관련 사업을 하는 벤처기업들이 닭띠 해의 뜨는 별...  \n",
            "2  2005년 통신시장은 위성 지상파 이른바 휴대 방송의 등장으로 인한 통신방송 융합 ...  \n",
            "3  올해 휴대폰 시장의 트렌드는 다양한 디지털 기기의 기능을 하나로 묶은 융복합화의 가...  \n",
            "4  기술적 분석가들은 새해 증시에 낙관론을 펴고 지난해 폐장 주가가 급등하며 장기 골든...  \n"
          ]
        }
      ],
      "source": [
        "# CSV 파일을 데이터프레임으로 불러오기\n",
        "df = pd.read_csv('2005_한국경제.csv')\n",
        "\n",
        "# 데이터 정제 ) null값과 중복 제거\n",
        "df.dropna(subset=['body'], inplace=True)\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# 'body' 컬럼에 클렌징 함수 적용\n",
        "df['cleaned_body'] = df['body'].apply(clean_text)\n",
        "df['cleaned_title'] = df['title'].apply(clean_text)\n",
        "\n",
        "# 결과 확인\n",
        "print(df[['body', 'cleaned_body']].head())\n",
        "\n",
        "# 클렌징된 데이터프레임 저장 (원하는 파일명으로 변경)\n",
        "df.to_csv('cleaned_2005_한국경제.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTLCbXzqnzbM"
      },
      "source": [
        "### 여러개 연도 for문 돌려서 전처리하고 싶을 때 (한국경제랑 매일경제는 직접 설정해서)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Kmp7zANsjckD"
      },
      "outputs": [],
      "source": [
        "# 파일 경로 및 범위 설정\n",
        "start_year = 2005\n",
        "end_year = 2006\n",
        "\n",
        "# 모든 연도의 CSV 파일을 처리\n",
        "for year in range(start_year, end_year + 1):\n",
        "    filename = f'/{year}_한국경제.csv'\n",
        "\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"Processing file: {filename}\")\n",
        "\n",
        "        # CSV 파일을 데이터프레임으로 불러오기\n",
        "        df = pd.read_csv(filename)\n",
        "\n",
        "        df.dropna(subset=['body'], inplace=True)\n",
        "        df.drop_duplicates(inplace=True)\n",
        "\n",
        "        # 'body'와 'title' 컬럼에 클렌징 함수 적용\n",
        "        df['cleaned_body'] = df['body'].apply(clean_text)\n",
        "        df['cleaned_title'] = df['title'].apply(clean_text)\n",
        "\n",
        "        # 클렌징된 데이터프레임 저장\n",
        "        output_filename = f'cleaned_{year}_한국경제.csv'\n",
        "        df.to_csv(output_filename, index=False)\n",
        "\n",
        "        print(f\"Saved cleaned file: {output_filename}\")\n",
        "    else:\n",
        "        print(f\"File not found: {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W2tRPqcogn3"
      },
      "source": [
        "여기까지, 연도마다 전처리했다면 전처리 완료된 csv파일을 새로 저장해서 모델에 사용하기!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJFR3U9Mbv_V"
      },
      "source": [
        "# 혹시, BERT 모델에 필요할까봐 해둔 전처리\n",
        "\n",
        "토크나이저, 최대 길이 맞추기 & padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn6zflCyVGPD",
        "outputId": "f3e71aae-4f18-4b70-dfe8-72903528e1cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# BERT 토크나이저와 모델 로드\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def preprocess_text(texts, max_length=512):\n",
        "    # 텍스트를 토큰화하고 길이를 맞춤\n",
        "    encoding = tokenizer(texts, max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "    return encoding\n",
        "\n",
        "# #간단한 bert 모델링\n",
        "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# def predict(texts):\n",
        "#     # 모델을 평가 모드로 설정\n",
        "#     model.eval()\n",
        "\n",
        "#     # 입력 텍스트를 전처리\n",
        "#     encoding = preprocess_text(texts)\n",
        "\n",
        "#     # `encoding`이 딕셔너리일 때 `input_ids`와 `attention_mask`를 올바르게 추출\n",
        "#     input_ids = encoding['input_ids']\n",
        "#     attention_mask = encoding['attention_mask']\n",
        "\n",
        "#     # 예측 수행\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         logits = outputs.logits\n",
        "\n",
        "#     # 예측 결과\n",
        "#     predictions = torch.argmax(logits, dim=1)\n",
        "#     return predictions\n",
        "\n",
        "# predict(df['body'][0:2].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaTqVgtJpCZv"
      },
      "source": [
        "# 전체 데이터프레임으로 EDA해보려했지만 넘나리 대용량이라 시간상 실패.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "e6Gs7kQ6pHsF"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv('/combined_매일경제.csv')\n",
        "df2 = pd.read_csv('/combined_한국경제.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oJPMCEmrUEt",
        "outputId": "172976c5-71d6-4fdb-db4f-bd13050caf85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 0s (25.9 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 123594 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "8kZz98f2UH3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a57fb9ae-2dd4-4820-a9ca-2a9e9ffb98ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-nanum is already the newest version (20200506-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ],
      "source": [
        "# (1) 나눔폰트 설치 하기  =====================================\n",
        "# sudo : 운영자 권한으로 명령어를 실행하라는 의미\n",
        "# apt-get (Advanced Packaging Tool) : 패키지 관리 tool\n",
        "\n",
        "\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "\n",
        "# (2) 런타임 다시시작!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Lspnmm_UUH3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0511522-179f-40c0-a2d9-e78735be52a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "◎ matplotlib version :  3.7.1\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/share/fonts/truetype/nanum/NanumSquareB.ttf',\n",
              " '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\n",
              " '/usr/share/fonts/truetype/nanum/NanumSquareRoundR.ttf',\n",
              " '/usr/share/fonts/truetype/nanum/NanumGothic.ttf',\n",
              " '/usr/share/fonts/truetype/nanum/NanumGothicCodingBold.ttf',\n",
              " '/usr/share/fonts/truetype/nanum/NanumSquareR.ttf',\n",
              " '/usr/share/fonts/truetype/nanum/NanumSquareRoundB.ttf',\n",
              " '/usr/share/fonts/truetype/nanum/NanumGothicCoding.ttf',\n",
              " '/usr/share/fonts/truetype/nanum/NanumMyeongjoBold.ttf',\n",
              " '/usr/share/fonts/truetype/nanum/NanumGothicBold.ttf',\n",
              " '/usr/share/fonts/truetype/nanum/NanumBarunGothicBold.ttf',\n",
              " '/usr/share/fonts/truetype/nanum/NanumMyeongjo.ttf']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# (3) 나눔폰트 설치 확인 하기  ==================================\n",
        "\n",
        "import  matplotlib\n",
        "import  matplotlib.font_manager  as fm\n",
        "import  matplotlib.pyplot  as plt\n",
        "\n",
        "print('◎ matplotlib version : ', matplotlib.__version__)\n",
        "print()\n",
        "\n",
        "sys_font  = fm.findSystemFonts ( )\n",
        "[ font  for  font  in  sys_font  if  \"Nanum\"  in font ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ykbw6xlnUH3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85e893cd-0def-49f7-b38c-de5508ec6269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "◎ 폰트 이름 :  NanumGothic\n"
          ]
        }
      ],
      "source": [
        "# (4) 나눔폰트 설정 하기  ==================================\n",
        "font_path = \"/usr/share/fonts/truetype/nanum/NanumGothicBold.ttf\"\n",
        "\n",
        "font_name  = fm.FontProperties(fname=font_path, size=12).get_name( )\n",
        "\n",
        "print(\"◎ 폰트 이름 : \",font_name)\n",
        "\n",
        "plt.rc(\"font\", family= font_name)       # Nanum 폰트 세팅\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# np.nan 값을 빈 문자열로 대체\n",
        "df1['body'] = df1['body'].fillna('')"
      ],
      "metadata": {
        "id": "EpIsqkyAhHoG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEyiRPyGqNwW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f73162df-4c60-47fa-c8bf-759a0960846c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['가서', '같은', '것과', '결과에', '결론을', '관계가', '관련이', '그런', '그럼에도', '그렇게', '그에', '그치지', '김에', '까닭에', '낫다', '년도', '논하지', '누가', '다시', '달려', '대로', '대해', '되는', '되다', '되어', '들면', '들자면', '듯하다', '따르는', '따름이다', '따지지', '때가', '만은', '만이', '많은', '말하면', '말할것도', '몰라도', '몰랏다', '못하다', '미치다', '바꾸어서', '바꿔', '밖에', '방면으로', '보면', '보아', '부류의', '비길수', '비추어', '뿐만', '사람들', '상대적으로', '생각이다', '서술한바와같이', '쓰여', '아니다', '아니라', '안다', '안된다', '않고', '않기', '않는다면', '않다', '않다면', '않도록', '않으면', '알겠는가', '어쩔수', '없고', '없다', '예를', '외에', '요만한', '우에', '위에서', '이렇게', '이로', '이르다', '이와', '이유는', '인하여', '임에', '점에서', '정도에', '정도의', '종합한것과같이', '주저하지', '줄은', '지경이다', '틀림없다', '편이', '하고', '하기', '하기만', '하는', '하는것만', '하는것이', '하다', '하면', '하지', '한하다', '할수록', '함으로써', '해도', '해서는', '형식으로', '힘이'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# CountVectorizer를 사용하여 단어의 빈도를 계산합니다.\n",
        "vectorizer = CountVectorizer(stop_words=stopwords.words('korean'))\n",
        "\n",
        "X = vectorizer.fit_transform(df1['body'])\n",
        "\n",
        "# 단어와 빈도수를 DataFrame으로 변환합니다.\n",
        "word_freq = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()).sum().sort_values(ascending=False)\n",
        "print(word_freq.head(10))\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "# 가장 많이 등장하는 단어로 WordCloud를 생성합니다.\n",
        "wordcloud = WordCloud(font_path=font_path, width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# np.nan 값을 빈 문자열로 대체\n",
        "df2['body'] = df2['body'].fillna('')"
      ],
      "metadata": {
        "id": "Pi1v2eNfhZDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLHLzLKEhZDF"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# CountVectorizer를 사용하여 단어의 빈도를 계산합니다.\n",
        "vectorizer = CountVectorizer(stop_words=stopwords.words('korean'))\n",
        "\n",
        "X = vectorizer.fit_transform(df2['body'])\n",
        "\n",
        "# 단어와 빈도수를 DataFrame으로 변환합니다.\n",
        "word_freq = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()).sum().sort_values(ascending=False)\n",
        "print(word_freq.head(10))\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "# 가장 많이 등장하는 단어로 WordCloud를 생성합니다.\n",
        "wordcloud = WordCloud(font_path=font_path, width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XTLCbXzqnzbM",
        "CJFR3U9Mbv_V",
        "RaTqVgtJpCZv"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}