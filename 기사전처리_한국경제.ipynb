{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfxuT5WPcnSN"
   },
   "source": [
    "# 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YK6YnrjOcnTn",
    "outputId": "1f888b89-76e3-40bc-ad55-8a72b9fdf198"
   },
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치\n",
    "!pip install hanja   # 한자를 한글로 변환하기 위한 패키지\n",
    "!pip install nltk   # 불용어 제거를 위한 패키지\n",
    "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git   # 띄어쓰기 바로 잡기 위한 패키지\n",
    "!pip install git+https://github.com/ssut/py-hanspell.git   # 오타 및 맞춤법을 수정해주는 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqMAXtbhcnT2"
   },
   "source": [
    "# 라이브러리 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1QuUYltjcnT6"
   },
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 호출\n",
    "import pandas as pd\n",
    "import re\n",
    "import hanja\n",
    "from hanspell import spell_checker\n",
    "from nltk.corpus import stopwords\n",
    "from pykospacing import Spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPMY2Hu8cnT8"
   },
   "source": [
    "# 불용어 목록 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rrfTAhDhcnT9",
    "outputId": "07db0058-509f-47a2-a6c9-ad21422165f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gustj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK 불용어 다운로드\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# stopwords 폴더 안에 korean 불용어 파일 만들고 노션에 있는거 복붙해서 넣기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToexR9wWcnT-"
   },
   "source": [
    "# 코랩 사용할 경우 파일 옮기는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FPN8LmlrcnUE",
    "outputId": "4430bfb1-7408-49dc-a0b5-4778ebc78702"
   },
   "outputs": [],
   "source": [
    "# 주피터가 아니라 코랩 쓰는 경우에만 참고\n",
    "#지연) 구글 코랩으로 작성해서 직접 생성한 korean파일을 옮기는 코드가 필요했음.. 주피터로 하면 필요없을듯!\n",
    "# import shutil\n",
    "\n",
    "# # 파일 경로\n",
    "# src_path = '/content/korean'\n",
    "# dest_path = '/root/nltk_data/corpora/stopwords'\n",
    "\n",
    "# # 파일 이동\n",
    "# shutil.move(src_path, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3F46yVzccnUH"
   },
   "source": [
    "# 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식 테스트\n",
    "patterns = [\n",
    "    r'<.*?>|\\[.*?\\]|【.*?】|\\{.*?\\}',\n",
    "    r'[^가-힣0-9\\s◆◇▲▼]'\n",
    "]\n",
    "\n",
    "text = '안녕하세요! [안녕하세요] (안녕하세요★☆)'\n",
    "for pattern in patterns:\n",
    "    text = re.sub(pattern, '', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '안녕하세요.이것은 테스트 문장입니다...그렇습니다.'\n",
    "text2 = '안녕하세요.. 이것은 테스트 문장입니다. 그렇습니다.'\n",
    "\n",
    "def split_text(article):   # split하는 함수 정의\n",
    "    sentences = re.split(r'(?<!\\.)\\.(?!\\.)', article)  # 마침표를 기준으로 문장 나누기, 단 마침표가 2개 이상 연달아 있는 경우 제외\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]   # 공백 제거 및 빈 문장 필터링\n",
    "    return sentences\n",
    "\n",
    "print(split_text(text1))\n",
    "print('=====')\n",
    "print(split_text(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZbHR08aYcnUJ"
   },
   "outputs": [],
   "source": [
    "# 데이터 클렌징 함수 정의\n",
    "def clean_text(text):\n",
    "\n",
    "    # 0. 한자를 한글로 변환\n",
    "    # text = re.sub(r'\\([^()]*\\)', '', text)  # 괄호 안의 한자는 삭제\n",
    "    text = hanja.translate(text, 'substitution')  # 그 외 한자는 한글로 변환\n",
    "\n",
    "    # 1. 첫 번째 문장에서 `◆...◆` 패턴 제거\n",
    "    sentences = text.split('. ')  # 마침표와 공백을 기준으로 문장을 나누기\n",
    "    if sentences:  # 문장이 존재할 경우\n",
    "        sentences[0] = re.sub(r'◆.*?◆', '', sentences[0])  # 첫 번째 문장에서 패턴 제거\n",
    "    text = '. '.join(sentences)  # 문장들을 다시 결합\n",
    "\n",
    "    # 2. 제거할 단어 및 패턴 목록\n",
    "    # 기사 전체 제거, 패턴 제거 순으로 함수 실행하기\n",
    "    patterns = [\n",
    "        r'[^.]*※[^.]*\\.',            # '※'를 포함하는 문장 전체를 제거\n",
    "        r'[^.]*\\[사설\\][^.]*\\.',      # '[사설]'을 포함하는 문장 전체를 제거\n",
    "        r'[^.]*마켓PRO[^.]*\\.',       # '마켓PRO'를 포함하는 문장 전체를 제거\n",
    "        r'[^.]*한경유레카[^.]*\\.',   # '한경유레카'를 포함하는 문장 전체를 제거\n",
    "        r'[^.]*이 기사는[^.]*\\.',    # '이 기사는'을 포함하는 문장 전체를 제거\n",
    "        r'[^.]*@[^.]*\\.',            # '@'를 포함하는 문장 전체를 제거 / 이메일을 제거하기 위함\n",
    "        r'[^.]*#장면[^.]*\\.',          # '#장면'을 포함하는 문장 전체를 제거\n",
    "        r'[^.]*\\[기자\\][^.]*\\.',      # '[기자]'를 포함하는 문장 전체를 제거\n",
    "        r'[^.]*\\[저작권\\][^.]*\\.',    # '[저작권]'을 포함하는 문장 전체를 제거\n",
    "        r'[^.]*\\[퀴즈\\][^.]*\\.',       # '[퀴즈]'를 포함하는 문장 전체를 제거\n",
    "        r'[^.]*<용 어>[^.]*\\.',\n",
    "        r'<.*?>|\\[.*?\\]|【.*?】|\\{.*?\\}', # ()제외, 온갖 종류의 괄호와 그 안의 내용 전부 제거\n",
    "        \n",
    "        r'< 이 기사는 기획 PR 기사 입니다\\. >',\n",
    "        r'< 이 기사는 BizⓝCEO 기획특별판 입니다 >',\n",
    "        r'[^.?!]*?한경닷컴[^.?!]*?[.?!]',\n",
    "        r'[^.?!]*?www\\.[^.?!]*?([.?!]|$)',\n",
    "        r'▶ 삼성미술품 \\d+만\\d+천점 수조원대 이르러…\\d+억원 \\'행복한 눈물\\'은 어디에',\n",
    "        r'▶ 에버랜드 압수수색',\n",
    "        r'▶ 삼성특검, 이순동 사장 소환',\n",
    "        r'▶ 삼성 에버랜드 압수수색 이틀째…긴장 고조',\n",
    "        r'▶ 특검, \\`삼성 전략기획실\\' 임원 첫 소환조사',\n",
    "        r'무단 전재 및 재배포 금지',\n",
    "        r'한경닷컴\\[한경\\+ 구독신청\\] \\[기사구매\\] \\[모바일앱\\]',\n",
    "        r'ⓒ \\'성공을 부르는 습관\\' 한경닷컴, 무단 전재 및 재배포 금지',\n",
    "        r'\\(사진\\)',\n",
    "        r'\\(가나다순\\)',\n",
    "        r'\\(\\d{3}\\)\\d{3}-\\d{4}',   # 전화번호\n",
    "        r'☎',\n",
    "        r'\\s?\\w{3}\\s*기자',   # ooo 기자\n",
    "        r'그래픽=',\n",
    "        r'사진=',\n",
    "        r'▲대한제강=오형근에서 오치훈으로 대표이사 변경',\n",
    "        r'[^.!?]*◆특별취재팀[^.!?]*[.!?]',\n",
    "        r'\\[기사보다 빠른 주식정보 , 슈퍼개미 APP\\]',\n",
    "        r'■ SSDsolid state drive',\n",
    "        r'한경\\+는 PC·폰·태블릿에서 읽을 수 있는 한경 디지털 신문입니다구독 plus\\.hankyung\\.com 문의',\n",
    "        r'한경부동산',\n",
    "        r'뉴스룸',\n",
    "        r'한국경제TV',\n",
    "        r'최근 애널리스트 분석의견',\n",
    "        r'주체별 매매동향',\n",
    "        r'기사제보 및 보도자료',\n",
    "        r'뉴스래빗 페이스북 facebook\\.com/newslabit',\n",
    "        r'#믿고 보는 #기자 \\'한경 기자 코너\\'',\n",
    "        r'▶ 네이버에서 한국경제 뉴스를 받아보세요',\n",
    "        r'▶ 한경닷컴 바로가기',\n",
    "        r'▶ 모바일한경 구독신청',\n",
    "        r'▶ 경제지 네이버 구독 첫 400만, 한국경제 받아보세요',\n",
    "        r'▶ 한경 고품격 뉴스레터, 원클릭으로 구독하세요',\n",
    "        r'▶ 한국경제신문과 WSJ, 모바일한경으로 보세요',\n",
    "        r'▶ 국내 최고의 \\'재테크 전문가\\'들과 만나는 공간',\n",
    "        r'▶ 한국경제앱 다운받고 \\‘암호화폐\\’ 받아가세요',\n",
    "        r'▶ 한국경제 구독신청-구독료 10% 암호화폐 적립',\n",
    "        r'ⓒ 한국경제 & hankyung\\.com, 무단전재 및 재배포 금지',\n",
    "        r'THE WALL STREET JOURNAL 한경 독점제휴',\n",
    "        r'이 기사는 한국경제신문과 금융 AI 전문기업 씽크풀이 공동 개발한 기사 자동생성 알고리즘에 의해 실시간으로 작성된 것입니다\\.',\n",
    "        r'\\‘와우밴드\\’ 앱을 설치한 후 종목 상담을 신청하면 와우넷 주간수익률 베스트파트너가 자세한 종목 진단을 해 드립니다\\.',\n",
    "    ]\n",
    "\n",
    "    # 단어 및 패턴을 제거하는 함수\n",
    "    def remove_patterns(text):\n",
    "        for pattern in patterns:\n",
    "            text = re.sub(pattern, '', text)\n",
    "        return text\n",
    "\n",
    "    # 패턴 제거\n",
    "    # text = remove_article(text)\n",
    "    text = remove_patterns(text)\n",
    "    \n",
    "    # 3. 문장 단위로 split하여 리스트에 저장\n",
    "    def split_text(article):   # split하는 함수 정의\n",
    "        sentences = re.split(r'(?<!\\.)\\.(?!\\.)', article)  # 마침표를 기준으로 문장 나누기, 단 마침표가 2개 이상 연달아 있는 경우 제외\n",
    "        sentences = [sentence.strip() for sentence in sentences if sentence.strip()]   # 공백 제거 및 빈 문장 필터링\n",
    "        return sentences\n",
    "\n",
    "    # 각 문장들을 요소로 갖는 리스트인 text_list\n",
    "    text_list = split_text(text)\n",
    "\n",
    "    # 4. 특수 문자 제거 (◆, ◇, ▲, ▼ 제외)\n",
    "    text_list = [re.sub(r'[^가-힣0-9\\s◆◇▲▼]', '', s) for s in text_list]  # 한글과 숫자, 공백, 그리고 ◆, ◇, ▲, ▼만 남김\n",
    "\n",
    "    # 5. 띄어쓰기 교정\n",
    "    spacing = Spacing()\n",
    "    text_list = [spacing(s) for s in text_list]\n",
    "\n",
    "    # 6. 오타 수정\n",
    "    def correct_text(sentence):\n",
    "        result = spell_checker.check(sentence)\n",
    "        return result.checked   # 수정된 문장 반환\n",
    "    # 직접 spell_checker 라이브러리 코드 열어서 payload의 key값에 passportKey와 _callback 추가, value는 네이버 맞춤법 검사기 개발자 도구 참고하기\n",
    "    # data 변수도 바꿔줘야함 -> 자세한건 노션에 서술\n",
    "\n",
    "    text_list = [correct_text(s) for s in text_list]\n",
    "\n",
    "    #아래 오류가 났을 때의 코드 -> API 응답이 뜨지 않고 오류 발생이라고 뜨면 라이브러리 코드를 수정 안했거나 잘못한 것\n",
    "    '''\n",
    "    def correct_typos(sentence):\n",
    "        if not isinstance(sentence, str):\n",
    "            print(\"입력값이 문자열이 아닙니다.\")\n",
    "            return sentence  # 문자열이 아닐 경우 원본 반환\n",
    "        try:\n",
    "            result = spell_checker.check(sentence)\n",
    "            if not result.result:\n",
    "                print(f\"수정 실패: {result.original}\")\n",
    "                return sentence  # 수정 실패 시 원본 문장 반환\n",
    "            return result.checked  # 수정된 문장 반환\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {e}\")\n",
    "            return sentence  # 오류 발생 시 원본 문장 반환\n",
    "\n",
    "    corrected_text_list = []\n",
    "    for sentence in text_list:\n",
    "        try:\n",
    "            result = spell_checker.check(sentence)\n",
    "            print(f\"API 응답: {result}\")  # API 응답 확인\n",
    "            corrected_text_list.append(result.checked)\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {e}\")\n",
    "            corrected_text_list.append(sentence)  # 오류 발생 시 원본 문장 반환\n",
    "\n",
    "    for original, corrected in zip(text_list, corrected_text_list):\n",
    "        print(f\"원본: {original}\\n수정: {corrected}\\n\")\n",
    "    '''\n",
    "\n",
    "    # 7. 불용어 제거\n",
    "    stop_words = set(stopwords.words('korean'))   # korean 불용어 파일 만들어져 있어야 함\n",
    "    def remove_stopwords(text):   # 불용어 제거 함수 정의\n",
    "        return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    text_list = [remove_stopwords(s) for s in text_list]\n",
    "\n",
    "    # 8. 분리된 문장들 다시 하나의 텍스트로 합치기\n",
    "    text = ' '.join(text_list)   # 다시 하나의 문자열로 합쳐진걸 원하면 return text 하면 됨\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSImGnuLcnUN"
   },
   "source": [
    "# 특정 연도 csv 파일 하나만 돌릴 때 아래 코드 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEnAfYsbcnUO"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# CSV 파일을 데이터프레임으로 불러오기\n",
    "start = time.time()\n",
    "df = pd.read_csv('2005_한국경제_test.csv')\n",
    "\n",
    "# 데이터 정제 ) null값과 중복 제거\n",
    "df.dropna(subset=['body'], inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# 패턴 목록 정의\n",
    "patterns = [\n",
    "    r'<표>',\n",
    "    r'\\[표\\]',\n",
    "    r'\\[포토\\]',\n",
    "    r'무신불립',\n",
    "]\n",
    "\n",
    "# 패턴이 포함된 행을 삭제하기 위한 조건식 작성\n",
    "condition1 = df['title'].apply(lambda text: any(re.search(pattern, text) for pattern in patterns))\n",
    "df = df[~condition1]\n",
    "condition2 = df['body'].apply(lambda text: any(re.search(pattern, text) for pattern in patterns))\n",
    "df = df[~condition2]\n",
    "\n",
    "# 'title'과 'body' 컬럼에 클렌징 함수 적용\n",
    "df['cleaned_title'] = df['title'].apply(clean_text)\n",
    "df['cleaned_body'] = df['body'].apply(clean_text)\n",
    "\n",
    "# 결과 확인\n",
    "print(df[['body', 'cleaned_body']].head())\n",
    "\n",
    "# 클렌징된 데이터프레임 저장 (원하는 파일명으로 변경)\n",
    "df.to_csv('cleaned_2005_한국경제_test.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VbLaLfqcnUQ"
   },
   "source": [
    "# 여러 연도 csv 파일 반복문으로 돌릴 때 아래 코드 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-_75UBPcnUS"
   },
   "outputs": [],
   "source": [
    "# 파일 경로 및 범위 설정\n",
    "start_year = 2005\n",
    "end_year = 2024\n",
    "\n",
    "# 모든 연도의 CSV 파일을 처리\n",
    "for year in range(start_year, end_year + 1):\n",
    "    filename = f'/{year}_한국경제.csv'\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Processing file: {filename}\")\n",
    "\n",
    "        # CSV 파일을 데이터프레임으로 불러오기\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        df.dropna(subset=['body'], inplace=True)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "\n",
    "        # 패턴 목록 정의\n",
    "        patterns = [\n",
    "            r'<표>',\n",
    "            r'\\[표\\]',\n",
    "            r'\\[포토\\]',\n",
    "            r'무신불립',\n",
    "        ]\n",
    "\n",
    "        # 패턴이 포함된 행을 삭제하기 위한 조건식 작성\n",
    "        condition1 = df['title'].apply(lambda text: any(re.search(pattern, text) for pattern in patterns))\n",
    "        df = df[~condition1]\n",
    "        condition2 = df['body'].apply(lambda text: any(re.search(pattern, text) for pattern in patterns))\n",
    "        df = df[~condition2]\n",
    "\n",
    "        # 'body'와 'title' 컬럼에 클렌징 함수 적용\n",
    "        df['cleaned_body'] = df['body'].apply(clean_text)\n",
    "        df['cleaned_title'] = df['title'].apply(clean_text)\n",
    "\n",
    "        # 클렌징된 데이터프레임 저장\n",
    "        output_filename = f'cleaned_{year}_한국경제.csv'  \n",
    "        df.to_csv(output_filename, index=False)\n",
    "\n",
    "        print(f\"Saved cleaned file: {output_filename}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"File not found: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
