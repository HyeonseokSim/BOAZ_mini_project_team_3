{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRafbpvewstv",
        "outputId": "5c49e5ec-5333-409a-a665-e372b6023195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "# 트랜스포머 설치\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cw0Xg6oxXvs",
        "outputId": "44704888-0835-4cce-8130-4dd5c5287d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: adabelief-pytorch==0.2.0 in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch==0.2.0) (2.4.0+cu121)\n",
            "Requirement already satisfied: colorama>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch==0.2.0) (0.4.6)\n",
            "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch==0.2.0) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->adabelief-pytorch==0.2.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->adabelief-pytorch==0.2.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# AdaBelief optimizer 설치\n",
        "!pip install adabelief-pytorch==0.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "llgkmy1S1o1X"
      },
      "outputs": [],
      "source": [
        "# library import\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import json\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from transformers import BertModel, TFBertModel, TFRobertaModel, RobertaTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelForSequenceClassification\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split,KFold,StratifiedKFold\n",
        "from adabelief_pytorch import AdaBelief\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import shutil\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import urllib.request\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fW3SvTlb0DW8"
      },
      "outputs": [],
      "source": [
        "#random seed 설정\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "#배치사이즈, 에폭 설정\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 6\n",
        "\n",
        "L_RATE = 1e-5\n",
        "\n",
        "#입력데이터 최대 길이\n",
        "MAX_LEN = 512\n",
        "\n",
        "#Gradient Clipping -> L2-norm 이상 Gradient 벡터 정규화\n",
        "max_grad_norm=1\n",
        "\n",
        "#로그 출력 주기\n",
        "log_interval=200\n",
        "\n",
        "#CPU코어 개수\n",
        "NUM_CORES = os.cpu_count()\n",
        "\n",
        "#GPU 활용\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "DATA_IN_PATH = './data'\n",
        "DATA_OUT_PATH = \"./data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zva6k_G1yfX"
      },
      "source": [
        "## 금융 문장 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WK9hXvtw19qB"
      },
      "outputs": [],
      "source": [
        "# 깃허브 번역 금융 뉴스 기사 문장 데이터셋 경로\n",
        "DATASET_URL = \"https://raw.githubusercontent.com/ukairia777/finance_sentiment_corpus/main/finance_data.csv\"\n",
        "DATASET_NAME = \"finance_data.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL_MXEUR2EA3",
        "outputId": "c0ac3043-63c7-402a-9733-8c48a9ba4f49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('finance_data.csv', <http.client.HTTPMessage at 0x7ce4f7b7e200>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# 파일 다운로드\n",
        "urllib.request.urlretrieve(DATASET_URL, filename = DATASET_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "KB4wxHi12IeN",
        "outputId": "59ca1b2e-9ff8-4bb0-fb48-b9243186832a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     labels                                           sentence  \\\n",
              "0   neutral  According to Gran, the company has no plans to...   \n",
              "1   neutral  Technopolis plans to develop in stages an area...   \n",
              "2  negative  The international electronic industry company ...   \n",
              "3  positive  With the new production plant the company woul...   \n",
              "4  positive  According to the company's updated strategy fo...   \n",
              "\n",
              "                                        kor_sentence  \n",
              "0  Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...  \n",
              "1  테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...  \n",
              "2  국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...  \n",
              "3  새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...  \n",
              "4  2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-752cc6bd-47fa-4dc9-a94b-77c9831c28b3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>sentence</th>\n",
              "      <th>kor_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>According to Gran, the company has no plans to...</td>\n",
              "      <td>Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "      <td>테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "      <td>국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "      <td>새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>According to the company's updated strategy fo...</td>\n",
              "      <td>2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-752cc6bd-47fa-4dc9-a94b-77c9831c28b3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-752cc6bd-47fa-4dc9-a94b-77c9831c28b3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-752cc6bd-47fa-4dc9-a94b-77c9831c28b3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-71f8e203-80d5-40a7-be8d-f802b711b96d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-71f8e203-80d5-40a7-be8d-f802b711b96d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-71f8e203-80d5-40a7-be8d-f802b711b96d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 4846,\n  \"fields\": [\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"neutral\",\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4838,\n        \"samples\": [\n          \"The Company serves approximately 3,000 customers in over 100 countries.\",\n          \"On Dec. 1, Grimaldi acquired 1.5 million shares and a 50.1-percent stake in Finnlines.\",\n          \"The extracted filtrates are very high in clarity while the dried filter cakes meet required transport moisture limits (TMLs)for their ore grades.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"kor_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4827,\n        \"samples\": [\n          \"YIT\\uc5d0 \\ub300\\ud55c \\uace0\\uac1d\\ub4e4\\uc758 \\uc2e0\\ub8b0 \\uc99d\\uac00\\ub294 \\uc544\\ud30c\\ud2b8 \\ub9e4\\ub9e4\\uac00 \\uac00\\uc18d\\ud654\\ub41c \\uac83\\uc73c\\ub85c \\ubcfc \\uc218 \\uc788\\uc2b5\\ub2c8\\ub2e4.\",\n          \"\\uc0dd\\uc0b0\\uc740 2010\\ub144 \\ub3d9\\uc548 \\uba55\\uc2dc\\ucf54\\uc640 \\ud5dd\\uac00\\ub9ac\\ub97c \\ud3ec\\ud568\\ud55c \\uc5d8\\ucf54\\ud14d\\uc758 \\ub2e4\\ub978 \\uc9c0\\uc5ed\\uc73c\\ub85c \\ud655\\ub300\\ub420 \\uac83\\uc774\\ub2e4.\",\n          \"\\ubc14\\uc774\\uc0b4\\ub77c\\ub294 \\ub610\\ud55c 2009\\ub144\\uc758 2\\uc5b55220\\ub9cc \\uc720\\ub85c\\uc5d0 \\ube44\\ud574 2010\\ub144\\uc5d0\\ub294 2\\uc5b55320\\ub9cc \\uc720\\ub85c\\uac00 \\uc21c\\ub9e4\\ucd9c\\ub420 \\uac83\\uc73c\\ub85c \\uc608\\uc0c1\\ud55c\\ub2e4\\uace0 \\ubc1d\\ud614\\ub2e4.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# 금융 뉴스 데이터셋\n",
        "dataset = pd.read_csv(DATASET_NAME)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A7przkJR2iBk"
      },
      "outputs": [],
      "source": [
        "# 기존 영문 문장 제거\n",
        "del dataset['sentence']\n",
        "\n",
        "# 데이터셋 라벨데이터를 숫자로 치환\n",
        "dataset['labels'] = dataset['labels'].replace(['neutral', 'positive', 'negative'],[0, 1, 2])\n",
        "\n",
        "# 중복 데이터 제거\n",
        "DATASET_PREP_FILE = 'dataset_prep.csv'\n",
        "dataset.drop_duplicates(subset = ['kor_sentence'], inplace = True)\n",
        "dataset.to_csv(DATASET_PREP_FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW7aNgfr2maS"
      },
      "source": [
        "## 학습 세팅\n",
        "사용하고자 하는 klue/roberta-base 언어모델의 tokenizer를 불러와서 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLhoDmfc6GqT",
        "outputId": "bd658ca9-5dc4-4250-ec0f-6a9f447b5b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Klue-roberta-base model tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\", cache_dir='bert_ckpt', do_lower_case=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YvR-d_TV94fz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_csv('dataset_prep.csv')\n",
        "dataset.drop('Unnamed: 0',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UhQ84RPh-QJM"
      },
      "outputs": [],
      "source": [
        "# 입출력 데이터 분리\n",
        "X_data = dataset['kor_sentence']\n",
        "y_data = dataset['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MON1EBsK-RpE"
      },
      "outputs": [],
      "source": [
        "# train, test split\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,\n",
        "                                                    test_size = TEST_SIZE,\n",
        "                                                    random_state = RANDOM_STATE,\n",
        "                                                    stratify = y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KauaENda-sbR"
      },
      "outputs": [],
      "source": [
        "# 데이터셋으로 변환해주기 위해 형식을 맞추도록 하겠습니다.\n",
        "X=pd.concat([X_train,y_train],axis=1)\n",
        "X.reset_index(drop=True,inplace=True)\n",
        "X_test=X_test.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qlzaMyemL797"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 전처리\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df_data = df\n",
        "    def __getitem__(self, index):\n",
        "        # 문장 불러오기\n",
        "        sentence = self.df_data.loc[index, 'kor_sentence']\n",
        "        encoded_dict = tokenizer(\n",
        "          text = sentence,\n",
        "          add_special_tokens = True, # 문장 처음과 끝 토큰 추가\n",
        "          max_length = MAX_LEN, #최대 길이를 설정\n",
        "          pad_to_max_length = True, #길이에 따라 초과하는 부분 자르고,\n",
        "          truncation=True, #모자란 부븐 패딩\n",
        "          return_tensors=\"pt\") #텐서로 반환\n",
        "\n",
        "        padded_token_list = encoded_dict['input_ids'][0] #패딩된 토큰 id 리스트\n",
        "        token_type_id = encoded_dict['token_type_ids'][0] #세그먼트 id -> 문장 구분\n",
        "        att_mask = encoded_dict['attention_mask'][0] #마스킹 된 부분은 0, 실제 데이터셋이 있는 토큰 위치는 1\n",
        "        target = torch.tensor(self.df_data.loc[index, \"labels\"]) # 타겟 텐서로 반환\n",
        "        sample = (padded_token_list, token_type_id , att_mask, target)\n",
        "        return sample\n",
        "    def __len__(self):\n",
        "        return len(self.df_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HGI93Fs1L8ge"
      },
      "outputs": [],
      "source": [
        "# 동일한 방식 전처리\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df_data = df\n",
        "    def __getitem__(self, index):\n",
        "        sentence = self.df_data.loc[index, 'kor_sentence']\n",
        "        encoded_dict = tokenizer(\n",
        "          text = sentence,\n",
        "          add_special_tokens = True,\n",
        "          max_length = MAX_LEN,\n",
        "          pad_to_max_length = True,\n",
        "          truncation=True,\n",
        "          return_tensors=\"pt\")\n",
        "\n",
        "        padded_token_list = encoded_dict['input_ids'][0]\n",
        "        token_type_id = encoded_dict['token_type_ids'][0]\n",
        "        att_mask = encoded_dict['attention_mask'][0]\n",
        "        sample = (padded_token_list, token_type_id , att_mask)\n",
        "        return sample\n",
        "    def __len__(self):\n",
        "        return len(self.df_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RyogefSyL9_l"
      },
      "outputs": [],
      "source": [
        "# 정확도 계산\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0a64edf1500b40d5aa6257e8a16199e8",
            "aff7f82e8c754ef782bad2b306210d16",
            "63345ad270254aafa4d3884215ae2f6a",
            "e5a996dcced342cfbaf7af4135335830",
            "9a41b76a568042f6a447a2e7f4abfcde",
            "8ef4c7d72ab54d89af1e6ba1e23dd7b8",
            "545a33f5458f40cd822f406eef2c288a",
            "e63da71294ab453398a0ff173d0b45bc",
            "ecfcbb323e28411b9377ee27a154db8b",
            "eaa3cec8724c450da3ae31c5d03d435a",
            "799db078221e47c6974c9451364fddf7",
            "0ed4c35672fc48b385a8d98c784d3a12",
            "35bbbff88f194e5b8ab4cb869bf87a32",
            "0962c5dd16544e84a2ea69b7156c63c1",
            "8128901c619244c1a7e4f825ecef1365",
            "6ade17158af441f8891e760974064cba",
            "f9c62e7a142f4405a5f5eb5b30fe5ddf",
            "5574aa6ad8f54ae385fd2ca61813a463",
            "ef50b518b796417f8b9a43df9c40cac1",
            "dbf2270aecdc461ba8ace414848077fe",
            "cb3189ee615d4b9a9c291cbb30e48cdd",
            "7f7f049f5ff84088967c305fd0b8daac",
            "e922fbc4a62c47de82b71f048bec507e",
            "e4ce8e60b1b54ab3b9eb0a05eb077bc4",
            "ef68bf630e034bec9d464709d9fc4da6",
            "ec81b84e201743b7b606f7c648d047ae",
            "0af2c4729b78470086a2f7f2aa6744b3",
            "0b0ea2fdecf74f8d8ce2bc08c081a0c1",
            "2d9c5c6f20d94745ac0a54764e49d811",
            "b000534b4ee646b0a86aed136892bcc8",
            "db16c1b814e747dbada283538dc5d8c4",
            "2b0154de48d84dc497c4608b1b6c8458",
            "bd0a4894934044d3964aef97ba587846",
            "ef0f3fc941854b0fa24210fe22453409",
            "46934d94668d414380d74723870ba04d",
            "9021b3725f5d48e289475445dadc1278",
            "f3c3bcd10cfe45efae4ed5701e0fe5c5",
            "afcb81cc9525414686a847c14e10c0b8",
            "e772f67354b548ff87ff2d7efa251a4b",
            "f29e183b401b490c9b8833f097ea339b",
            "6e2e8822a97140299c1ee84094a6ae03",
            "3475490566404627943a659e22332129",
            "526025ffb89d4f6792d121abe024ec23",
            "4fec4df44f664e1db9638706f2ccf19e",
            "642a20dee0644e81876e2abc8d8122d9",
            "129242362d3246dd8975968d39a1651c",
            "d8fc273082a6430898338f19e39a81a0",
            "a15d4c3075064571a2b73cfec45ca3e1",
            "4d375086f71e4d499276ca41a52a5986",
            "ce46895eba8444779158cd516be41b23",
            "ecf45a26c78a489b8d4bb2034cd3389e",
            "a844598428664500b5067d7b593905f2",
            "4f42e9b0dbf24a2a83c0311745a71122",
            "696fe8e39eaf4982b2e9214c23cf1243",
            "3ab70c1a8f5b48dc94d0bf9713020a49",
            "595388fb1fa948d0b580a68f40e50879",
            "e69fb5b943eb47cfb432c7385804e42d",
            "2d232a0379b14d86b4239481c3f8f692",
            "ffb6b447b8b945638a38e268514ec06d",
            "9cf58fd1e74c402d8f4c9c345bc76282",
            "a77d4c1298a24ff5b319cef2a71275a3",
            "d966dfda02284dad99c5edbc1aede483",
            "1883d3d58afb4dcaa069450418c0a90f",
            "8632478077e446f58976faaea72e01d1",
            "f410797350c64b10a757f5c26a171a92",
            "fc2a7bb15e514dc8b9edb189d32fbe78",
            "a8639f59e0b2449c95c2b63797e54c6e",
            "0bd0a5b9f70243539e434a86732580ab",
            "8428fb2a0561425ea6ad9ecf316584bc",
            "7ea91b322ce64ac7b4f77bc019e91330",
            "126c29d7a5824a628bc22821ed80b63c",
            "e09dccf7216d430b84e42476436f871a",
            "3242d6e329ba470abd2ffca2d07cbb9d",
            "dbe18b780fb74cdab90e360726f0a12e",
            "46bd84b677504ea397e6e62b6133ba51",
            "8d6bd8f2de8f4ae8b305659b38ec6c7f",
            "ca571f859d23487aa729ae25ade61437"
          ]
        },
        "id": "WguRJRwTMCHb",
        "outputId": "a05985a4-751f-4bc5-f646-4c358df9f964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  ---------\n",
            "adabelief-pytorch=0.0.5  1e-08  False              False\n",
            ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n",
            "Weight decoupling enabled in AdaBelief\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-521531063dbb>:37: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for batch_id, (input_id,token_type_id,attention_mask,label) in enumerate(tqdm_notebook(train_dataloader)):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/121 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a64edf1500b40d5aa6257e8a16199e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 batch id 1 loss 1.1191691160202026 train acc 0.32051282051282054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 train acc 0.7335996567417257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-521531063dbb>:37: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for batch_id, (input_id,token_type_id,attention_mask,label) in enumerate(tqdm_notebook(train_dataloader)):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/121 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ed4c35672fc48b385a8d98c784d3a12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 batch id 1 loss 0.4155726432800293 train acc 0.873936547323644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 train acc 0.8482987140879079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-521531063dbb>:37: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for batch_id, (input_id,token_type_id,attention_mask,label) in enumerate(tqdm_notebook(train_dataloader)):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/121 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e922fbc4a62c47de82b71f048bec507e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 batch id 1 loss 0.2846636176109314 train acc 0.8396291208791209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 train acc 0.9039218235485511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-521531063dbb>:37: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for batch_id, (input_id,token_type_id,attention_mask,label) in enumerate(tqdm_notebook(train_dataloader)):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/121 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef0f3fc941854b0fa24210fe22453409"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4 batch id 1 loss 0.15036490559577942 train acc 0.9366319444444444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4 train acc 0.9416246303765479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-521531063dbb>:37: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for batch_id, (input_id,token_type_id,attention_mask,label) in enumerate(tqdm_notebook(train_dataloader)):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/121 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "642a20dee0644e81876e2abc8d8122d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5 batch id 1 loss 0.023715456947684288 train acc 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5 train acc 0.9588205401737698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-521531063dbb>:37: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for batch_id, (input_id,token_type_id,attention_mask,label) in enumerate(tqdm_notebook(train_dataloader)):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/121 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "595388fb1fa948d0b580a68f40e50879"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6 batch id 1 loss 0.023823164403438568 train acc 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6 train acc 0.9664424881984603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-521531063dbb>:61: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for batch_id, (input_id,token_type_id,attention_mask) in enumerate(tqdm_notebook(test_dataloader)):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/31 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8639f59e0b2449c95c2b63797e54c6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#dataset -> 배치사이즈 만큼\n",
        "train_data = TrainDataset(X)\n",
        "\n",
        "test_data = TestDataset(X_test)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=True,\n",
        "                                      num_workers=0)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=False,\n",
        "                                      num_workers=0)\n",
        "\n",
        "#\"klue/roberta-base\" 모델 -> 중립(0), 긍정(1), 부정(2), label 분류\n",
        "model = AutoModelForSequenceClassification.from_pretrained('klue/roberta-base',num_labels=3)\n",
        "\n",
        "#model GPU 할당\n",
        "model.to(device)\n",
        "\n",
        "#Optimizer 설정\n",
        "optimizer = AdaBelief(model.parameters(), lr=1e-5, eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = False)\n",
        "\n",
        "\n",
        "# 안정적인 학습을 위해 학습률을 점진적으로 증가시키고, 코사인 함수 형태로 감소하는 스케쥴러 설정 (전체 스텝의 10프로만)\n",
        "warmup_ratio = 0.1\n",
        "t_total = len(train_dataloader) * NUM_EPOCHS\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "for e in range(NUM_EPOCHS):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    best_acc =0.0\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    for batch_id, (input_id,token_type_id,attention_mask,label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad() #가중치 초기화\n",
        "        input_id = input_id.long().to(device)\n",
        "        token_type_id = token_type_id.long().to(device)\n",
        "        attention_mask = attention_mask.long().to(device)\n",
        "        label = label.to(device)\n",
        "        outputs = model(input_ids=input_id, token_type_ids=token_type_id, attention_mask=attention_mask, labels=label)\n",
        "        loss = outputs[0]\n",
        "        out = outputs[1] #출력 로짓\n",
        "        loss.backward() # 역전파\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) #gradient clipping\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        _, preds = torch.max(out, dim=1)\n",
        "        train_acc += f1_score(preds.cpu(), label.cpu(),average='weighted') #cpu로 이동, 샘플 가중치 주어 f1_score 계산\n",
        "        if batch_id % log_interval == 0: # log_interval 마다 출력\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "\n",
        "preds = []\n",
        "\n",
        "#평가 - 예측값 pres에 저장\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "for batch_id, (input_id,token_type_id,attention_mask) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "    input_id = input_id.long().to(device)\n",
        "    token_type_id = token_type_id.long().to(device)\n",
        "    attention_mask = attention_mask.long().to(device)\n",
        "    outputs = model(input_ids=input_id, token_type_ids=token_type_id, attention_mask=attention_mask)\n",
        "    out = outputs[0]\n",
        "    for inp in out:\n",
        "      preds.append(inp.detach().cpu().numpy())\n",
        "Preds = np.array(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VQbtrgMEMsvr"
      },
      "outputs": [],
      "source": [
        "# 가장 로짓 높은 라벨\n",
        "predicted_label = np.argmax(Preds, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27PSm_RaM1HF",
        "outputId": "a02298c5-0854-435c-9d59-02874ebbce0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
              "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       2, 0, 0, 2, 0, 2, 2, 0, 1, 1, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
              "       1, 1, 2, 0, 0, 1, 0, 0, 2, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 0, 2, 2, 2, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0,\n",
              "       2, 1, 0, 0, 2, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 2,\n",
              "       0, 2, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 2,\n",
              "       0, 2, 1, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 1, 2, 1, 0, 2, 2, 0, 0, 1,\n",
              "       0, 0, 2, 0, 1, 0, 1, 1, 1, 0, 0, 2, 1, 0, 1, 1, 0, 0, 0, 1, 2, 0,\n",
              "       0, 0, 0, 1, 1, 0, 2, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
              "       0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       0, 2, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 1, 1, 0, 0, 2, 0, 1, 1,\n",
              "       0, 2, 1, 0, 2, 1, 0, 2, 1, 1, 1, 1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 1, 2, 2, 0, 1, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 0, 2, 1, 2, 0,\n",
              "       0, 1, 0, 1, 2, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2,\n",
              "       0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
              "       1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1,\n",
              "       1, 2, 1, 0, 0, 2, 1, 1, 1, 1, 2, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0,\n",
              "       0, 0, 0, 1, 2, 1, 0, 1, 0, 1, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
              "       0, 0, 1, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 2,\n",
              "       2, 1, 0, 1, 2, 0, 0, 0, 1, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 0, 0, 2, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 0, 2,\n",
              "       0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2,\n",
              "       1, 1, 0, 0, 0, 1, 2, 2, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 1, 0,\n",
              "       0, 1, 0, 1, 2, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 1, 0,\n",
              "       0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
              "       0, 0, 1, 1, 2, 1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
              "       2, 0, 1, 1, 0, 1, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
              "       1, 0, 1, 0, 2, 0, 0, 1, 1, 1, 0, 1, 0, 1, 2, 1, 0, 2, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
              "       1, 0, 0, 2, 1, 0, 1, 2, 0, 0, 1, 1, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 1, 0, 0, 1, 2, 1, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       0, 2, 0, 0, 2, 0, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 2, 1, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 2, 0, 1, 2, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 0, 1, 0,\n",
              "       0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 2, 1, 1, 1, 0, 0, 1, 2, 1,\n",
              "       1, 0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 1, 1, 2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "predicted_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGiBHcb2hPAA",
        "outputId": "f3682f3d-f7f3-4144-dd93-5c2f48287630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision  recall  f1-score  support\n",
            "0                 0.898   0.890     0.894  572.000\n",
            "1                 0.798   0.824     0.811  273.000\n",
            "2                 0.872   0.843     0.857  121.000\n",
            "accuracy          0.865   0.865     0.865    0.865\n",
            "macro avg         0.856   0.852     0.854  966.000\n",
            "weighted avg      0.866   0.865     0.866  966.000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, \\\n",
        "                            roc_auc_score, confusion_matrix, classification_report, \\\n",
        "                            matthews_corrcoef, cohen_kappa_score, log_loss\n",
        "\n",
        "# 사전학습 성능\n",
        "path2data='./metric'\n",
        "if not os.path.exists(path2data):\n",
        "    os.mkdir(path2data)\n",
        "CL_REPORT_FILE = \"./metric/cl_report.csv\"\n",
        "\n",
        "cl_report = classification_report(y_test, predicted_label, output_dict = True)\n",
        "cl_report_df = pd.DataFrame(cl_report).transpose()\n",
        "cl_report_df = cl_report_df.round(3)\n",
        "cl_report_df.to_csv(CL_REPORT_FILE)\n",
        "print(cl_report_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ItsScI6pM2mi"
      },
      "outputs": [],
      "source": [
        "# 모델 저장\n",
        "torch.save(model.state_dict(), 'klue_base_fold6_s.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0PPxyU_hM1ou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74881da9-5403-4726-b1c5-fe7969f12e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# GPU\n",
        "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "WijOX7SL-Nn8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hb9rNthdR8rJ"
      },
      "outputs": [],
      "source": [
        "# 매일경제 10개년치 전처리 완료된 데이터셋\n",
        "import pandas as pd\n",
        "final_매일경제 = pd.read_csv('/content/drive/MyDrive/BOAZ/23기 분석 BASE/미니프로젝트/감성분석/final_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코랩 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gCxEVsgiIMF",
        "outputId": "4895b41b-8db2-410a-d0bd-6d1ad96eb3e7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a06uiDThSENM",
        "outputId": "2199e974-bf63-4f65-e464-9fb320b94dda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            index                                               link  \\\n",
              "0      2015000001  https://n.news.naver.com/mnews/article/009/000...   \n",
              "1      2015000002  https://n.news.naver.com/mnews/article/009/000...   \n",
              "2      2015000003  https://n.news.naver.com/mnews/article/009/000...   \n",
              "3      2015000005  https://n.news.naver.com/mnews/article/009/000...   \n",
              "4      2015000006  https://m.entertain.naver.com/article/009/0003...   \n",
              "...           ...                                                ...   \n",
              "42911  2024003350  https://n.news.naver.com/mnews/article/009/000...   \n",
              "42912  2024003351  https://n.news.naver.com/mnews/article/009/000...   \n",
              "42913  2024003352  https://n.news.naver.com/mnews/article/009/000...   \n",
              "42914  2024003353  https://n.news.naver.com/mnews/article/009/000...   \n",
              "42915  2024003354  https://n.news.naver.com/mnews/article/009/000...   \n",
              "\n",
              "                      date                     title  \\\n",
              "0      2015-01-01 04:03:00   CES 놀라게 할 Made In Korea   \n",
              "1      2015-01-01 04:03:00  삼성전자 바닥 치고 2분기부터 실적 개선될듯   \n",
              "2      2015-01-01 04:03:00  리서치센터장 5인이 보는 새해 증시 투자전략   \n",
              "3      2015-01-01 15:31:00      삼성전자 타이젠 TV CES에서 공개   \n",
              "4      2015-01-01 17:07:00          [매경e신문] 오늘의 프리미엄   \n",
              "...                    ...                       ...   \n",
              "42911  2024-08-14 17:53:00    AI 특화 스마트폰 구글도 본격 뛰어든다   \n",
              "42912  2024-08-14 18:02:00    돌아온 엔비디아…반도체·소부장 '기지개'   \n",
              "42913  2024-08-14 18:02:00   삼성전자 소액주주 1년새 142만명 줄었다   \n",
              "42914  2024-08-14 18:03:00       삼성전자 2분기 R&D에 8조 썼다   \n",
              "42915  2024-08-14 18:05:00       美 법무부 칼날에 구글 쪼개질 신세   \n",
              "\n",
              "                                                    body  year  month  day  \\\n",
              "0      미국 라스베이거스에서 열리는 세계 최대 가전전시회 CES 2015를 깜짝 놀라게 할...  2015      1    1   \n",
              "1      ◆ 2015 업종별 유망주 ① IT·전자 ◆‘반도체 호황의 지속, 대형 패널 수요의...  2015      1    1   \n",
              "2      ‘상저하고(上低下高·주가가 상반기에 부진하고 하반기에는 올라가는 것 ), 지배구조·...  2015      1    1   \n",
              "3      삼성전자는 타이젠 운영체제(OS)를 기반으로 한 스마트TV를 6일 미국 라스베이거스...  2015      1    1   \n",
              "4      ◆ PC시장서 재기 노리는 삼성전자삼성전자가 올해 초 ‘노트북9 2015년 에디션’...  2015      1    1   \n",
              "...                                                  ...   ...    ...  ...   \n",
              "42911  제미나이 탑재 픽셀9 공개애플보다 출시 두달 앞당겨OS동맹 삼성과 휴대폰 경쟁갤S2...  2024      8   14   \n",
              "42912  美증시 AI관련주 강세에엔비디아 이달 17% 상승이턴 등 전력인프라도 '쑥'외국인,...  2024      8   14   \n",
              "42913  올들어 주가 급등락하자566만명서 424만명으로'국민주' 삼성전자의 소액주주가 45...  2024      8   14   \n",
              "42914  작년동기보다 12% 늘어 최대반도체 시설투자도 10조 달해삼성전자가 인공지능(AI)...  2024      8   14   \n",
              "42915  정부, 구글 반독점 정조준크롬·안드로이드 분리 요청MS 기업 분할 실패 이후20년만...  2024      8   14   \n",
              "\n",
              "       hour  minute               cleaned_title  \\\n",
              "0         4       3                      놀라게 할.   \n",
              "1         4       3  삼성전자 바닥 치고 2분기부터 실적 개선될 듯.   \n",
              "2         4       3   리서치센터장 5인이 보는 새해 증시 투자전략.   \n",
              "3        15      31                삼성전자 타이젠 공개.   \n",
              "4        17       7                   오늘의 프리미엄.   \n",
              "...     ...     ...                         ...   \n",
              "42911    17      53        특화 스마트폰 구글도 본격 뛰어든다.   \n",
              "42912    18       2     돌아온 엔비디아 반도체 소부장 '기지개'.   \n",
              "42913    18       2   삼성전자 소액주주 1년 새 142만명 줄었다.   \n",
              "42914    18       3             삼성전자 2분기 8조 썼다.   \n",
              "42915    18       5        미 법무부 칼날에 구글 쪼개질 신세.   \n",
              "\n",
              "                                            cleaned_body  \n",
              "0      미국 라스베이거스에서 열리는 세계 최대 가전 전시회 2015를 깜짝 놀라게 할 제품...  \n",
              "1      반도체 호황의 지속, 대형 패널 수요의 증가, 스마트폰의 반등 전문가들이 말하는 새...  \n",
              "2      상저하고 상저하고 주가가 상반기에 부진하고 하반기에는 올라가는 , 지배구조 배당 관...  \n",
              "3      삼성전자는 타이젠 운영체제를 기반으로 한 스마트를 6일 미국 라스베이거스에서 열리는...  \n",
              "4      ◆ 시장서 재기 노리는 삼성전자 삼성전자가 올해 초 노트북 9 2015년 에디션 올...  \n",
              "...                                                  ...  \n",
              "42911  제미나이 탑재 픽셀 9 공개 애플보다 출 시 두 달 앞당겨 동맹 삼성과 휴대폰 경쟁...  \n",
              "42912  미 증시 관련주 강세에 엔비디아 달 17% 상승이 턴 전력 인프라도 '쑥' 외국인,...  \n",
              "42913  올 들어 주가 급등락하자 566만명 서 424만명으로 '국민주' 삼성전자의 소액주주...  \n",
              "42914  작년 동기보다 12% 늘어 최대 반도체 시설 투자도 10조 달해 삼성전자가 인공지능...  \n",
              "42915  정부, 구글 반독점 정 조준 크롬 안드로이드 분리 요청 기업 분할 실패 이후 20년...  \n",
              "\n",
              "[42916 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eef5f184-0a18-4c4b-99e0-b277a07c815c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>link</th>\n",
              "      <th>date</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>minute</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015000001</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2015-01-01 04:03:00</td>\n",
              "      <td>CES 놀라게 할 Made In Korea</td>\n",
              "      <td>미국 라스베이거스에서 열리는 세계 최대 가전전시회 CES 2015를 깜짝 놀라게 할...</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>놀라게 할.</td>\n",
              "      <td>미국 라스베이거스에서 열리는 세계 최대 가전 전시회 2015를 깜짝 놀라게 할 제품...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015000002</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2015-01-01 04:03:00</td>\n",
              "      <td>삼성전자 바닥 치고 2분기부터 실적 개선될듯</td>\n",
              "      <td>◆ 2015 업종별 유망주 ① IT·전자 ◆‘반도체 호황의 지속, 대형 패널 수요의...</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>삼성전자 바닥 치고 2분기부터 실적 개선될 듯.</td>\n",
              "      <td>반도체 호황의 지속, 대형 패널 수요의 증가, 스마트폰의 반등 전문가들이 말하는 새...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015000003</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2015-01-01 04:03:00</td>\n",
              "      <td>리서치센터장 5인이 보는 새해 증시 투자전략</td>\n",
              "      <td>‘상저하고(上低下高·주가가 상반기에 부진하고 하반기에는 올라가는 것 ), 지배구조·...</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>리서치센터장 5인이 보는 새해 증시 투자전략.</td>\n",
              "      <td>상저하고 상저하고 주가가 상반기에 부진하고 하반기에는 올라가는 , 지배구조 배당 관...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015000005</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2015-01-01 15:31:00</td>\n",
              "      <td>삼성전자 타이젠 TV CES에서 공개</td>\n",
              "      <td>삼성전자는 타이젠 운영체제(OS)를 기반으로 한 스마트TV를 6일 미국 라스베이거스...</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>31</td>\n",
              "      <td>삼성전자 타이젠 공개.</td>\n",
              "      <td>삼성전자는 타이젠 운영체제를 기반으로 한 스마트를 6일 미국 라스베이거스에서 열리는...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015000006</td>\n",
              "      <td>https://m.entertain.naver.com/article/009/0003...</td>\n",
              "      <td>2015-01-01 17:07:00</td>\n",
              "      <td>[매경e신문] 오늘의 프리미엄</td>\n",
              "      <td>◆ PC시장서 재기 노리는 삼성전자삼성전자가 올해 초 ‘노트북9 2015년 에디션’...</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>7</td>\n",
              "      <td>오늘의 프리미엄.</td>\n",
              "      <td>◆ 시장서 재기 노리는 삼성전자 삼성전자가 올해 초 노트북 9 2015년 에디션 올...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42911</th>\n",
              "      <td>2024003350</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2024-08-14 17:53:00</td>\n",
              "      <td>AI 특화 스마트폰 구글도 본격 뛰어든다</td>\n",
              "      <td>제미나이 탑재 픽셀9 공개애플보다 출시 두달 앞당겨OS동맹 삼성과 휴대폰 경쟁갤S2...</td>\n",
              "      <td>2024</td>\n",
              "      <td>8</td>\n",
              "      <td>14</td>\n",
              "      <td>17</td>\n",
              "      <td>53</td>\n",
              "      <td>특화 스마트폰 구글도 본격 뛰어든다.</td>\n",
              "      <td>제미나이 탑재 픽셀 9 공개 애플보다 출 시 두 달 앞당겨 동맹 삼성과 휴대폰 경쟁...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42912</th>\n",
              "      <td>2024003351</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2024-08-14 18:02:00</td>\n",
              "      <td>돌아온 엔비디아…반도체·소부장 '기지개'</td>\n",
              "      <td>美증시 AI관련주 강세에엔비디아 이달 17% 상승이턴 등 전력인프라도 '쑥'외국인,...</td>\n",
              "      <td>2024</td>\n",
              "      <td>8</td>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>돌아온 엔비디아 반도체 소부장 '기지개'.</td>\n",
              "      <td>미 증시 관련주 강세에 엔비디아 달 17% 상승이 턴 전력 인프라도 '쑥' 외국인,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42913</th>\n",
              "      <td>2024003352</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2024-08-14 18:02:00</td>\n",
              "      <td>삼성전자 소액주주 1년새 142만명 줄었다</td>\n",
              "      <td>올들어 주가 급등락하자566만명서 424만명으로'국민주' 삼성전자의 소액주주가 45...</td>\n",
              "      <td>2024</td>\n",
              "      <td>8</td>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>삼성전자 소액주주 1년 새 142만명 줄었다.</td>\n",
              "      <td>올 들어 주가 급등락하자 566만명 서 424만명으로 '국민주' 삼성전자의 소액주주...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42914</th>\n",
              "      <td>2024003353</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2024-08-14 18:03:00</td>\n",
              "      <td>삼성전자 2분기 R&amp;D에 8조 썼다</td>\n",
              "      <td>작년동기보다 12% 늘어 최대반도체 시설투자도 10조 달해삼성전자가 인공지능(AI)...</td>\n",
              "      <td>2024</td>\n",
              "      <td>8</td>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "      <td>삼성전자 2분기 8조 썼다.</td>\n",
              "      <td>작년 동기보다 12% 늘어 최대 반도체 시설 투자도 10조 달해 삼성전자가 인공지능...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42915</th>\n",
              "      <td>2024003354</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2024-08-14 18:05:00</td>\n",
              "      <td>美 법무부 칼날에 구글 쪼개질 신세</td>\n",
              "      <td>정부, 구글 반독점 정조준크롬·안드로이드 분리 요청MS 기업 분할 실패 이후20년만...</td>\n",
              "      <td>2024</td>\n",
              "      <td>8</td>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "      <td>미 법무부 칼날에 구글 쪼개질 신세.</td>\n",
              "      <td>정부, 구글 반독점 정 조준 크롬 안드로이드 분리 요청 기업 분할 실패 이후 20년...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>42916 rows × 12 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eef5f184-0a18-4c4b-99e0-b277a07c815c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eef5f184-0a18-4c4b-99e0-b277a07c815c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eef5f184-0a18-4c4b-99e0-b277a07c815c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-24c5b769-bd37-499d-940b-cde1793356f7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-24c5b769-bd37-499d-940b-cde1793356f7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-24c5b769-bd37-499d-940b-cde1793356f7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_393d7d4b-12b8-492d-9ec4-ad73f3113202\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('final_매일경제')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_393d7d4b-12b8-492d-9ec4-ad73f3113202 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('final_매일경제');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "final_매일경제",
              "summary": "{\n  \"name\": \"final_\\ub9e4\\uc77c\\uacbd\\uc81c\",\n  \"rows\": 42916,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2822057,\n        \"min\": 2015000001,\n        \"max\": 2024003354,\n        \"num_unique_values\": 42909,\n        \"samples\": [\n          2017002671,\n          2015003286,\n          2020001132\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42870,\n        \"samples\": [\n          \"https://n.news.naver.com/mnews/article/009/0003942059?sid=101\",\n          \"https://n.news.naver.com/mnews/article/009/0003873438?sid=101\",\n          \"https://n.news.naver.com/mnews/article/009/0005337246?sid=101\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39163,\n        \"samples\": [\n          \"2015-08-07 13:56:00\",\n          \"2019-01-03 17:56:00\",\n          \"2016-12-20 20:48:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42532,\n        \"samples\": [\n          \"\\u2018\\uac24\\ub7ed\\uc2dc\\ub178\\ud2b87\\u2019 \\uacf5\\uac1c D-1\\u2026\\ud504\\ub9ac\\ubbf8\\uc5c4 \\uc2a4\\ub9c8\\ud2b8\\ud3f0 \\ub300\\uc804 \\ud3ec\\ubb38\",\n          \"[Hot-Line] \\\"\\uc0bc\\uc131\\uc804\\uae30, 3\\ubd84\\uae30 \\uc2e4\\uc801 \\uc2dc\\uc7a5\\ucd94\\uc815\\uce58 \\ub300\\ud3ed \\uc6c3\\ub3cc\\uc544\\\"\",\n          \"\\uc5f0\\uae30\\uae08, \\ubc18\\ub144\\ub9cc\\uc5d0 \\uc791\\ub144 \\uc2e4\\uc801 \\ub118\\uc5b4\\uc130\\ub2e4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42792,\n        \"samples\": [\n          \"\\ub9e4\\uacbd\\uc774 \\uc804\\ud558\\ub294 \\uc138\\uc0c1\\uc758 \\uc9c0\\uc2dd(\\ub9e4-\\uc138-\\uc9c0, 9\\uc6d4 21\\uc77c)1. \\ubb38\\uc7ac\\uc778 \\uc815\\ubd80\\uc758 '\\uc18c\\ub4dd\\uc8fc\\ub3c4 \\uc131\\uc7a5'\\uc774 \\uc5ed\\uc124\\uc801\\uc73c\\ub85c \\uc800\\uc18c\\ub4dd\\uce35\\uc758 \\uc77c\\uc790\\ub9ac\\ub97c \\ube7c\\uc557\\ub294 \\uc0ac\\ub840\\uac00 \\uc18d\\ucd9c\\ud568. \\uc815\\ubd80\\uc758 \\uc784\\uae08\\uc778\\uc0c1 \\uc555\\ubc15\\uc5d0 \\uc0dd\\uacc4\\uac00 \\uc5b4\\ub824\\uc6b4 \\uc790\\uc601\\uc5c5\\uc790\\ub4e4\\uc774 \\uace0\\uc6a9\\uc744 \\uc904\\uc774\\ub294 \\ubc29\\uc2dd\\uc73c\\ub85c \\ub300\\uc751\\ud558\\uace0 \\uc788\\uae30 \\ub54c\\ubb38\\uc784. \\uc800\\uc18c\\ub4dd \\uadfc\\ub85c\\uc790\\uac00 \\uc9d1\\uc911\\ub41c \\uc219\\ubc15\\u00b7\\uc74c\\uc2dd\\uc5c5 \\uace0\\uc6a9\\uc740 6\\uc6d4 \\uc774\\ud6c4 \\uc11d\\ub2ec \\uc5f0\\uc18d \\uac10\\uc18c\\ud568. \\u25b6\\ubc14\\ub85c\\uac00\\uae302. SK\\ud558\\uc774\\ub2c9\\uc2a4\\uac00 \\ucc38\\uc5ec\\ud55c '\\ud55c\\ubbf8\\uc77c \\uc5f0\\ud569'\\uc774 \\ub3c4\\uc2dc\\ubc14 \\ubc18\\ub3c4\\uccb4 \\uc0ac\\uc5c5 \\ubd80\\ubb38 \\uc778\\uc218\\uc790\\ub85c \\uacb0\\uc815\\ub428. \\ud55c\\ubbf8\\uc77c \\uc5f0\\ud569\\uc774 \\uc9c0\\ub09c 6\\uc6d4 \\uc6b0\\uc120\\ud611\\uc0c1\\ub300\\uc0c1\\uc790\\ub85c \\uc120\\uc815\\ub41c \\ud6c4 3\\uac1c\\uc6d4 \\ub9cc\\uc784. \\uc778\\uc218\\uac00\\uc561 \\uc57d 2\\uc870\\uc5d4 \\uc678\\uc5d0 \\uc124\\ube44\\ud22c\\uc790 \\uba85\\ubaa9\\uc73c\\ub85c \\uc5f0 4000\\uc5b5\\uc5d4\\uc744 \\ucd94\\uac00 \\uc81c\\uacf5\\ud558\\ub294 \\uc870\\uac74\\uc73c\\ub85c \\uc2b9\\uae30\\ub97c \\uc7a1\\uc74c. \\u25b6\\ubc14\\ub85c\\uac00\\uae30\\uae40\\uc0c1\\uc870 \\uacf5\\uc815\\uac70\\ub798\\uc704\\uc6d0\\uc7a5\\uc774 \\uc9c0\\ub09c 19\\uc77c \\uc624\\ud6c4 \\uc11c\\uc6b8 \\ucda9\\ubb34\\ub85c \\ub9e4\\uacbd\\ubbf8\\ub514\\uc5b4\\uc13c\\ud130\\uc5d0\\uc11c \\uc5f4\\ub9b0 '\\ub9e4\\uacbd \\uc774\\ucf54\\ub178\\ubbf8\\uc2a4\\ud2b8\\ud074\\ub7fd'\\uc5d0 \\ucc38\\uc11d\\ud574 \\uac15\\uc5f0\\uc744 \\ud558\\uace0 \\uc788\\ub2e4. \\uae40 \\uc704\\uc6d0\\uc7a5\\uc740 \\uc774\\ub0a0 \\uac15\\uc5f0\\uc5d0\\uc11c \\\"\\ub300\\uae30\\uc5c5\\uacfc 1\\ucc28 \\ud611\\ub825\\uc5c5\\uccb4 \\uac04 \\ubd88\\uacf5\\uc815\\uac70\\ub798\\ub294 \\ub9ce\\uc774 \\uac1c\\uc120\\ub410\\uc9c0\\ub9cc, 2\\ucc28\\u00b73\\ucc28 \\ud611\\ub825\\uc5c5\\uccb4\\uc5d0 \\ub300\\ud55c \\uac11\\uc9c8\\uc740 \\uac1c\\uc120\\ub418\\uc9c0 \\uc54a\\uace0 \\uc788\\uace0 \\ubc95\\ub960\\ub85c \\uac15\\uc81c\\ud558\\uae30\\ub3c4 \\uc5b4\\ub824\\uc6b4 \\ubb38\\uc81c\\\"\\ub77c\\uba70 \\\"\\uc6d0\\uc0ac\\uc5c5\\uc790\\uc778 \\ub300\\uae30\\uc5c5\\uc774 \\uc774 \\ubb38\\uc81c\\ub97c \\uac1c\\uc120\\ud558\\ub294 \\ubaa8\\ub378\\uc744 \\ub9cc\\ub4e4\\uc5b4\\uc92c\\uc73c\\uba74 \\ud55c\\ub2e4\\\"\\uace0 \\ub9d0\\ud588\\ub2e4. /\\uc0ac\\uc9c4=\\uae40\\ud638\\uc601 \\uae30\\uc7903. \\uae40\\uc0c1\\uc870 \\uacf5\\uc815\\uc704\\uc6d0\\uc7a5\\uc740 \\ub9e4\\uacbd \\uc774\\ucf54\\ub178\\ubbf8\\uc2a4\\ud2b8\\ud074\\ub7fd \\uac15\\uc5f0\\uc5d0\\uc11c \\\"\\ud604 \\uc815\\ubd80\\uac00 \\uacfc\\uac70\\ubcf4\\ub2e4 \\ud070 \\uc815\\ubd80\\uc778 \\uac83\\uc740 \\ub9de\\uc9c0\\ub9cc \\ubcc0\\ud654 \\ucd08\\uae30\\ub2e8\\uacc4\\uc758 \\uc624\\ubc84\\uc288\\ud305\\\"\\uc774\\ub77c\\uace0 \\ubc1d\\ud798. \\ub610 \\\"\\uc77c\\uc815 \\uc2dc\\uc810 \\uc774\\ud6c4 \\uc801\\uc808\\ud55c \\uade0\\ud615\\uc810\\uc744 \\ucc3e\\uc544\\uac08 \\uac83\\\"\\uc774\\ub77c\\uba70 \\\"\\uc2dc\\uc810\\uacfc \\uacbd\\ub85c\\ub97c \\uc798 \\uc124\\uc815\\ud574 \\uc2e4\\ud328\\ud558\\uc9c0 \\uc54a\\ub3c4\\ub85d \\ub178\\ub825\\ud558\\uaca0\\ub2e4\\\"\\uace0 \\uac15\\uc870\\ud568. \\u25b6\\ubc14\\ub85c\\uac00\\uae304. \\ubbfc\\uc8fc\\ub2f9\\uacfc \\uc815\\ubd80\\ub294 20\\uc77c \\ucd5c\\uc800\\uc784\\uae08 \\uc778\\uc0c1\\uc5d0 \\ub530\\ub978 \\uc18c\\uc0c1\\uacf5\\uc778\\u00b7\\uc790\\uc601\\uc5c5\\uc790 \\uacbd\\uc601\\ub09c \\uc644\\ud654 \\ubc29\\uc548\\uc744 \\uc758\\ub17c\\ud568. \\ud604\\uc7ac 9%\\uc778 \\uc0c1\\uac00 \\uc784\\ub300\\ub8cc \\uc778\\uc0c1\\ub960 \\uc0c1\\ud55c\\uc744 \\ub0ae\\ucd94\\uace0 \\uc0c1\\uac00 \\uc784\\ucc28\\uc778\\uc758 \\uacc4\\uc57d\\uac31\\uc2e0\\uccad\\uad6c\\uad8c \\ud589\\uc0ac\\uae30\\uac04\\uc744 10\\ub144\\uc73c\\ub85c \\uc5f0\\uc7a5\\ud558\\ub294 \\ubc29\\uc548 \\ub4f1\\uc774 \\uac70\\ub860\\ub428. \\u25b6\\ubc14\\ub85c\\uac00\\uae305. \\ubb38\\uc7ac\\uc778 \\ub300\\ud1b5\\ub839\\uc774 \\uc6d4\\uac00 \\uae08\\uc735\\uc804\\ubb38\\uac00 200\\uc5ec\\uba85\\uc744 \\ucd08\\uccad\\ud574 \\uc124\\uba85\\ud68c\\ub97c \\uac1c\\ucd5c\\ud568. \\ubb38 \\ub300\\ud1b5\\ub839\\uc740 \\\"\\ubd81\\ud575\\ub9ac\\uc2a4\\ud06c\\uc5d0\\ub3c4 \\ud55c\\uad6d \\uacbd\\uc81c\\uc640 \\uae08\\uc735\\uc2dc\\uc7a5\\uc740 \\uc548\\uc815\\uc801\\\"\\uc774\\ub77c\\uba70 \\ubcc0\\ud568\\uc5c6\\ub294 \\uc2e0\\ub8b0\\uc640 \\ud22c\\uc790\\ub97c \\uc694\\uccad\\ud568. \\ubb38 \\ub300\\ud1b5\\ub839\\uc740 21\\uc77c \\ud55c\\ubbf8\\uc77c \\uc815\\uc0c1\\ud68c\\ub2f4\\uc744 \\ub05d\\uc73c\\ub85c \\ub274\\uc695 \\uc21c\\ubc29 \\uc77c\\uc815\\uc744 \\ub9c8\\uce58\\uac8c \\ub428. \\u25b6\\ubc14\\ub85c\\uac00\\uae306. \\uae40\\uba85\\uc218 \\ub300\\ubc95\\uc6d0\\uc7a5 \\ud6c4\\ubcf4\\uc758 \\uc784\\uba85\\ub3d9\\uc758\\uc548 \\ud45c\\uacb0\\uc744 \\ud558\\ub8e8 \\uc55e\\ub454 20\\uc77c \\ubbfc\\uc8fc\\ub2f9 \\uc758\\uc6d0 121\\uba85 \\uc804\\uc6d0\\uc774 \\ud22c\\uc785\\ub3fc \\uc57c\\ub2f9\\uc744 \\uc124\\ub4dd\\ud568. \\uad6d\\ubbfc\\uc758\\ub2f9\\uc5d0\\uc11c 30\\uba85\\uc758 \\ucc2c\\uc131\\ud45c\\ub97c \\uc5bb\\uc5b4\\uc57c \\uc548\\uc815\\uc801 \\uc758\\uacb0\\uc815\\uc871\\uc218\\ub97c \\ud655\\ubcf4\\ud560 \\uc804\\ub9dd\\uc784. \\uc790\\uc720\\ud55c\\uad6d\\ub2f9\\uc740 \\uae40 \\ud6c4\\ubcf4\\uc790 \\uc784\\uba85\\ub3d9\\uc758\\uc548 \\ubd80\\uacb0\\uc744 \\ub2f9\\ub860\\uc73c\\ub85c \\ucc44\\ud0dd\\ud568. \\u25b6\\ubc14\\ub85c\\uac00\\uae30\\uae40\\uba85\\uc218 \\ub300\\ubc95\\uc6d0\\uc7a5 \\ud6c4\\ubcf4\\uc790\\uac00 \\uc784\\uba85\\ub3d9\\uc758\\uc548 \\uad6d\\ud68c \\ubcf8\\ud68c\\uc758 \\ud45c\\uacb0\\uc744 \\ud558\\ub8e8 \\uc55e\\ub454 20\\uc77c \\uc624\\uc804 \\uc11c\\uc6b8 \\uc11c\\ucd08\\uad6c \\uc0ac\\ubc95\\ubc1c\\uc804\\uc7ac\\ub2e8\\uc5d0 \\ub9c8\\ub828\\ub41c \\uc0ac\\ubb34\\uc2e4\\ub85c \\ub4e4\\uc5b4\\uac00\\uace0 \\uc788\\ub2e4. /\\uc0ac\\uc9c4=\\uc5f0\\ud569\\ub274\\uc2a47. \\uc0bc\\uc131\\uc804\\uc790 \\uc218\\ub1cc\\ubd80\\uac00 \\ucd9c\\ub3d9\\ud574 \\uc2e4\\ub9ac\\ucf58\\ubc38\\ub9ac\\uc5d0\\uc11c '\\ud14c\\ud06c\\ud3ec\\ub7fc 2017'\\uc744 \\uc5f4\\uace0 \\ud604\\uc9c0 \\uc778\\uc7ac\\ub4e4\\uacfc IT\\uc758 \\ubbf8\\ub798\\uc5d0 \\ub300\\ud574 \\ud1a0\\ub860\\ud568. \\uad6c\\uae00\\u00b7\\uc560\\ud50c \\ub4f1 \\ud604\\uc9c0 IT\\uae30\\uc5c5 \\uc5d4\\uc9c0\\ub2c8\\uc5b4\\uc640 \\ub514\\uc790\\uc774\\ub108 \\ub4f1 100\\uc5ec\\uba85\\uc774 \\ucc38\\uc11d\\ud574 \\uc778\\uacf5\\uc9c0\\ub2a5\\u00b7\\uc0ac\\ubb3c\\uc778\\ud130\\ub137\\u00b7\\uc804\\uc7a5\\uae30\\uc220 \\ub4f1\\uc5d0 \\ub300\\ud574 \\uc0bc\\uc131 \\uc0ac\\uc7a5\\ub2e8\\uacfc \\uc9c8\\uc758\\uc751\\ub2f5 \\uc2dc\\uac04\\uc744 \\uac00\\uc9d0. \\u25b6\\ubc14\\ub85c\\uac00\\uae308. \\uc9c0\\ub09c 7\\uc77c 100\\ub144\\ub9cc\\uc758 \\ucd5c\\ub300\\uaddc\\ubaa8 \\uc9c0\\uc9c4\\uc774 \\ubc1c\\uc0dd\\ud55c \\uba55\\uc2dc\\ucf54\\uc5d0\\uc11c \\ub2e4\\uc2dc 7.1\\uaddc\\ubaa8 \\uac15\\uc9c4\\uc774 \\ubc1c\\uc0dd\\ud574 200\\uba85 \\uc774\\uc0c1\\uc774 \\uc0ac\\ub9dd\\ud568. \\uc0ac\\uc0c1\\uc790 \\uc218\\uac00 \\uc810\\uc810 \\ub298\\uc5b4\\ub098\\uba70 \\ud53c\\ud574\\uaddc\\ubaa8\\uac00 \\ucee4\\uc9c0\\uace0 \\uc788\\uace0 \\uc5ec\\uc9c4\\uc5d0 \\ub300\\ud55c \\ubd88\\uc548\\uac10\\uc73c\\ub85c \\uba55\\uc2dc\\ucf54\\ub294 \\ud328\\ub2c9\\uc5d0 \\ube60\\uc9d0. \\ud55c\\uad6d\\uc778 1\\uba85\\uc774 \\uc0ac\\ub9dd\\ud55c \\uac83\\uc73c\\ub85c \\ud655\\uc778\\ub428. \\u25b6\\ubc14\\ub85c\\uac00\\uae30[\\ub9e4\\uc77c\\uacbd\\uc81c \\uacf5\\uc2dd \\ud398\\uc774\\uc2a4\\ubd81] [\\uc624\\ub298\\uc758 \\uc778\\uae30\\ub274\\uc2a4] [\\ub9e4\\uacbd \\ud504\\ub9ac\\ubbf8\\uc5c4][\\u24d2 \\ub9e4\\uc77c\\uacbd\\uc81c & mk.co.kr, \\ubb34\\ub2e8\\uc804\\uc7ac \\ubc0f \\uc7ac\\ubc30\\ud3ec \\uae08\\uc9c0]\",\n          \"6\\uac1c \\ubd84\\uc57c 12\\uac1c \\uacfc\\uc81c \\uc120\\uc815\\ud574\\uc62c\\ud574 152\\uc5b5\\uc6d0 \\uc5f0\\uad6c\\ube44 \\uc9c0\\uc6d0\\uc0bc\\uc131\\uc804\\uc790\\uac00 \\uad6d\\uac00\\uc801\\uc73c\\ub85c \\uc5f0\\uad6c\\uac00 \\ud544\\uc694\\ud55c \\ubbf8\\ub798 \\uacfc\\ud559\\uae30\\uc220 \\ubd84\\uc57c\\ub97c \\ubc1c\\uc804\\uc2dc\\ud0a4\\uae30 \\uc704\\ud574 \\uc5b4\\ub4dc\\ubc34\\uc2a4\\ud2b8 \\uc778\\uacf5\\uc9c0\\ub2a5(AI), \\ucc28\\uc138\\ub300 \\uc554\\ud638 \\uc2dc\\uc2a4\\ud15c \\ub4f1 6\\uac1c \\ubd84\\uc57c\\uc5d0 \\ub300\\ud55c \\uc9c0\\uc6d0\\uc5d0 \\ub098\\uc120\\ub2e4. 15\\uc77c \\uc0bc\\uc131\\uc804\\uc790\\ub294 2021\\ub144 '\\uc0bc\\uc131 \\ubbf8\\ub798\\uae30\\uc220 \\uc721\\uc131\\uc0ac\\uc5c5' \\uc9c0\\uc815 \\ud14c\\ub9c8 \\uc5f0\\uad6c \\uc9c0\\uc6d0 \\uacfc\\uc81c 12\\uac1c\\uc5d0 \\uc5f0\\uad6c\\ube44 \\ucd1d 152\\uc5b51000\\ub9cc\\uc6d0\\uc744 \\uc9c0\\uc6d0\\ud55c\\ub2e4\\uace0 \\ubc1d\\ud614\\ub2e4. \\uc0bc\\uc131 \\ubbf8\\ub798\\uae30\\uc220 \\uc721\\uc131\\uc0ac\\uc5c5\\uc740 \\uc6b0\\ub9ac\\ub098\\ub77c \\ubbf8\\ub798\\ub97c \\ucc45\\uc784\\uc9c8 \\uacfc\\ud559\\uae30\\uc220 \\uc721\\uc131\\u00b7\\uc9c0\\uc6d0\\uc744 \\ubaa9\\ud45c\\ub85c \\uc0bc\\uc131\\uc804\\uc790\\uac00 2013\\ub144\\ubd80\\ud130 1\\uc8705000\\uc5b5\\uc6d0\\uc744 \\ucd9c\\uc5f0\\ud574 \\uc2dc\\ud589\\ud558\\uace0 \\uc788\\ub294 \\uc5f0\\uad6c \\uc9c0\\uc6d0 \\uacf5\\uc775\\uc0ac\\uc5c5\\uc774\\ub2e4.\\ub9e4\\ub144 \\uc0c1\\u00b7\\ud558\\ubc18\\uae30\\uc5d0 \\uae30\\ucd08\\uacfc\\ud559, \\uc18c\\uc7ac, \\uc815\\ubcf4\\ud1b5\\uc2e0\\uae30\\uc220 \\ubd84\\uc57c\\uc5d0\\uc11c \\uc9c0\\uc6d0\\ud560 \\uacfc\\uc81c\\ub97c \\uc120\\uc815\\ud55c\\ub2e4. \\ucd94\\uac00\\ub85c 2014\\ub144\\ubd80\\ud130\\ub294 \\ub9e4\\ub144 \\ud55c \\ucc28\\ub840 '\\uc9c0\\uc815 \\ud14c\\ub9c8 \\uacfc\\uc81c \\uacf5\\ubaa8'\\ub97c \\ud1b5\\ud574 \\uad6d\\uac00\\uc801\\uc73c\\ub85c \\ud544\\uc694\\ud55c \\ubbf8\\ub798 \\uae30\\uc220 \\ubd84\\uc57c\\ub97c \\uc9c0\\uc815\\ud574 \\uc5f0\\uad6c\\ub97c \\uc9c0\\uc6d0\\ud558\\uace0 \\uc788\\ub2e4.\\uc62c\\ud574 \\uc9c0\\uc815\\ub41c \\ud14c\\ub9c8 \\uc5f0\\uad6c \\uc9c0\\uc6d0 \\uacfc\\uc81c\\ub294 '\\uc5b4\\ub4dc\\ubc34\\uc2a4\\ud2b8 AI' '\\ucc28\\uc138\\ub300 \\uc554\\ud638 \\uc2dc\\uc2a4\\ud15c' 'B(Beyond)5G&6G' '\\ub85c\\ubd07' '\\ucc28\\uc138\\ub300 \\ub514\\uc2a4\\ud50c\\ub808\\uc774' '\\ubc18\\ub3c4\\uccb4 \\uc18c\\uc790 \\ubc0f \\uacf5\\uc815' \\ub4f1 \\ucd1d 6\\uac1c \\ubd84\\uc57c\\uc5d0\\uc11c 12\\uac1c\\ub97c \\uc120\\uc815\\ud588\\ub2e4. \\uc5b4\\ub4dc\\ubc34\\uc2a4\\ud2b8 AI\\uc640 \\ucc28\\uc138\\ub300 \\uc554\\ud638 \\uc2dc\\uc2a4\\ud15c\\uc740 \\uc774\\ubc88\\uc5d0 \\uc0c8\\ub86d\\uac8c \\uc120\\uc815\\ub41c \\ubd84\\uc57c\\ub2e4. \\uc5b4\\ub4dc\\ubc34\\uc2a4\\ud2b8 AI\\ub294 \\uace0\\ucc28\\uc6d0\\uc758 \\ub17c\\ub9ac \\ucd94\\ub860\\uae4c\\uc9c0 \\uac00\\ub2a5\\ud558\\uac8c \\ud558\\ub294 AI\\ub85c \\uc774 \\ubd84\\uc57c\\uc5d0\\uc11c\\ub294 \\ud669\\ub3c4\\uc2dd \\uc5f0\\uc138\\ub300 \\uc804\\uae30\\uc804\\uc790\\uacf5\\ud559\\ubd80 \\uad50\\uc218\\uc758 '\\uc21c\\ud658 \\ucd94\\ub860\\ud615 \\uc778\\uacf5\\uc9c0\\ub2a5-\\uc790\\uae30 \\uc9c8\\uc758\\uc751\\ub2f5 \\uae30\\ubc18 \\uc790\\ub3d9 \\uc758\\ub8cc\\uc9c4\\ub2e8 \\uae30\\uc220' \\ub4f1 \\ucd1d 2\\uac1c \\uacfc\\uc81c\\uac00 \\uc120\\uc815\\ub410\\ub2e4. \\ucc28\\uc138\\ub300 \\uc554\\ud638 \\uc2dc\\uc2a4\\ud15c \\ubd84\\uc57c\\uc5d0\\uc11c\\ub294 \\uc1a1\\uc6a9\\uc218 \\uc11c\\uc6b8\\ub300 \\ucef4\\ud4e8\\ud130\\uacf5\\ud559\\ubd80 \\uad50\\uc218\\uc758 '\\ub2e4\\uc790\\uac04 \\uadfc\\uc0ac\\uacc4\\uc0b0 \\uc554\\ud638 \\uc6d0\\ucc9c\\uae30\\uc220 \\uac1c\\ubc1c' \\uacfc\\uc81c\\uac00 \\uc120\\uc815\\ub410\\ub2e4. \\ud074\\ub77c\\uc6b0\\ub4dc\\uc5d0 \\ubcf4\\uad00\\ub41c \\uc790\\ub8cc\\uc758 \\ube44\\ubc00\\uc131\\uc740 \\uc720\\uc9c0\\ud558\\uba74\\uc11c \\ub370\\uc774\\ud130\\ub97c \\ubd84\\uc11d\\ud560 \\uc218 \\uc788\\ub294 \\uae30\\uc220\\uc774\\ub2e4.[\\ubc15\\uc7ac\\uc601 \\uae30\\uc790]\",\n          \"\\uc62c\\ud574 1\\ubd84\\uae30 \\uc2e4\\uc801\\uc744 \\ubc1c\\ud45c\\ud55c \\uc0c1\\uc7a5\\uc0ac \\uc911 \\uc2dc\\uc7a5\\uc804\\ub9dd\\uce58(\\ucee8\\uc13c\\uc11c\\uc2a4)\\ub97c \\ud6e8\\uc52c \\ub6f0\\uc5b4\\ub118\\ub294 \\u2018\\uc5b4\\ub2dd \\uc11c\\ud504\\ub77c\\uc774\\uc988\\u2019\\ub97c \\uae30\\ub85d\\ud55c \\uacf3\\uc774 40%\\uc5d0 \\ub2ec\\ud558\\ub294 \\uac83\\uc73c\\ub85c \\uc870\\uc0ac\\ub410\\ub2e4. \\ubd80\\uc815\\ud655\\ud55c \\uc2e4\\uc801 \\ucd94\\uc815\\uc73c\\ub85c \\uc5b4\\ub2dd \\uc1fc\\ud06c\\u00b7\\uc11c\\ud504\\ub77c\\uc774\\uc988\\uac00 \\ubc1c\\uc0dd\\ud588\\ub358 \\uc774\\uc804 \\uc2e4\\uc801\\uc2dc\\uc98c\\uacfc \\ube44\\uad50\\ud574 \\ub69c\\ub837\\ud55c \\uc774\\uc775\\uc2f8\\uc774\\ud074\\uc758 \\uac1c\\uc120\\uc774 \\ub098\\ud0c0\\ub098\\uace0 \\uc788\\uc5b4 \\uae0d\\uc815\\uc801\\uc774\\ub77c\\ub294 \\ubd84\\uc11d\\uc774\\ub2e4.29\\uc77c \\ub9e4\\uc77c\\uacbd\\uc81c\\uac00 \\uc5d0\\ud504\\uc564\\uac00\\uc774\\ub4dc\\uc5d0 \\uc758\\ub8b0\\ud574 \\uc774\\ub0a0\\uae4c\\uc9c0 \\uc2e4\\uc801\\uc744 \\ubc1c\\ud45c\\ud55c \\uc0c1\\uc7a5\\uc0ac\\ub4e4 \\uc911 \\uc99d\\uad8c\\uc0ac\\ub4e4\\uc758 \\ub9e4\\ucd9c\\uc561\\uacfc \\uc601\\uc5c5\\uc774\\uc775 \\ucd94\\uc815\\uce58\\uac00 3\\uac1c \\uc774\\uc0c1 \\uc874\\uc7ac\\ud558\\ub294 70\\uacf3\\uc758 \\uc2e4\\uc801\\uc744 \\ubd84\\uc11d\\ud55c \\uacb0\\uacfc\\ub2e4. \\ucd94\\uc815\\uce58\\uc640 \\uc2e4\\uc801\\ubc1c\\ud45c\\uce58\\ub97c \\ubd84\\uc11d\\ud55c \\uacb0\\uacfc \\uc5b4\\ub2dd \\uc11c\\ud504\\ub77c\\uc774\\uc988\\ub294 28\\uacf3, \\uc5b4\\ub2dd \\uc1fc\\ud06c\\ub97c \\ub0b8 \\uae30\\uc5c5\\uc740 14\\uacf3\\uc73c\\ub85c \\uac01\\uac01 \\uc9d1\\uacc4\\ub410\\ub2e4. \\uc5b4\\ub2dd \\uc11c\\ud504\\ub77c\\uc774\\uc988 \\uae30\\uc5c5 \\ube44\\uc911\\uc740 40.0%\\ub97c \\ucc28\\uc9c0\\ud55c \\ubc18\\uba74 \\uc5b4\\ub2dd \\uc1fc\\ud06c \\uae30\\uc5c5 \\ube44\\uc911\\uc740 20%\\uc5d0 \\ubd88\\uacfc\\ud574 \\ub450\\ubc30 \\ucc28\\uc774\\uac00 \\ub0ac\\ub2e4.\\ub300\\uac1c \\uc99d\\uad8c\\uc0ac\\ub4e4\\uc774 \\ub0b4\\ub193\\ub294 \\uc2e4\\uc801 \\uc804\\ub9dd\\uce58 \\ud3c9\\uade0(\\ucee8\\uc13c\\uc11c\\uc2a4)\\ubcf4\\ub2e4 \\uae30\\uc5c5\\uc758 \\uc2e4\\uc81c \\uc601\\uc5c5\\uc774\\uc775\\uc774 10% \\uc774\\uc0c1 \\ub9ce\\uc73c\\uba74 \\uc5b4\\ub2dd \\uc11c\\ud504\\ub77c\\uc774\\uc988\\ub85c, 10% \\uc774\\uc0c1 \\uc801\\uc73c\\uba74 \\uc5b4\\ub2dd \\uc1fc\\ud06c\\ub85c \\uac01\\uac01 \\ubd84\\ub958\\ud55c\\ub2e4. \\ud751\\uc790\\u00b7\\uc801\\uc790 \\uc804\\ud658\\ud558\\uac70\\ub098 \\uc801\\uc790\\ud3ed\\uc774 \\ud655\\ub300\\ub41c \\uacbd\\uc6b0\\ub3c4 \\uc5b4\\ub2dd\\uc1fc\\ud06c\\ub85c \\ubd84\\ub958\\ub418\\ub294 \\uac8c \\uc77c\\ubc18\\uc801\\uc774\\ub2e4. \\uc544\\uc9c1 \\uc2e4\\uc801\\uc2dc\\uc98c \\ucd08\\ubc18\\uc774\\uc9c0\\ub9cc \\uc0bc\\uc131\\uc804\\uc790, LG\\uc804\\uc790 \\ub4f1 \\uc5c5\\uc885 \\ub300\\ud45c\\uc8fc\\ub4e4\\uc758 \\uc2e4\\uc801\\uc774 \\ub300\\ubd80\\ubd84 \\uacf5\\uac1c\\ub3fc \\uc0c1\\uc7a5\\uc0ac\\ub4e4\\uc758 1\\ubd84\\uae30 \\uc2e4\\uc801\\uc744 \\uc5b4\\ub290\\uc815\\ub3c4 \\uac00\\ub984\\ud560 \\uc218 \\uc788\\ub2e4.1\\ubd84\\uae30 \\uac00\\uc7a5 \\uc5b4\\ub2dd \\uc11c\\ud504\\ub77c\\uc774\\uc988\\ub97c \\uae30\\ub85d\\ud55c \\uc885\\ubaa9 \\uc911 \\ub208\\uc5d0 \\ub744\\ub294 \\uc885\\ubaa9\\uc740 LG\\uc0dd\\uba85\\uacfc\\ud559\\uc774\\ub2e4. LG\\uc0dd\\uba85\\uacfc\\ud559\\uc758 1\\ubd84\\uae30 \\uc601\\uc5c5\\uc774\\uc775\\uc740 171\\uc5b5\\uc6d0\\uc73c\\ub85c \\ucee8\\uc13c\\uc11c\\uc2a4 37\\uc5b5\\uc6d0\\ubcf4\\ub2e4 354.5% \\ub192\\uc558\\ub2e4. \\ubc30\\uae30\\ub2ec \\uc2e0\\ud55c\\uae08\\uc735\\ud22c\\uc790 \\uc5f0\\uad6c\\uc6d0\\uc740 \\u201c1\\ubd84\\uae30 \\uc5b4\\ub2dd \\uc11c\\ud504\\ub77c\\uc774\\uc988\\ub294 \\uae30\\uc220 \\uc218\\ucd9c\\ub8cc(150\\uc5b5\\uc6d0) \\uc720\\uc785\\uacfc \\uc8fc\\ub825 \\ud488\\ubaa9 \\ub9e4\\ucd9c \\ud638\\uc870 \\ub355\\ubd84\\u201d\\uc774\\ub77c\\uba70 \\u201c\\ub2f9\\ub1e8\\ubcd1 \\uce58\\ub8cc\\uc81c \\u2018\\uc81c\\ubbf8\\uae00\\ub85c\\u2019 \\ud310\\ub9e4\\ub7c9\\uc774 \\uc804\\ub144\\ub3d9\\uae30\\ub300\\ube44 121.3% \\uc99d\\uac00\\ud558\\ub294 \\ub4f1 \\ub192\\uc740 \\uc131\\uc7a5\\uc744 \\uc9c0\\uc18d\\ud558\\uace0 \\uc788\\ub2e4\\u201d\\uace0 \\ubd84\\uc11d\\ud588\\ub2e4. \\uc2e0\\ud55c\\uae08\\uc735\\ud22c\\uc790\\ub294 LG\\uc0dd\\uba85\\uacfc\\ud559\\uc758 \\ubaa9\\ud45c\\uc8fc\\uac00\\ub97c \\uae30\\uc874 8\\ub9cc\\uc6d0\\uc5d0\\uc11c 8\\ub9cc5000\\uc6d0\\uc73c\\ub85c \\uc0c1\\ud5a5\\ud588\\ub2e4.\\ud48d\\uc0b0\\ub3c4 393\\uc5b5\\uc6d0\\uc758 \\uc601\\uc5c5\\uc774\\uc775\\uc744 \\ub0b4\\uba70 \\ucee8\\uc13c\\uc11c\\uc2a4(182\\uc5b5\\uc6d0)\\ub97c 116.1% \\uc0c1\\ud68c\\ud588\\ub2e4. \\uc804\\ub144\\ub3d9\\uae30 \\uc601\\uc5c5\\uc774\\uc775\\uacfc \\ube44\\uad50\\ud558\\uba74 588% \\uc99d\\uac00\\ud55c \\uae08\\uc561\\uc774\\ub2e4. HMC\\ud22c\\uc790\\uc99d\\uad8c\\uc740 \\ud48d\\uc0b0\\uc758 2\\ubd84\\uae30 \\uc601\\uc5c5\\uc774\\uc775\\uc744 482\\uc5b5\\uc6d0\\uc73c\\ub85c \\ucd94\\uc815\\ud558\\uba70 1\\ubd84\\uae30 \\uc0c1\\uc2b9\\uc138\\ub97c \\uc774\\uc5b4\\ub098\\uac08 \\uac83\\uc73c\\ub85c \\uc608\\uce21\\ud588\\ub2e4. 2\\ubd84\\uae30 \\uc131\\uc218\\uae30\\uc5d0 \\uc811\\uc5b4\\ub4e4\\uc5b4 \\ud310\\ub9e4\\ub7c9\\uc774 \\uc99d\\uac00\\ud558\\uace0 \\uad6c\\ub9ac\\uac00\\uaca9 \\uc0c1\\uc2b9\\uc73c\\ub85c \\uc778\\ud55c \\uc774\\uc775\\uc774 \\uc608\\uc0c1\\ub41c\\ub2e4\\ub294 \\uac83\\uc774\\ub2e4.\\uc870\\uc120\\uc5c5\\uacc4 \\uad6c\\uc870\\uc870\\uc815 \\ubd88\\uc548\\uac10\\uc774 \\uac00\\uc911\\ub410\\ub358 \\ud604\\ub300\\uc911\\uacf5\\uc5c5\\uc740 \\uc608\\uc0c1\\uc678\\ub85c \\ucee8\\uc13c\\uc11c\\uc2a4(1427\\uc5b5\\uc6d0)\\uc744 127.9% \\uc6c3\\ub3c4\\ub294 3252\\uc5b5\\uc6d0\\uc758 \\uc601\\uc5c5\\uc774\\uc775\\uc744 \\ub0c8\\ub2e4. \\ud604\\ub300\\ubbf8\\ud3ec\\uc870\\uc120\\ub3c4 576\\uc5b5\\uc6d0\\uc758 \\uc601\\uc5c5\\uc774\\uc775\\uc744 \\uae30\\ub85d\\ud574 \\uc99d\\uad8c\\uac00 \\uc804\\ub9dd\\uce58 214\\uc5b5\\uc6d0\\uc744 \\ub450 \\ubc30\\uc774\\uc0c1 \\uc55e\\uc9c8\\ub800\\ub2e4. \\ud604\\ub300\\ub85c\\ud15c\\uc740 \\ucee8\\uc13c\\uc11c\\uc2a4\\ub97c 77.5% \\uc6c3\\ub3cc\\uc558\\uace0 KT&G\\ub294 39.6%, SK\\uc774\\ub178\\ubca0\\uc774\\uc158\\uc740 25.9%, \\uae30\\uc544\\ucc28\\ub294 19.2%, \\ub300\\ub9bc\\uc0b0\\uc5c5\\uc740 16.9% \\uc6c3\\ub3cc\\uc558\\ub2e4.\\ubc18\\uba74 \\uc2e4\\uc801\\uac1c\\uc120 \\ud750\\ub984\\uc18d\\uc5d0\\uc11c\\ub3c4 \\uc5b4\\ub2dd\\uc1fc\\ud06c\\ub97c \\ub0b8 \\uae30\\uc5c5\\ub4e4\\uc774 \\ub354\\ub7ec \\uc788\\ub2e4. 727\\uc5b5\\uc6d0\\uc758 \\uc601\\uc5c5\\uc774\\uc775\\uc774 \\uc608\\uc0c1\\ub410\\ub358 \\uc0bc\\uc131\\ubb3c\\uc0b0\\uc740 \\uc624\\ud788\\ub824 4348\\uc5b5\\uc6d0 \\uc601\\uc5c5\\uc190\\uc2e4\\uc744 \\uae30\\ub85d\\ud588\\ub2e4. \\uc724\\ud0dc\\ud638 \\ud55c\\uad6d\\ud22c\\uc790\\uc99d\\uad8c \\uc5f0\\uad6c\\uc6d0\\uc740 \\u201c\\uc190\\uc2e4\\uc744 \\ubc18\\uc601\\ud55c \\ud574\\uc678\\ud504\\ub85c\\uc81d\\ud2b8 \\uc0c1\\ub2f9\\uc218\\ub294 \\uc644\\uacf5 \\uc608\\uc815\\uc77c\\uc774 2017~2018\\ub144\\uc5d0 \\uac78\\uccd0 \\uc788\\uc5b4 \\ud5a5\\ud6c4 \\ub098\\ub220 \\ubc18\\uc601\\ud560 \\uc218 \\uc788\\uc5c8\\ub294 \\ub370 \\uc774\\ubc88 1\\ubd84\\uae30 \\uc2e4\\uc801\\uc740 \\uc758\\uc678\\uc758 \\uacb0\\uacfc\\u201d\\ub77c\\uba70 \\u201c\\ud5a5\\ud6c4 \\uac01 \\uc0ac\\uc5c5\\ubd80 \\uac1c\\ud3b8\\uacfc \\uc9c0\\uc8fc \\uc804\\ud658 \\uc774\\uc804 \\uac01 \\uc0ac\\uc5c5\\ubd80\\ubcc4 \\ubd80\\ub2f4\\uc774 \\ub418\\ub294 \\uc7a0\\uc7ac\\uc190\\uc2e4\\uc744 \\uc801\\uadf9\\uc801\\uc73c\\ub85c \\uc120\\ubc18\\uc601\\ud55c \\uac83\\uc73c\\ub85c \\ud310\\ub2e8\\ub41c\\ub2e4\\u201d\\uace0 \\ub9d0\\ud588\\ub2e4.\\ud604\\ub300\\uc704\\uc544\\ub294 800\\uc5b5\\uc6d0\\uc758 \\uc601\\uc5c5\\uc774\\uc775\\uc744 \\uae30\\ub85d\\ud574 \\ucee8\\uc13c\\uc11c\\uc2a4(1149\\uc5b5\\uc6d0)\\ub97c 30.4% \\ud558\\ud68c\\ud588\\uace0, \\uc0bc\\uc131\\uc804\\uae30\\ub294 34.4%, \\ud604\\ub300\\uc81c\\ucca0\\uc740 18.1% \\uc529 \\ucee8\\uc13c\\uc11c\\uc2a4\\ub97c \\uac01\\uac01 \\ubc11\\ub3cc\\uc558\\ub2e4. \\ud55c\\ubbf8\\uc57d\\ud488\\uc740 \\uc804\\ub9dd\\uce58(854\\uc5b5\\uc6d0)\\ub97c 73.6% \\ud558\\ud68c\\ud588\\ub294\\ub370, \\ud68c\\uacc4\\uc778\\uc2dd \\uc0c1\\uc758 \\ubb38\\uc81c\\ub97c \\uc81c\\uc678\\ud558\\uba74 1\\ubd84\\uae30 \\uc601\\uc5c5\\uc774\\uc775 226\\uc5b5\\uc6d0\\uc740 \\ub300\\uccb4\\ub85c \\uc804\\ub9dd\\uacfc \\uc77c\\uce58\\ud55c\\ub2e4\\ub294 \\ud3c9\\uac00\\uac00 \\uc9c0\\ubc30\\uc801\\uc774\\ub2e4.\\uc791\\ub144 11\\uc6d4 \\uc0ac\\ub178\\ud53c\\uc5d0 \\uae30\\uc220 \\uc218\\ucd9c\\ub85c \\uc720\\uc785\\ub41c \\uacc4\\uc57d\\uae08 5200\\uc5b5\\uc6d0 \\uc911 \\uc791\\ub144 4\\ubd84\\uae30\\uc5d0 \\uc778\\uc2dd(2556\\uc5b5)\\ub418\\uace0 \\ub0a8\\uc740 2644\\uc5b5\\uc6d0\\uc744 \\uc62c\\ud574 \\uc804\\ubd80 \\uc778\\uc2dd\\ud558\\uc9c0 \\uc54a\\uace0 3\\ub144 \\ub3d9\\uc548 \\ubd84\\ud560 \\uc778\\uc2dd\\ud588\\uae30 \\ub54c\\ubb38\\uc774\\ub2e4. \\ud55c\\ud3b8 1\\ubd84\\uae30 \\uc2e4\\uc801\\ubc1c\\ud45c\\ub294 \\ub2e4\\ub978 \\ubd84\\uae30\\uc5d0 \\ube44\\ud574 \\uc815\\ud655\\ub3c4\\uac00 \\ub192\\uc740 \\ud3b8\\uc774\\uc5b4\\uc11c \\uc62c 1\\ubd84\\uae30 \\uc5b4\\ub2dd\\uc2dc\\uc98c\\uc774 \\ud2b9\\ud788 \\uae0d\\uc815\\uc801\\uc774\\ub77c\\ub294 \\ubd84\\uc11d\\ub3c4 \\ub098\\uc654\\ub2e4. \\uc720\\uc548\\ud0c0\\uc99d\\uad8c\\uc5d0 \\ub530\\ub974\\uba74 2011~2015\\ub144 \\ub3d9\\uc548 1\\ubd84\\uae30 \\uc804\\ub9dd\\uce58 \\ub2ec\\uc131\\ub960\\uc740 94.7%\\ub85c 4\\ubd84\\uae30(77.0%) \\ub4f1 \\ub2e4\\ub978 \\ubd84\\uae30\\ubcf4\\ub2e4 \\uc801\\uc911\\ub960\\uc774 \\ub192\\uc740 \\uac83\\uc73c\\ub85c \\ub098\\ud0c0\\ub0ac\\ub2e4.[\\uae40\\ud0dc\\uc900 \\uae30\\uc790][\\u24d2 \\ub9e4\\uc77c\\uacbd\\uc81c & mk.co.kr, \\ubb34\\ub2e8\\uc804\\uc7ac \\ubc0f \\uc7ac\\ubc30\\ud3ec \\uae08\\uc9c0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 2015,\n        \"max\": 2024,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          2023,\n          2016,\n          2020\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          11,\n          10,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1,\n        \"max\": 31,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          28,\n          16,\n          24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hour\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 23,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          0,\n          6,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"minute\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18,\n        \"min\": 0,\n        \"max\": 59,\n        \"num_unique_values\": 60,\n        \"samples\": [\n          3,\n          46,\n          44\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42423,\n        \"samples\": [\n          \"\\ub9c8\\ub8e8\\uce74\\ud3ab \\ucd5c\\uc801\\uc758 \\uccad\\uc18c \\ubaa8\\ub4dc \\uc2a4\\uc2a4\\ub85c \\ucc3e\\uc544.\",\n          \"\\ud30c\\uc778\\ud14d 4~5\\uc77c \\uacf5\\ubaa8.\",\n          \"\\ub77c\\uc628\\ud14c\\ud06c, \\uae30\\uc220\\uc131 \\ud3c9\\uac00 \\ub3cc\\uc785 \\ucf54\\uc2a4\\ub2e5 \\uc804 \\ubcf8\\uaca9\\ud654.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_body\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42777,\n        \"samples\": [\n          \"\\uc678\\uad6d\\uc778 \\uc21c\\ub9e4\\uc218 \\ubab0\\ub9ac\\uba70 6\\ub144 \\ub9cc\\uc5d0 \\uc0ac\\uc0c1 \\ucd5c\\uace0\\uce58 \\uacbd\\uc2e0 \\ucf54\\uc2a4\\ud53c\\uac00 2240\\uc120\\uc744 \\ubc1f\\uc73c\\uba70 \\uc0ac\\uc0c1 \\ucd5c\\uace0\\uce58 \\uae30\\ub85d\\uc744 \\ub2e4\\uc2dc \\uc37c\\ub2e4. 4\\uc77c \\uc720\\uac00 \\uc99d\\uad8c\\uc2dc\\uc7a5\\uc5d0\\uc11c \\ucf54\\uc2a4\\ud53c\\ub294 \\uc804\\ub0a0\\ubcf4\\ub2e4 21. 57\\ud3ec\\uc778\\ud2b80. 97% \\uc624\\ub978 2241. 24\\ub97c \\uae30\\ub85d\\ud558\\uba70 \\uc7a5\\uc744 \\ub9c8\\ucce4\\ub2e4. \\uc885\\uac00 \\uae30\\uc900 \\uc774\\uc804 \\ucd5c\\uace0\\uce58\\uc600\\ub358 2228. 962011\\ub144 5\\uc6d4 2\\uc77c\\uc744 \\ub3cc\\ud30c\\ud55c \\uac83\\uc740 \\ubb3c\\ub860\\uc774\\uace0 \\uae30\\uc874 \\uc7a5 \\uc911 \\ucd5c\\uace0\\uce58 \\uae30\\ub85d 2011\\ub144 4\\uc6d4 27\\uc77c 2231. 94\\ub3c4 \\ub118\\uc5b4\\uc120 \\uac83\\uc774\\ub2e4. \\uc774\\ub85c\\uc368 \\ud55c\\uad6d \\uc720\\uac00\\uc99d\\uad8c\\uc2dc\\uc7a5\\uc740 '\\ubc15\\uc2a4\\ud53c\\ubc15\\uc2a4\\uad8c\\uc5d0 \\uac16\\ud78c \\ucf54\\uc2a4\\ud53c'\\ub77c\\ub294 \\uc624\\uba85\\uc744 6\\ub144 \\ub9cc\\uc5d0 \\ub5bc\\uac8c \\ub410\\ub2e4. \\uc774\\ub0a0 \\ud55c\\uad6d\\uc99d\\uc2dc\\uc758 \\uc2dc\\uac00 \\ucd1d\\uc561 \\uae30\\ub85d\\ub3c4 \\uacbd\\uc2e0\\ub410\\ub2e4. \\ucf54\\uc2a4\\ud53c \\uc2dc\\uac00\\ucd1d\\uc561\\uc740 \\ub0a0 \\uc885\\uac00 1454\\uc8705775\\uc5b5\\uc6d0\\uc73c\\ub85c \\uc9d1\\uacc4\\ub410\\ub2e4. \\uc804\\ub0a0 1441\\uc870 1544\\uc5b5\\uc6d0\\uc774\\uc5c8\\ub358 \\uc2dc\\uac00\\ucd1d\\uc561\\uc740 \\ub0a0 13\\uc870\\uc6d0 \\uac00\\ub7c9 \\ub298\\uc5b4\\ub098\\uba74\\uc11c \\ucc98\\uc74c\\uc73c\\ub85c 1450\\uc870\\uc6d0\\uc744 \\ub118\\uc5b4\\uc11c\\uac8c \\ub41c \\uac83\\uc774\\ub2e4. \\uc99d\\uc2dc \\uc804\\ubb38\\uac00\\ub4e4\\uc740 \\uae00\\ub85c\\ubc8c \\uacbd\\uae30\\uac00 \\ud638\\uc870\\ub97c \\ubcf4\\uc774\\uace0 \\uae30\\uc5c5 \\uc2e4\\uc801\\uc774 \\uc774\\ub97c \\ub4b7\\ubc1b\\uce68\\ud558\\uba74\\uc11c \\uc678\\uad6d\\uc778 \\uc790\\uae08\\uc774 \\ucf54\\uc2a4\\ud53c\\ub97c \\ub808\\ubca8\\uc5c5 \\uc2dc\\ucf30\\ub2e4\\uace0 \\ubd84\\uc11d\\ud588\\ub2e4. \\uc774\\ub0a0\\ub3c4 \\ucf54\\uc2a4\\ud53c \\uae09\\ub4f1\\uc758 1 \\uacf5\\uc2e0\\uc740 \\uc678\\uad6d\\uc778 \\ud22c\\uc790\\uc790\\uc600\\ub2e4. \\uc678\\uad6d\\uc778\\uc740 \\ub0a0 \\uc720\\uac00\\uc99d\\uad8c\\uc2dc\\uc7a5\\uc5d0\\uc11c 3614\\uc5b5\\uc6d0 \\uc5b4\\uce58\\ub97c \\uc21c\\ub9e4\\uc218\\ud558\\uba74\\uc11c \\uc9c0\\uc218 \\uc0c1\\uc2b9\\uc744 \\ub04c\\uc5c8\\ub2e4. \\uae30\\uad00\\uacfc \\uac1c\\uc778 \\ud22c\\uc790\\uc790\\ub4e4\\uc774 3336\\uc5b5\\uc6d0, 709\\uc5b5\\uc6d0 \\uc5b4\\uce58\\ub97c \\uc21c\\ub9e4\\ub3c4\\ud588\\uc9c0\\ub9cc \\uc678\\uad6d\\uc778\\uc740 \\uc774\\ud2c0\\uc9f8 '\\ubc14\\uc774\\ucf54\\ub9ac\\uc544'\\ub97c \\uc774\\uc5b4\\uac14\\ub2e4. \\uc678\\uad6d\\uc778\\uc740 \\ud55c\\ubc18\\ub3c4\\ub97c \\ub458\\ub7ec\\uc2fc \\uc9c0\\uc815\\ud559\\uc801 \\uc704\\ud5d8\\uc774 \\ucee4\\uc84c\\uc74c\\uc5d0\\ub3c4 \\uc9c0\\ub09c\\ub2ec 20\\uc77c\\ubd80\\ud130 \\ucf54\\uc2a4\\ud53c\\uc5d0\\uc11c \\ub9e4\\uc218 \\uae30\\uc870\\ub97c \\uc720\\uc9c0\\ud574\\uc624\\uba74\\uc11c \\ub0a0\\uae4c\\uc9c0 2\\uc8701000\\uc5b5\\uc6d0 \\uc5b4\\uce58\\ub97c \\uc21c\\ub9e4\\uc218\\ud588\\ub2e4. \\uadf8\\ub9cc\\ud07c \\ud55c\\uad6d \\uae30\\uc5c5 \\uc2e4\\uc801 \\ud638\\uc870\\uc640 \\uae30\\uc5c5\\uc9c0\\ubc30\\uad6c\\uc870 \\uac1c\\uc120\\uc5d0 \\ub300\\ud55c \\ubbff\\uc74c\\uc774 \\ud070 \\uac83\\uc73c\\ub85c \\ud480\\uc774\\ub41c\\ub2e4. \\ucf54\\uc2a4\\ud53c\\uac00 \\ub9ce\\uc774 \\uc62c\\ub790\\ub2e4\\uace0 \\uc99d\\uc2dc\\uc5d0\\uc11c\\ub294 \\uc2e4\\uc81c \\uc2e4\\uc801 \\uac1c\\uc120\\uc5d0 \\ub35c \\uc624\\ub978 \\uac83\\uc73c\\ub85c \\ud3c9\\uac00\\uac00 \\ub098\\uc624\\uace0. \\ud55c\\uad6d \\uc8fc\\uc2dd\\uc774 \\uc77c\\ubcf8\\uc774\\ub098 \\ub300\\ub9cc\\uc5d0 \\ube44\\ud574 \\uc2f8\\ub2e4\\ub294 \\ubd84\\uc11d\\uc774 \\ub098\\uc62c \\uc815\\ub3c4\\ub2e4. \\ucf54\\uc2a4\\ud53c \\uc0c1\\uc7a5\\uc0ac\\uc758 \\uc5f0\\uac04 \\uc21c\\uc774\\uc775\\uc740 \\uc9c0\\ub09c\\ud574 94\\uc870\\uc6d0\\uc744 \\uae30\\ub85d\\ud55c \\uc774\\ud6c4 \\uc62c\\ud574\\ub294 120\\uc870\\uc6d0\\uc744 \\ub3cc\\ud30c\\ud560 \\uac83\\uc73c\\ub85c \\uc608\\uc0c1\\ub418\\uae30 \\ub54c\\ubb38\\uc774\\ub2e4. \\uc2dc\\uac00\\ucd1d\\uc561 1\\uc704\\uc778 \\uc0bc\\uc131\\uc804\\uc790\\uac00 \\uc5f0\\uc77c \\uc0ac\\uc0c1 \\ucd5c\\uace0\\uac00\\ub97c \\uae30\\ub85d\\ud558\\uace0 \\uc788\\ub294 \\uac83\\ub3c4 \\ucf54\\uc2a4\\ud53c \\ub808\\ubca8\\uc5c5\\uc758 \\uc8fc\\uc694\\ud55c \\uc6d0\\uc778\\uc774\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uc62c\\ud574 \\uae5c\\uc9dd \\uc2e4\\uc801\\uc744 \\uae30\\ub85d\\ud560 \\uac83\\uc73c\\ub85c \\uc804\\ub9dd\\ub418\\uba74\\uc11c \\uc9c0\\ub09c\\ud574 \\uc774\\ud6c4 \\uc8fc\\uac00\\uac00 30% \\uc62c\\ub790\\ub2e4. \\uc5ec\\uae30\\uc5d0 \\uc790\\uc0ac\\uc8fc \\uc18c\\uac01\\uc744 \\ube44\\ub86f\\ud55c \\uc8fc\\uc8fc \\ud658\\uc6d0\\ucc45\\uae4c\\uc9c0 \\ub0b4\\ub193\\uc790 \\uc0bc\\uc131\\uc804\\uc790 \\uc8fc\\uac00\\ub294 227\\ub9cc\\uc6d0\\uc120\\uc744 \\ub3cc\\ud30c\\ud55c \\uc0c1\\ud0dc\\ub2e4. \\ucd5c\\ud604\\ub9cc \\ubbf8\\ub798\\uc5d0\\uc14b\\ub300\\uc6b0 \\uc218\\uc11d\\ubd80\\ud68c\\uc7a5\\uc740 \\\"\\uc218\\ucd9c\\uacfc \\ud22c\\uc790\\ub97c \\uc911\\uc2ec\\uc73c\\ub85c \\ud55c \\uacbd\\uae30\\uc9c0\\ud45c\\uac00 \\uac1c\\uc120\\ub418\\uace0 \\uc788\\ub294 \\ub370\\ub2e4 \\uc0c1\\uc7a5 \\uae30\\uc5c5\\ub4e4\\uc758 \\uc21c\\uc774\\uc775\\ub3c4 \\uc0ac\\uc0c1 \\ucd5c\\ub300\\ub97c \\ub118\\uc5b4\\uc124 \\uac83\\uc73c\\ub85c \\uc608\\uc0c1\\ub418\\uba74\\uc11c \\uc62c 2\\ubd84\\uae30 \\uc774\\ud6c4 \\ucf54\\uc2a4\\ud53c \\uc804\\ub9dd\\uc740 \\ub354\\uc6b1 \\ubc1d\\ub2e4\\\"\\uace0 \\ub0b4\\ub2e4\\ubd24\\ub2e4.\",\n          \"\\uc0bc\\uc131\\uc804\\uc790\\uac00 26\\uc77c \\ubbf8\\uad6d '\\uc0bc\\uc131 \\uac1c\\ubc1c\\uc790 \\ucf58\\ud37c\\ub7f0\\uc2a4 202121'\\uc744 \\uc5f4\\uace0 \\ud601\\uc2e0\\uc801\\uc778 \\uace0\\uac1d \\uacbd\\ud5d8\\uc744 \\uac00\\ub2a5\\ud558\\uac8c \\ud558\\ub294 \\ucc28\\uc138\\ub300 \\uae30\\uc220\\ub4e4\\uc744 \\ub17c\\uc758\\ud588\\ub2e4. 2019\\ub144 \\uc774\\ud6c4 2\\ub144 \\ub9cc\\uc5d0 \\uc5f4\\ub9b0 \\ud589\\uc0ac\\ub294 \\uc628\\ub77c\\uc778\\uc73c\\ub85c \\uac1c\\ucd5c\\ub410\\ub2e4. \\uc774\\ub0a0 \\ud589\\uc0ac\\uc5d0\\uc11c\\ub294 \\u25b2\\uc778\\uacf5\\uc9c0\\ub2a5\\uc0ac\\ubb3c\\uc778\\ud130\\ub137 \\ubcf4\\uc548 \\ud50c\\ub7ab\\ud3fc \\ud601\\uc2e0 \\u25b2\\ud0c0\\uc774\\uc820 \\uae30\\ubc18\\uc758 \\uc2a4\\ud06c\\ub9b0 \\uacbd\\ud5d8 \\ud655\\uc7a5 \\u25b2\\uac24\\ub7ed\\uc2dc \\uc0dd\\ud0dc\\uacc4 \\uac15\\ud654 \\ubc29\\uc548\\uc5d0 \\ub300\\ud574 \\uc804 \\uc138\\uacc4 \\uac1c\\ubc1c\\uc790, \\ud30c\\ud2b8\\ub108\\uc0ac, \\uc18c\\ube44\\uc790\\ub4e4\\uc5d0\\uac8c \\uacf5\\uac1c\\ud588\\ub2e4. \\ud2b9\\ud788, \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uc18c\\ube44\\uc790 \\uc911\\uc2ec \\ud601\\uc2e0\\uc758 \\uc911\\uc694\\uc131\\uc744 \\uac15\\uc870\\ud558\\uba70, \\ube45\\uc2a4\\ube44, \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4, \\uc0bc\\uc131 \\ub179\\uc2a4 \\ud50c\\ub7ab\\ud3fc\\uc758 \\ubc1c\\uc804\\uc774 \\uc18c\\ube44\\uc790\\ub4e4\\uc758 \\uc2e4\\uc0dd\\ud65c\\uc5d0 \\ub3c4\\uc6c0\\uc744 \\uc904 \\uc218 \\uc788\\ub294 \\ubc29\\uc548\\ub4e4\\uc744 \\uc120 \\ubcf4\\uc600\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790 \\uace0\\ub3d9\\uc9c4 \\ub300\\ud45c\\uc774\\uc0ac \\uc0ac\\uc7a5\\uc740 \\ub0a0 \\uae30\\uc870\\uc5f0\\uc124\\uc5d0\\uc11c \\\"\\uc0bc\\uc131\\uc804\\uc790\\uac00 \\uc804 \\uc138\\uacc4 \\ucc3d\\uc758\\uc801\\uc778 \\uac1c\\ubc1c\\uc790\\ub4e4\\uacfc \\ud611\\ub825\\ud574 \\uc0c8\\ub85c\\uc6b4 \\uc2dc\\ub300\\ub97c \\uc5f4\\uc5b4 \\ub098\\uac08 \\uc218 \\uc788\\uc5b4 \\uc790\\ubd80\\uc2ec\\uc744 \\ub290\\ub080\\ub2e4\\\"\\uba70 \\\"\\uc218\\ub9ce\\uc740 \\uae30\\uae30\\uc640 \\ub124\\ud2b8\\uc6cc\\ud06c\\ub97c \\ud1b5\\ud574 \\uc628 \\uc138\\uc0c1\\uc774 \\ubc00 \\uc811\\ud558\\uac8c \\uc5f0\\uacb0\\ub418\\uace0 \\uc788\\ub294 \\ubaa8\\ub450\\uc758 \\uc0b6\\uc744 \\ub354 \\ud3b8\\ub9ac\\ud558\\uace0 \\uc2a4\\ub9c8\\ud2b8\\ud558\\uac8c \\ub9cc\\ub4e4 \\uc218 \\uc788\\ub3c4\\ub85d \\ud601\\uc2e0\\uc744 \\uc9c0\\uc18d\\ud558\\uaca0\\ub2e4\\\"\\ub77c\\uace0 \\ub9d0\\ud588\\ub2e4. \\uc18c\\ube44\\uc790 \\uacbd\\ud5d8 \\ud5a5\\uc0c1 \\uac1c\\ubc1c\\uc790 \\uc791\\uc5c5 \\uc9c0\\uc6d0 \\ud50c\\ub7ab\\ud3fc \\uac15\\ud654 \\ubc29\\uc548 \\uacf5\\uac1c \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\ubc88 \\ucf58\\ud37c\\ub7f0\\uc2a4\\uc5d0\\uc11c \\uc18c\\ube44\\uc790 \\uacbd\\ud5d8\\uc744 \\ud5a5\\uc0c1\\uc2dc\\ud0a4\\uace0 \\uac1c\\ubc1c\\uc790\\ub4e4\\uc758 \\uc791\\uc5c5\\uc744 \\uc9c0\\uc6d0\\ud558\\uae30 \\uc704\\ud55c \\ube45\\uc2a4\\ube44, \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4, \\uc0bc\\uc131 \\ub179\\uc2a4 \\ud50c\\ub7ab\\ud3fc \\uac15\\ud654 \\ubc29\\uc548\\uc744 \\uacf5\\uac1c\\ud588\\ub2e4. 3\\uc5b5 \\ub300 \\uc774\\uc0c1\\uc758 \\uc0bc\\uc131 \\uae30\\uae30\\uc5d0 \\uc801\\uc6a9\\ub41c \\ud50c\\ub7ab\\ud3fc \\ube45\\uc2a4\\ube44\\ub294 \\uc131\\ub2a5\\uc774 \\ub354\\uc6b1 \\uac15\\ud654\\ub41c\\ub2e4. \\ube45\\uc2a4\\ube44\\ub294 \\ucd5c\\uc2e0 \\uc2a4\\ub9c8\\ud2b8\\ud3f0\\uc5d0\\uc11c \\uc628\\ub514\\ubc14\\uc774\\uc2a4 \\ud65c\\uc6a9\\ud574 \\uae30\\uae30 \\uc790\\uccb4\\uc5d0\\uc11c \\uba85\\ub839\\uc744 \\uc2e4\\ud589\\ud568\\uc73c\\ub85c\\uc368 \\ubc18\\uc751 \\uc18d\\ub3c4\\uac00 \\ucd5c\\ub300 35% \\ube68\\ub77c\\uc84c\\ub2e4. \\ud50c\\ub7ab\\ud3fc\\uc778 \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4\\uc758 \\uc5f0\\uacb0\\uc131\\uacfc \\ud638\\ud658 \\uae30\\uae30\\ub3c4 \\ub354\\uc6b1 \\ud655\\ub300\\ub41c\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\ub294 '\\uc2a4\\ub9c8\\ud2b8\\uc2f1\\uc2a4 \\ud5c8\\ube0c ' \\uc18c\\ud504\\ud2b8\\uc6e8\\uc5b4\\ub97c \\ud328\\ubc00\\ub9ac \\ud5c8\\ube0c \\ub0c9\\uc7a5\\uace0 \\uc0bc\\uc131 \\uc81c\\ud488\\uc5d0 \\ud0d1\\uc7ac\\ud574, \\ud574\\ub2f9 \\uc81c\\ud488\\uc5d0\\uc11c \\uae30\\uae30\\uc640 \\uc5f0\\uacb0\\ud560 \\uc218 \\uc788\\ub3c4\\ub85d \\uc9c0\\uc6d0\\ud560 \\uacc4\\ud68d\\uc774\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uac1c\\ubc1c\\uc790\\ub4e4\\uc774 \\ube45\\uc2a4\\ube44\\uc640 \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4 \\uac04 \\uc5f0\\uacc4\\ub97c \\uac15\\ud654\\ud560 \\uc218 \\uc788\\ub294 '\\ube45\\uc2a4\\ube44 \\ud648' \\ud50c\\ub7ab\\ud3fc\\ub3c4 \\uc120\\ubcf4\\uc600\\ub2e4. '\\ube45\\uc2a4\\ube44 \\ud648' \\ud50c\\ub7ab\\ud3fc\\uc740 \\ud604\\uc7ac \\uc5f0\\uacb0\\ub3fc \\uc788\\ub294 \\uc81c\\ud488\\ub4e4\\uc758 \\uc0c1\\ud0dc\\ub97c \\uace0\\ub824\\ud574 \\uc74c\\uc131 \\uba85\\ub839\\uc744 \\uc218\\ud589\\ud560 \\uc218. \\\"\\uc601\\ud654 \\uc7ac\\uc0dd\\ud574\\uc918\\\"\\ub77c\\uace0 \\ub9d0\\ud558\\uba74 \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4\\uc5d0 \\uc5f0\\uacb0\\ub41c , \\uc0ac\\uc6b4\\ub4dc\\ubc14, \\uc870\\uba85 \\uac19\\uc740 \\uae30\\uae30\\ub4e4\\uc774 \\uc791\\ub3d9\\ud574 \\uc2a4\\ub9c8\\ud2b8\\ud648 \\uacbd\\ud5d8\\uc744 \\ub354\\uc6b1 \\ud3b8\\ub9ac\\ud558\\uac8c \\ud574\\uc900\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4\\uc5d0 \\uc5c5\\uacc4 \\ucd5c\\uc2e0 \\ud1b5\\uc2e0 \\uaddc\\uaca9\\uc778 '\\ub9e4\\ud130'\\ub97c \\uc801\\uc6a9\\ud574, \\ud30c\\ud2b8\\ub108\\uc0ac\\ub97c \\ud655\\ub300\\ud558\\uace0 \\ud638\\ud658 \\uae30\\uae30\\ub4e4\\uc774 \\ub354 \\uc27d\\uac8c \\uc5f0\\uacb0\\ub418\\ub3c4\\ub85d \\ud560 \\uacc4\\ud68d\\uc774\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\ubcf4\\uc548\\uc5d0 \\uc788\\uc5b4\\uc11c \\u25b2\\ubcf4\\ud638 \\u25b2\\uc120\\ud0dd \\u25b2\\ud22c\\uba85\\uc131\\uc758 3\\ub300 \\uc6d0\\uce59\\uc744 \\uc900\\uc218\\ud558\\uace0 \\uc18c\\ube44\\uc790, \\uac1c\\ubc1c\\uc790, \\ud30c\\ud2b8\\ub108\\uc0ac\\uc5d0\\uac8c \\ub370\\uc774\\ud130 \\uad00\\ub9ac \\ubc29\\ubc95\\uc744 \\uacb0\\uc815\\ud558\\uae30 \\uc704\\ud574 \\ud544\\uc694\\ud55c \\uc815\\ubcf4\\ub97c \\uc81c\\uacf5\\ud558\\uace0. \\ub610, \\ubcf4\\uc548 \\uae30\\uc220\\uc744 \\uc704\\ud55c \\ud611\\uc5c5\\uacfc \\uac1c\\ubc29\\ud615 \\ud601\\uc2e0 \\ucca0\\ud559\\uc744 \\uc911\\uc2dc\\ud558\\uba70, \\ubcf4\\uc548 \\ubd84\\uc11d \\ub3c4\\uad6c\\ub97c \\uc624\\ud508\\uc18c\\uc2a4 \\ud504\\ub85c\\uc81d\\ud2b8\\ub85c \\uc9c0\\uc18d \\ucd9c\\uc2dc\\ud560 \\uacc4\\ud68d\\uc774\\ub2e4. '\\uc2a4\\ud06c\\ub9b0 \\ud3ec \\uc62c' \\uc0ac\\uc6a9 \\uacbd\\ud5d8 \\ud655\\uc7a5\\uc0c8\\ub85c\\uc6b4 \\uae30\\ub2a5 \\uc18c\\uac1c '\\uc2a4\\ud06c\\ub9b0 \\ud3ec \\uc62c ' \\ube44\\uc804\\uc744 \\ubc14\\ud0d5\\uc73c\\ub85c \\uc138\\uacc4 \\uc2dc\\uc7a5\\uc744 \\uc120\\ub3c4 \\ud558\\ub294 \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uc0ac\\uc6a9 \\uacbd\\ud5d8\\uc744 \\ud655\\uc7a5\\ud558\\ub294 \\uc0c8\\ub85c\\uc6b4 \\uae30\\ub2a5\\ub4e4\\uc744 \\uc18c\\uac1c\\ud588\\ub2e4. \\ud654\\uc0c1\\ud1b5\\ud654\\uc758 \\uacbd\\uc6b0, \\uad6c\\uae00\\uacfc\\uc758 \\ud611\\ub825\\uc744 \\ud1b5\\ud574 \\uae30\\ubc18\\uc73c\\ub85c \\uc778\\ubb3c\\uc744 \\ud3ec\\ucee4\\uc2a4\\ud558\\uac70\\ub098 \\uc90c\\uc778\\ud558\\ub294 \\uae30\\ub2a5\\uc744 \\uc0ac\\uc6a9\\ud560 \\uc218 \\uc788\\uc5b4 \\ud3b8\\uc758\\uc131\\uc774 \\uac15\\ud654\\ub41c\\ub2e4. \\uc6a9 \\uc0bc\\uc131 \\ud5ec\\uc2a4\\uc5d0\\uc11c\\ub294 \\uc6f9\\ucea0, \\ubaa8\\ubc14\\uc77c \\uae30\\uae30\\uc640 \\uc5f0\\uacb0\\ud558\\uba74 \\uc2a4\\ub9c8\\ud2b8 \\ud2b8\\ub808\\uc774\\ub108\\uac00 \\uc0ac\\uc6a9\\uc790\\uc758 \\uc6b4\\ub3d9\\uc744 \\ubd84\\uc11d\\ud558\\uace0 \\uc2e4\\uc2dc\\uac04 \\ud53c\\ub4dc\\ubc31\\uc744 \\uc81c\\uacf5\\ud560 \\uc218. \\ucc28\\uc138\\ub300 \\ud654\\uc9c8 \\uae30\\uc220\\uc778 10 \\ud45c\\uc900\\uc774 \\uac8c\\uc784\\uc73c\\ub85c\\ub3c4 \\ud655\\ub300 \\uc801\\uc6a9\\ub3fc \\ub354 \\uc2e4\\uc801\\uc774\\uace0 \\ubab0\\uc785\\uac10 \\uc788\\ub294 \\uac8c\\uc784\\uc744 \\uc990\\uae38 \\uc218. \\uc0bc\\uc131\\uc804\\uc790\\ub294 2 \\uc11c\\ube44\\uc2a4\\uc778 '\\uae30\\uc5c5\\uc6a9 \\ud0c0\\uc774\\uc820 '\\uc744 \\uc120 \\ubcf4\\uc600\\ub2e4. \\uc11c\\ube44\\uc2a4\\ub97c \\ud1b5\\ud574 \\uc5ec\\ub7ec \\uae30\\uae30\\ub97c \\uc2e4\\uc2dc\\uac04\\uc73c\\ub85c \\ubaa8\\ub2c8\\ud130\\ub9c1\\ud558\\uace0 \\uc6d0\\uaca9 \\uc81c\\uc5b4\\ud560 \\uc218 \\uc788\\uc5b4 \\uae30\\uc5c5\\uae30\\uad00\\ub4e4\\uc740 \\ud559\\uad50\\uc758 \\uce60\\ud310, \\uc1fc\\ud551\\ubab0 \\ud0a4\\uc624\\uc2a4\\ud06c\\uc5d0\\uc11c \\uc9c0\\ud558\\ucca0\\uacfc \\uacf5\\ud56d\\uc758 \\uc2a4\\ud06c\\ub9b0\\uae4c\\uc9c0 \\ub2e4\\uc591\\ud55c \\ub514\\uc2a4\\ud50c\\ub808\\uc774\\ub97c \\ud3b8\\ub9ac\\ud558\\uac8c \\uad00\\ub9ac \\uac00\\ub2a5\\ud558\\ub2e4. '\\ud3f4\\ub354\\ube14\\ud3f0 \\uc6e8\\uc5b4\\ub7ec\\ube14 \\uace0\\uc720 \\uc0ac\\uc6a9\\uc790 \\uacbd\\ud5d8\\uc73c\\ub85c \\uac24\\ub7ed\\uc2dc \\uc0dd\\ud0dc\\uacc4 \\uac15\\ud654 \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uac24\\ub7ed\\uc2dc \\ubaa8\\ubc14\\uc77c \\uae30\\uae30\\uc758 \\ucd5c\\uc2e0 \\uc0ac\\uc6a9\\uc790 \\uacbd\\ud5d8\\uc778 '\\uc6d0 4 4'\\ub97c \\uacf5\\uac1c\\ud588\\ub2e4. \\uc6d0 4\\ub294 \\uc0ac\\uc6a9\\uc790\\uac00 \\uc790\\uc2e0\\ub9cc\\uc758 \\uacbd\\ud5d8\\uc744 \\ub9cc\\ub4e4 \\uc218 \\uc788\\ub3c4\\ub85d \\uc0c8\\ub85c\\uc6b4 \\ucc28\\uc6d0\\uc758 \\ub9de\\ucda4\\ud615 \\uae30\\ub2a5\\uc744 \\uc81c\\uacf5\\ud558\\uba70, \\ubcf4\\uc548 \\uba74\\uc5d0\\uc11c\\ub3c4 \\uc0ac\\uc6a9\\uc790\\uac00 \\uc815\\ubcf4 \\uc774\\uc6a9 \\uad8c\\ud55c\\uc744 \\uc870\\uc815\\ud574 \\uc790\\uc2e0\\uc758 \\ub370\\uc774\\ud130\\ub97c \\uad00\\ub9ac\\ud560 \\uc218 \\uc788\\ub3c4\\ub85d \\ud55c\\ub2e4. \\ub610\\ud55c, '\\uac24\\ub7ed\\uc2dc \\ud3f4\\ub4dc3' '\\uac24\\ub7ed\\uc2dc \\ud50c\\ub9bd3' \\ud3f4\\ub354\\ube14\\ud3f0\\uc73c\\ub85c \\ud3fc\\ud329\\ud130\\uac00 \\ud655\\uc7a5\\ub428\\uc5d0 \\uac1c\\ubc1c\\uc790\\ub4e4\\uc740 \\ud50c\\ub809\\uc2a4 \\ubaa8\\ub4dc, \\uba40\\ud2f0 \\uc561\\ud2f0\\ube0c \\uc708\\ub3c4\\uc6b0 \\uc0bc\\uc131 \\ud3f4\\ub354\\ube14\\ud3f0 \\uace0\\uc720\\uc758 \\uc0ac\\uc6a9 \\uacbd\\ud5d8\\uc744 \\ud65c\\uc6a9\\ud560 \\uc218. \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\ub0a0 \\ucf58\\ud37c\\ub7f0\\uc2a4\\uc5d0\\uc11c '\\uac24\\ub7ed\\uc2dc \\uc6cc\\uce584' \\uc2dc\\ub9ac\\uc988\\uc5d0 \\ud0d1\\uc7ac\\ub41c \\uc0bc\\uc131\\uc758 \\ub3c5\\uc790\\uc801\\uc778 \\uc0ac\\uc6a9\\uc790 \\uacbd\\ud5d8\\uc778 '\\uc6d0 \\uc6cc\\uce58', \\uad6c\\uae00\\uacfc \\uacf5\\ub3d9 \\uac1c\\ubc1c\\ud55c \\uc2e0\\uaddc \\uc6e8\\uc5b4\\ub7ec\\ube14 \\ud1b5\\ud569 \\ud50c\\ub7ab\\ud3fc\\ub3c4 \\uc18c\\uac1c\\ud588\\ub2e4. \\ud589\\uc0ac\\uc5d0 \\ub300\\ud55c \\uc790\\uc138\\ud55c \\ub0b4\\uc6a9\\uc740 \\uc0bc\\uc131 \\uac1c\\ubc1c\\uc790 \\ucf58\\ud37c\\ub7f0\\uc2a4 \\uacf5\\uc2dd \\uc6f9\\uc0ac\\uc774\\ud2b8\\uc5d0\\uc11c \\ud655\\uc778\\ud560 \\uc218.\",\n          \"\\uc0bc\\uc131\\uc804\\uc790 \\ubd80\\ubb38\\uc740 \\uacbd\\uae30\\uc0ac\\ud68c\\ubcf5\\uc9c0\\uacf5\\ub3d9\\ubaa8\\uae08\\ud68c\\uc640 \\uacbd\\uae30 \\ud654\\uc131\\uc6a9\\uc778 \\ud3c9\\ud0dd \\uc9c0\\uc5ed\\uc5d0\\uc11c '\\ud589\\ubcf5 \\ubaa8\\uc790\\uc774\\ud06c \\ud504\\ub85c\\uc81d\\ud2b8'\\ub97c \\uc2dc\\ud589\\ud55c\\ub2e4\\uace0 4\\uc77c \\ubc1d\\ud614\\ub2e4. \\ud589\\ubcf5 \\ubaa8\\uc790\\uc774\\ud06c\\ub294 \\uc9c0\\uc5ed\\uc0ac\\ud68c\\uc758 \\uc18c\\uc678\\ub41c \\uc774\\uc6c3\\uc774 \\ud589\\ubcf5\\ud560 \\uc218 \\uc788\\ub3c4\\ub85d \\ucc38\\uc2e0\\ud55c \\ud504\\ub85c\\uadf8\\ub7a8\\uc744 \\uc9c0\\uc5ed \\uc804\\ubb38\\uac00\\ub4e4\\uc774 \\ubc1c\\uad74\\ud558\\uba74, \\uc0bc\\uc131\\uc804\\uc790 \\ubd80\\ubb38 \\uc784\\uc9c1\\uc6d0\\ub4e4\\uc758 \\uae30\\ubd80\\uae08\\uc73c\\ub85c \\ud504\\ub85c\\uadf8\\ub7a8\\uc744 \\uc9c4\\ud589\\ud558\\ub294 \\uc0ac\\ud68c \\uacf5\\ud5cc\\uc0ac\\uc5c5\\uc774\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\uac00 \\uc9c0\\ub09c 4\\uc6d4 \\ud55c \\ub2ec\\uac04 \\uc9c0\\uc5ed\\uc0ac\\ud68c 700\\uc5ec \\uac1c \\uc0ac\\ud68c\\ubcf5\\uc9c0\\uae30\\uad00\\uc744 \\ub300\\uc0c1\\uc73c\\ub85c \\uacf5\\ubaa8\\ud55c \\ub4a4 \\uad50\\uc218\\uc640 \\uacf5\\ubb34\\uc6d0, \\uc0ac\\ud68c\\uacf5\\ud5cc \\uc804\\ubb38 \\ucee8\\uc124\\ud134\\ud2b8\\ub85c \\uad6c\\uc131\\ub41c \\uc2ec\\uc0ac\\ub2e8\\uc744 \\ud1b5\\ud574 13\\uac1c \\ud504\\ub85c\\uc81d\\ud2b8\\ub97c \\ud655\\uc815\\ud588\\ub2e4. \\ud504\\ub85c\\uc81d\\ud2b8\\ub294 \\uae30\\ud68d \\uc8fc\\uccb4\\uc778 \\ubcf5\\uc9c0\\uae30\\uad00\\uc5d0\\uc11c \\ub0b4\\ub144 6\\uc6d4\\uae4c\\uc9c0 1\\ub144\\uac04 \\uc6b4\\uc601\\ud558\\uac8c \\ub41c\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790 \\ubd80\\ubb38 \\uc0ac\\ud68c \\ubd09\\uc0ac\\ub2e8\\uc7a5 \\uae40\\uc120\\uc2dd \\uc804\\ubb34\\ub294 \\\"\\uc608\\uc0c1\\ubcf4\\ub2e4 \\ub9ce\\uc740 \\uc544\\uc774\\ub514\\uc5b4\\uac00 \\uc811\\uc218\\ub410\\ub2e4\\\"\\uba74\\uc11c \\\"\\ud504\\ub85c\\uc81d\\ud2b8 \\uc120\\uc815 \\uae30\\uad00\\ub4e4\\uc774 \\uc9c0\\uc5ed \\ubb38\\uc81c \\ud574\\uacb0\\uc5d0 \\ud070 \\uc5f4\\uc758\\ub97c \\ubcf4\\uc5ec \\uc88b\\uc740 \\uc131\\uacfc\\uac00 \\uae30\\ub300\\ub41c\\ub2e4\\\"\\uace0 \\ub9d0\\ud588\\ub2e4.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "final_매일경제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kQNPt3dxSJct"
      },
      "outputs": [],
      "source": [
        "# 전처리 완료 된 본문 불러온 후 column명 맞춰주기\n",
        "result = final_매일경제['cleaned_body'].reset_index()\n",
        "result.rename(columns={'cleaned_body' : 'kor_sentence'}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "AqRKfdsRNUTv"
      },
      "outputs": [],
      "source": [
        "# 동일 형태로 전처리\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df_data = df\n",
        "    def __getitem__(self, index):\n",
        "        # get the sentence from the dataframe\n",
        "        sentence = self.df_data.loc[index, 'kor_sentence']\n",
        "        encoded_dict = tokenizer(\n",
        "          text = sentence,\n",
        "          add_special_tokens = True,\n",
        "          max_length = MAX_LEN,\n",
        "          pad_to_max_length = True,\n",
        "          truncation=True,\n",
        "          return_tensors=\"pt\")\n",
        "\n",
        "        padded_token_list = encoded_dict['input_ids'][0]\n",
        "        token_type_id = encoded_dict['token_type_ids'][0]\n",
        "        att_mask = encoded_dict['attention_mask'][0]\n",
        "        sample = (padded_token_list, token_type_id , att_mask)\n",
        "        return sample\n",
        "    def __len__(self):\n",
        "        return len(self.df_data)\n",
        "\n",
        "# 추론 데이터 dataloader에 올리기\n",
        "test_data = TestDataset(result)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=False,\n",
        "                                      num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7z-qUIhfNiph"
      },
      "outputs": [],
      "source": [
        "# library import\n",
        "import torch.cuda.amp as amp\n",
        "from transformers import XLMPreTrainedModel, XLMRobertaModel, XLMRobertaConfig, XLMRobertaTokenizer\n",
        "from transformers import XLMRobertaForSequenceClassification, BertForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import BertForSequenceClassification, DistilBertForSequenceClassification, XLNetForSequenceClassification,\\\n",
        "XLMRobertaForSequenceClassification, XLMForSequenceClassification, RobertaForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8U-H_lc_Natz"
      },
      "outputs": [],
      "source": [
        "# 파라미터 설정\n",
        "class args:\n",
        "    debug=False\n",
        "    amp = True\n",
        "    gpu = '0'\n",
        "\n",
        "    \"\"\"\n",
        "    epochs=5\n",
        "    batch_size=32\n",
        "    weight_decay=1e-6\n",
        "    n_fold=5\n",
        "    fold=3 # [0, 1, 2, 3, 4] # 원래는 3\n",
        "\n",
        "    exp_name = 'model_f'\n",
        "    dir_ = f'./saved_models/'\n",
        "    pt = 'mz_model'\n",
        "    max_len = 128\n",
        "\n",
        "    start_lr = 1e-5#1e-3,5e-5\n",
        "    min_lr=1e-6\n",
        "    # ---- Dataset ---- #\n",
        "\n",
        "    # ---- Else ---- #\n",
        "    num_workers=8\n",
        "    seed=2021\n",
        "    scheduler = None#'get_linear_schedule_with_warmup'\n",
        "    \"\"\"\n",
        "\n",
        "# valid set 예측\n",
        "def do_predict(net, valid_loader):\n",
        "\n",
        "    val_loss = 0\n",
        "    pred_lst = []\n",
        "    logit=[]\n",
        "    net.eval()\n",
        "    for batch_id, (input_id,token_type_id,attention_mask) in enumerate(tqdm_notebook(valid_loader)):\n",
        "        input_id = input_id.long().to(device)\n",
        "        token_type_id = token_type_id.long().to(device)\n",
        "        attention_mask = attention_mask.long().to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if args.amp:\n",
        "                with amp.autocast(): #메모리 사용량 감소, 연산 속도 향상\n",
        "                    # output\n",
        "                    output = net(input_ids=input_id, token_type_ids=token_type_id, attention_mask=attention_mask)[0]\n",
        "\n",
        "            else:\n",
        "                output = net(outputs = model(input_ids=input_id, token_type_ids=token_type_id, attention_mask=attention_mask))\n",
        "\n",
        "            pred_lst.extend(output.argmax(dim=1).tolist())\n",
        "            logit.extend(output.tolist())\n",
        "\n",
        "    return pred_lst,logit\n",
        "\n",
        "# 현재 입력 데이터셋에 대해 추론\n",
        "def run_predict(model_path,test_dataloader):\n",
        "\n",
        "\n",
        "    print('set testloader')\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    net = RobertaForSequenceClassification.from_pretrained('klue/roberta-base', num_labels = 3)\n",
        "\n",
        "\n",
        "    net.to(device)\n",
        "\n",
        "    # gpu 여러개면 병렬 처리\n",
        "    if len(args.gpu)>1:\n",
        "        net = nn.DataParallel(net)\n",
        "\n",
        "    f = torch.load(model_path) #가중치 저장된 모델 upload\n",
        "    net.load_state_dict(f, strict=True)\n",
        "    print('load saved models')\n",
        "\n",
        "    preds, logit = do_predict(net, test_dataloader) #output\n",
        "\n",
        "    print('complete predict')\n",
        "\n",
        "    return preds, np.array(logit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355,
          "referenced_widgets": [
            "57ef3457eda54651a0f6b3814a3f90bc",
            "43f0357c6ff6460d8d73efa62acf252a",
            "a2aa1e7159a2446f8c422bb9dd796fa0",
            "b608f0b298624175b6e7b107b0ed2ba7",
            "a8bfe8cd52424fe09f2dc1ef18621979",
            "e3b7a0fd86eb4b58887c56efe8df707d",
            "e0ffba37a4a34724917db913b99e2b3a",
            "17b013ffe2564527ab6beddaecdcb1a2",
            "e9923ae7e2ab4274ba558220b18d75bd",
            "c2acfd52feb34328acfc0820d13ca810",
            "757086af102a4c1c994a7ca9f1d2877a"
          ]
        },
        "id": "vKW7DwxFNoJ8",
        "outputId": "2f66a5ea-e8d2-424c-fde1-099b0f6463d9",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "set testloader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-f1615a308d5c>:60: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler()\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-f1615a308d5c>:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  f = torch.load(model_path) #가중치 저장된 모델 upload\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load saved models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-f1615a308d5c>:36: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for batch_id, (input_id,token_type_id,attention_mask) in enumerate(tqdm_notebook(valid_loader)):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1342 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57ef3457eda54651a0f6b3814a3f90bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "<ipython-input-30-f1615a308d5c>:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(): #메모리 사용량 감소, 연산 속도 향상\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "complete predict\n"
          ]
        }
      ],
      "source": [
        "# 저장된 모델로 예측 수행\n",
        "preds1, logit1 = run_predict('/content/drive/MyDrive/BOAZ/23기 분석 BASE/미니프로젝트/감성분석/klue_base_fold6_s.pth',test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 label\n",
        "preds1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8t6FDBWmAr6",
        "outputId": "f5661bf1-8934-45d9-fb68-e3221bb04734"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cagupWp2hPAB"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 합쳐서 view\n",
        "result['라벨링'] = preds1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1a5yL24xhPAB",
        "outputId": "a2d6c4cf-b4eb-46e1-e6ef-b94134df27bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    index                                       kor_sentence  라벨링\n",
              "0       0  미국 라스베이거스에서 열리는 세계 최대 가전 전시회 2015를 깜짝 놀라게 할 제품...    1\n",
              "1       1  반도체 호황의 지속, 대형 패널 수요의 증가, 스마트폰의 반등 전문가들이 말하는 새...    1\n",
              "2       2  상저하고 상저하고 주가가 상반기에 부진하고 하반기에는 올라가는 , 지배구조 배당 관...    2\n",
              "3       3  삼성전자는 타이젠 운영체제를 기반으로 한 스마트를 6일 미국 라스베이거스에서 열리는...    0\n",
              "4       4  ◆ 시장서 재기 노리는 삼성전자 삼성전자가 올해 초 노트북 9 2015년 에디션 올...    2\n",
              "5       5  연말 연초를 맞아 재계가 임직원 희망퇴직을 받고. 새해에도 업황 부진에다 통상임금,...    2\n",
              "6       6  삼성전자가 지난해 처음 애플을 꺾고 미국인이 가장 좋아하는 스마트폰 제조사 자리에 ...    1\n",
              "7       7  역대 정부는 경제 활성화와 사회 통합을 위해 대통령 고유 권한인 사면 제도를 적절하...    1\n",
              "8       8  배우 이정재가 대상그룹 회장 장녀 임세령과의 열애설에 대해 공식 입장을 밝혔다. 1...    0\n",
              "9       9  삼성전자가 2일 서울 삼성전자 서초 사옥에서 권오현 대표이사 부회장을 비롯해 사장단...    1\n",
              "10     10  배우 이정재가 대상그룹 회장 장녀 임세령과의 열애설에 대해 공식 입장을 밝혔다. 1...    0\n",
              "11     11  삼성전자가 오는 6일부터 9일까지 미국 라스베이거스에서 열리는 가전 전시회에 커브드...    0\n",
              "12     12  삼성전자가 오는 6일부터 9일까지 미국 라스베가스에서 열리는 세계 최대 소비자 가전...    0\n",
              "13     13  지난해 동반 부진에 빠졌던 스마트폰 부품 업체들의 운명이 올해에 극명하게 갈릴 것으...    2\n",
              "14     14  코스피가 개인 투자자들의 매수세에 상승세를 꾀하고. 2일 오전 11시31분 현재 코...    1\n",
              "15     15  코스피가 개인과 외국인의 쌍끌이 매수세 속에 1920선을 회복했다. 2일 오후 1시...    1\n",
              "16     16  새해 첫 거래일인 2일 증권파생상품시장 개장식이 여의도 거래소에서 열렸다. 왼쪽부터...    1\n",
              "17     17  구본무 그룹 회장이 2일 서울 여의도 트윈타워에서 열린 그룹 신년 하례식에서 참석자...    1\n",
              "18     18  장외주식시장이 오름세를 나타냈다. 2일 장외주식 정보 제공 전문업체 프리스닥. . ...    0\n",
              "19     19  삼성, 현대차, , 비롯한 주요 그룹이 2015년 신년사를 통해 '위기, 창조, 상...    2\n",
              "20     20  국내 주요 그룹 총수와 최고경영자들이 제시한 새해 경영 화두는 위기를 돌파할 도전 ...    2\n",
              "21     21  올해 글로벌 스마트폰 시장에서 한국 제조업체들은 중고를 겪을 전망이다. 성장률이 둔...    2\n",
              "22     22  수출 한국의 효자 업종 반도체는 2015년에도 청신호다. 삼성전자와 하이닉스 한국 ...    1\n",
              "23     23  스마트폰 케이스 대세가 플라스틱 재질에서 금속 재질 메탈로 바뀌고. 비용은 재질에 ...    2\n",
              "24     24  자동화 검사 장비 국산화에 성공한 주역들이 들고. 왼쪽부터 전 응석 부장, 김일태 ...    0\n",
              "25     25  텔레콤은 6일 현지시간 미국 라스베이거스에서 개막하는 세계 최대 가전 전시회 201...    0\n",
              "26     26  세계 최대 가전 전시회 2015 개막을 이틀 앞둔 4일 현지시간 삼성전자 직원들이 ...    0\n",
              "27     27  는 소비자가 전쇼라는 이름답게 삼성전자 소니 완제품 업체들이 주인공이다. 쇼가 흥행...    0\n",
              "28     28  삼성그룹 지배구조상 정점에 위치해 주목받는 제일모직 주가가 새해 들어서도 급등세를 ...    1\n",
              "29     29  기업 지배구조가 순환출자 형태인지 지주회사 형태인지에 배당 수준에 차이가 큰 것으로...    0\n",
              "30     30  주 국내 증시는 대장 주인 삼성전자의 4분기 잠정 실적 발표를 주목해야 할 듯하다....    2\n",
              "31     31  수정구슬을 통해 미래를 들여 다 보고 싶은 인간의 욕망은 꺾을 수 없다. 우리는 2...    0\n",
              "32     32  세계 최대 가전 전시회인 ' 2015'가 6일 현지시간 미국 라스베이거스 컨벤션센터...    0\n",
              "33     33  코스피가 1900선 초반까지 하락했다. 외국인과 기관이 동반 '팔자'에 나서 190...    2\n",
              "34     34  장외주식시장이 소폭 오름세를 나타냈다. 5일 장외주식 정보 제공 전문업체 프리스닥....    0\n",
              "35     35  작년 말부터 감지된 이동통신사의 보조금 확대 분위기가 새해까지 이어지며 스마트폰 시...    1\n",
              "36     36  삼성전자가 세계 최대 전 박람회 사물인터넷 관련 기술을 선보인다는 소식에 사물인터넷...    1\n",
              "37     37  미국 라스베이거스에서 6일 현지시간 개막하는 세계 최대 가전 정보기술 전시회 201...    1\n",
              "38     38  미국 라스베이거스에서 6일 현지시간 개막하는 세계가 전 전시회 2015에 등장하는 ...    0\n",
              "39     39  교보증권이 올해 솔브레인의 수익성 회복이 기대된다며 목표가를 기존 3만7000원에서...    1\n",
              "40     40  연이 정보통신이 개장과 가격제한폭까지 치솟았다. 전일 5일 공시한 자사주 100만주...    1\n",
              "41     41  코스피가 국제유가 급락 그리스의 유로존 탈퇴 우려에 1900선 아래로 내려갔다. 6...    2\n",
              "42     42  임세령 대상그룹 상무와 배우 이정재의 러브스토리가 세간에 알려지면서 대상그룹 경영권...    1\n",
              "43     43  신세계. 드라마 미생의 인기에 매출이 크게 상승한 것으로 나타났다. 6일 신세계. ...    1\n",
              "44     44  코스피 1900선 붕괴코스피 1900선 붕괴 소식이 눈길을 끌고. 6일 오전 9시 ...    2\n",
              "45     45  코스피 지수 하락폭이 오후 들어 더 확대되는 모양새다. 6일 오후 1시 45분 현재...    2\n",
              "46     46  윤부근 삼성전자 대표이사가 5일 현지 미국 라스베이거스에서 열리는 북미 최대 가전 ...    1\n",
              "47     47  현대차가 영화에서 나올 법한 다양한 미래 자동차 신기술을 선보인다. 현대자동차는 6...    0\n",
              "48     48  국제유가가 급락하면서 글로벌 증시가 일제히 하락한데 이어 코스피도 1900선 밑으로...    2\n",
              "49     49  제일모직 주가가 이틀 새 약세를 보였다. 6일 유가 증권시장에서 제일모직은 전날보다...    2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d66c1fa-7179-4d29-be80-f8b4d3a07f28\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>kor_sentence</th>\n",
              "      <th>라벨링</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>미국 라스베이거스에서 열리는 세계 최대 가전 전시회 2015를 깜짝 놀라게 할 제품...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>반도체 호황의 지속, 대형 패널 수요의 증가, 스마트폰의 반등 전문가들이 말하는 새...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>상저하고 상저하고 주가가 상반기에 부진하고 하반기에는 올라가는 , 지배구조 배당 관...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>삼성전자는 타이젠 운영체제를 기반으로 한 스마트를 6일 미국 라스베이거스에서 열리는...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>◆ 시장서 재기 노리는 삼성전자 삼성전자가 올해 초 노트북 9 2015년 에디션 올...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>연말 연초를 맞아 재계가 임직원 희망퇴직을 받고. 새해에도 업황 부진에다 통상임금,...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>삼성전자가 지난해 처음 애플을 꺾고 미국인이 가장 좋아하는 스마트폰 제조사 자리에 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>역대 정부는 경제 활성화와 사회 통합을 위해 대통령 고유 권한인 사면 제도를 적절하...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>배우 이정재가 대상그룹 회장 장녀 임세령과의 열애설에 대해 공식 입장을 밝혔다. 1...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>삼성전자가 2일 서울 삼성전자 서초 사옥에서 권오현 대표이사 부회장을 비롯해 사장단...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>배우 이정재가 대상그룹 회장 장녀 임세령과의 열애설에 대해 공식 입장을 밝혔다. 1...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>삼성전자가 오는 6일부터 9일까지 미국 라스베이거스에서 열리는 가전 전시회에 커브드...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>삼성전자가 오는 6일부터 9일까지 미국 라스베가스에서 열리는 세계 최대 소비자 가전...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>지난해 동반 부진에 빠졌던 스마트폰 부품 업체들의 운명이 올해에 극명하게 갈릴 것으...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>코스피가 개인 투자자들의 매수세에 상승세를 꾀하고. 2일 오전 11시31분 현재 코...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>코스피가 개인과 외국인의 쌍끌이 매수세 속에 1920선을 회복했다. 2일 오후 1시...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>새해 첫 거래일인 2일 증권파생상품시장 개장식이 여의도 거래소에서 열렸다. 왼쪽부터...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>구본무 그룹 회장이 2일 서울 여의도 트윈타워에서 열린 그룹 신년 하례식에서 참석자...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>장외주식시장이 오름세를 나타냈다. 2일 장외주식 정보 제공 전문업체 프리스닥. . ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>삼성, 현대차, , 비롯한 주요 그룹이 2015년 신년사를 통해 '위기, 창조, 상...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>국내 주요 그룹 총수와 최고경영자들이 제시한 새해 경영 화두는 위기를 돌파할 도전 ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>올해 글로벌 스마트폰 시장에서 한국 제조업체들은 중고를 겪을 전망이다. 성장률이 둔...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>수출 한국의 효자 업종 반도체는 2015년에도 청신호다. 삼성전자와 하이닉스 한국 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>스마트폰 케이스 대세가 플라스틱 재질에서 금속 재질 메탈로 바뀌고. 비용은 재질에 ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>자동화 검사 장비 국산화에 성공한 주역들이 들고. 왼쪽부터 전 응석 부장, 김일태 ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>텔레콤은 6일 현지시간 미국 라스베이거스에서 개막하는 세계 최대 가전 전시회 201...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>세계 최대 가전 전시회 2015 개막을 이틀 앞둔 4일 현지시간 삼성전자 직원들이 ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>는 소비자가 전쇼라는 이름답게 삼성전자 소니 완제품 업체들이 주인공이다. 쇼가 흥행...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>삼성그룹 지배구조상 정점에 위치해 주목받는 제일모직 주가가 새해 들어서도 급등세를 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>기업 지배구조가 순환출자 형태인지 지주회사 형태인지에 배당 수준에 차이가 큰 것으로...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>30</td>\n",
              "      <td>주 국내 증시는 대장 주인 삼성전자의 4분기 잠정 실적 발표를 주목해야 할 듯하다....</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>31</td>\n",
              "      <td>수정구슬을 통해 미래를 들여 다 보고 싶은 인간의 욕망은 꺾을 수 없다. 우리는 2...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>32</td>\n",
              "      <td>세계 최대 가전 전시회인 ' 2015'가 6일 현지시간 미국 라스베이거스 컨벤션센터...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>33</td>\n",
              "      <td>코스피가 1900선 초반까지 하락했다. 외국인과 기관이 동반 '팔자'에 나서 190...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>34</td>\n",
              "      <td>장외주식시장이 소폭 오름세를 나타냈다. 5일 장외주식 정보 제공 전문업체 프리스닥....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>35</td>\n",
              "      <td>작년 말부터 감지된 이동통신사의 보조금 확대 분위기가 새해까지 이어지며 스마트폰 시...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>36</td>\n",
              "      <td>삼성전자가 세계 최대 전 박람회 사물인터넷 관련 기술을 선보인다는 소식에 사물인터넷...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>37</td>\n",
              "      <td>미국 라스베이거스에서 6일 현지시간 개막하는 세계 최대 가전 정보기술 전시회 201...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>38</td>\n",
              "      <td>미국 라스베이거스에서 6일 현지시간 개막하는 세계가 전 전시회 2015에 등장하는 ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>39</td>\n",
              "      <td>교보증권이 올해 솔브레인의 수익성 회복이 기대된다며 목표가를 기존 3만7000원에서...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>40</td>\n",
              "      <td>연이 정보통신이 개장과 가격제한폭까지 치솟았다. 전일 5일 공시한 자사주 100만주...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>41</td>\n",
              "      <td>코스피가 국제유가 급락 그리스의 유로존 탈퇴 우려에 1900선 아래로 내려갔다. 6...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>42</td>\n",
              "      <td>임세령 대상그룹 상무와 배우 이정재의 러브스토리가 세간에 알려지면서 대상그룹 경영권...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>43</td>\n",
              "      <td>신세계. 드라마 미생의 인기에 매출이 크게 상승한 것으로 나타났다. 6일 신세계. ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>44</td>\n",
              "      <td>코스피 1900선 붕괴코스피 1900선 붕괴 소식이 눈길을 끌고. 6일 오전 9시 ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>45</td>\n",
              "      <td>코스피 지수 하락폭이 오후 들어 더 확대되는 모양새다. 6일 오후 1시 45분 현재...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>46</td>\n",
              "      <td>윤부근 삼성전자 대표이사가 5일 현지 미국 라스베이거스에서 열리는 북미 최대 가전 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>47</td>\n",
              "      <td>현대차가 영화에서 나올 법한 다양한 미래 자동차 신기술을 선보인다. 현대자동차는 6...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>48</td>\n",
              "      <td>국제유가가 급락하면서 글로벌 증시가 일제히 하락한데 이어 코스피도 1900선 밑으로...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>49</td>\n",
              "      <td>제일모직 주가가 이틀 새 약세를 보였다. 6일 유가 증권시장에서 제일모직은 전날보다...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d66c1fa-7179-4d29-be80-f8b4d3a07f28')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3d66c1fa-7179-4d29-be80-f8b4d3a07f28 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3d66c1fa-7179-4d29-be80-f8b4d3a07f28');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e921aed9-d5e6-4f46-ab29-87b64efd21e6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e921aed9-d5e6-4f46-ab29-87b64efd21e6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e921aed9-d5e6-4f46-ab29-87b64efd21e6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "result",
              "summary": "{\n  \"name\": \"result\",\n  \"rows\": 42916,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12388,\n        \"min\": 0,\n        \"max\": 42915,\n        \"num_unique_values\": 42916,\n        \"samples\": [\n          21970,\n          2265,\n          35786\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"kor_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42777,\n        \"samples\": [\n          \"\\uc678\\uad6d\\uc778 \\uc21c\\ub9e4\\uc218 \\ubab0\\ub9ac\\uba70 6\\ub144 \\ub9cc\\uc5d0 \\uc0ac\\uc0c1 \\ucd5c\\uace0\\uce58 \\uacbd\\uc2e0 \\ucf54\\uc2a4\\ud53c\\uac00 2240\\uc120\\uc744 \\ubc1f\\uc73c\\uba70 \\uc0ac\\uc0c1 \\ucd5c\\uace0\\uce58 \\uae30\\ub85d\\uc744 \\ub2e4\\uc2dc \\uc37c\\ub2e4. 4\\uc77c \\uc720\\uac00 \\uc99d\\uad8c\\uc2dc\\uc7a5\\uc5d0\\uc11c \\ucf54\\uc2a4\\ud53c\\ub294 \\uc804\\ub0a0\\ubcf4\\ub2e4 21. 57\\ud3ec\\uc778\\ud2b80. 97% \\uc624\\ub978 2241. 24\\ub97c \\uae30\\ub85d\\ud558\\uba70 \\uc7a5\\uc744 \\ub9c8\\ucce4\\ub2e4. \\uc885\\uac00 \\uae30\\uc900 \\uc774\\uc804 \\ucd5c\\uace0\\uce58\\uc600\\ub358 2228. 962011\\ub144 5\\uc6d4 2\\uc77c\\uc744 \\ub3cc\\ud30c\\ud55c \\uac83\\uc740 \\ubb3c\\ub860\\uc774\\uace0 \\uae30\\uc874 \\uc7a5 \\uc911 \\ucd5c\\uace0\\uce58 \\uae30\\ub85d 2011\\ub144 4\\uc6d4 27\\uc77c 2231. 94\\ub3c4 \\ub118\\uc5b4\\uc120 \\uac83\\uc774\\ub2e4. \\uc774\\ub85c\\uc368 \\ud55c\\uad6d \\uc720\\uac00\\uc99d\\uad8c\\uc2dc\\uc7a5\\uc740 '\\ubc15\\uc2a4\\ud53c\\ubc15\\uc2a4\\uad8c\\uc5d0 \\uac16\\ud78c \\ucf54\\uc2a4\\ud53c'\\ub77c\\ub294 \\uc624\\uba85\\uc744 6\\ub144 \\ub9cc\\uc5d0 \\ub5bc\\uac8c \\ub410\\ub2e4. \\uc774\\ub0a0 \\ud55c\\uad6d\\uc99d\\uc2dc\\uc758 \\uc2dc\\uac00 \\ucd1d\\uc561 \\uae30\\ub85d\\ub3c4 \\uacbd\\uc2e0\\ub410\\ub2e4. \\ucf54\\uc2a4\\ud53c \\uc2dc\\uac00\\ucd1d\\uc561\\uc740 \\ub0a0 \\uc885\\uac00 1454\\uc8705775\\uc5b5\\uc6d0\\uc73c\\ub85c \\uc9d1\\uacc4\\ub410\\ub2e4. \\uc804\\ub0a0 1441\\uc870 1544\\uc5b5\\uc6d0\\uc774\\uc5c8\\ub358 \\uc2dc\\uac00\\ucd1d\\uc561\\uc740 \\ub0a0 13\\uc870\\uc6d0 \\uac00\\ub7c9 \\ub298\\uc5b4\\ub098\\uba74\\uc11c \\ucc98\\uc74c\\uc73c\\ub85c 1450\\uc870\\uc6d0\\uc744 \\ub118\\uc5b4\\uc11c\\uac8c \\ub41c \\uac83\\uc774\\ub2e4. \\uc99d\\uc2dc \\uc804\\ubb38\\uac00\\ub4e4\\uc740 \\uae00\\ub85c\\ubc8c \\uacbd\\uae30\\uac00 \\ud638\\uc870\\ub97c \\ubcf4\\uc774\\uace0 \\uae30\\uc5c5 \\uc2e4\\uc801\\uc774 \\uc774\\ub97c \\ub4b7\\ubc1b\\uce68\\ud558\\uba74\\uc11c \\uc678\\uad6d\\uc778 \\uc790\\uae08\\uc774 \\ucf54\\uc2a4\\ud53c\\ub97c \\ub808\\ubca8\\uc5c5 \\uc2dc\\ucf30\\ub2e4\\uace0 \\ubd84\\uc11d\\ud588\\ub2e4. \\uc774\\ub0a0\\ub3c4 \\ucf54\\uc2a4\\ud53c \\uae09\\ub4f1\\uc758 1 \\uacf5\\uc2e0\\uc740 \\uc678\\uad6d\\uc778 \\ud22c\\uc790\\uc790\\uc600\\ub2e4. \\uc678\\uad6d\\uc778\\uc740 \\ub0a0 \\uc720\\uac00\\uc99d\\uad8c\\uc2dc\\uc7a5\\uc5d0\\uc11c 3614\\uc5b5\\uc6d0 \\uc5b4\\uce58\\ub97c \\uc21c\\ub9e4\\uc218\\ud558\\uba74\\uc11c \\uc9c0\\uc218 \\uc0c1\\uc2b9\\uc744 \\ub04c\\uc5c8\\ub2e4. \\uae30\\uad00\\uacfc \\uac1c\\uc778 \\ud22c\\uc790\\uc790\\ub4e4\\uc774 3336\\uc5b5\\uc6d0, 709\\uc5b5\\uc6d0 \\uc5b4\\uce58\\ub97c \\uc21c\\ub9e4\\ub3c4\\ud588\\uc9c0\\ub9cc \\uc678\\uad6d\\uc778\\uc740 \\uc774\\ud2c0\\uc9f8 '\\ubc14\\uc774\\ucf54\\ub9ac\\uc544'\\ub97c \\uc774\\uc5b4\\uac14\\ub2e4. \\uc678\\uad6d\\uc778\\uc740 \\ud55c\\ubc18\\ub3c4\\ub97c \\ub458\\ub7ec\\uc2fc \\uc9c0\\uc815\\ud559\\uc801 \\uc704\\ud5d8\\uc774 \\ucee4\\uc84c\\uc74c\\uc5d0\\ub3c4 \\uc9c0\\ub09c\\ub2ec 20\\uc77c\\ubd80\\ud130 \\ucf54\\uc2a4\\ud53c\\uc5d0\\uc11c \\ub9e4\\uc218 \\uae30\\uc870\\ub97c \\uc720\\uc9c0\\ud574\\uc624\\uba74\\uc11c \\ub0a0\\uae4c\\uc9c0 2\\uc8701000\\uc5b5\\uc6d0 \\uc5b4\\uce58\\ub97c \\uc21c\\ub9e4\\uc218\\ud588\\ub2e4. \\uadf8\\ub9cc\\ud07c \\ud55c\\uad6d \\uae30\\uc5c5 \\uc2e4\\uc801 \\ud638\\uc870\\uc640 \\uae30\\uc5c5\\uc9c0\\ubc30\\uad6c\\uc870 \\uac1c\\uc120\\uc5d0 \\ub300\\ud55c \\ubbff\\uc74c\\uc774 \\ud070 \\uac83\\uc73c\\ub85c \\ud480\\uc774\\ub41c\\ub2e4. \\ucf54\\uc2a4\\ud53c\\uac00 \\ub9ce\\uc774 \\uc62c\\ub790\\ub2e4\\uace0 \\uc99d\\uc2dc\\uc5d0\\uc11c\\ub294 \\uc2e4\\uc81c \\uc2e4\\uc801 \\uac1c\\uc120\\uc5d0 \\ub35c \\uc624\\ub978 \\uac83\\uc73c\\ub85c \\ud3c9\\uac00\\uac00 \\ub098\\uc624\\uace0. \\ud55c\\uad6d \\uc8fc\\uc2dd\\uc774 \\uc77c\\ubcf8\\uc774\\ub098 \\ub300\\ub9cc\\uc5d0 \\ube44\\ud574 \\uc2f8\\ub2e4\\ub294 \\ubd84\\uc11d\\uc774 \\ub098\\uc62c \\uc815\\ub3c4\\ub2e4. \\ucf54\\uc2a4\\ud53c \\uc0c1\\uc7a5\\uc0ac\\uc758 \\uc5f0\\uac04 \\uc21c\\uc774\\uc775\\uc740 \\uc9c0\\ub09c\\ud574 94\\uc870\\uc6d0\\uc744 \\uae30\\ub85d\\ud55c \\uc774\\ud6c4 \\uc62c\\ud574\\ub294 120\\uc870\\uc6d0\\uc744 \\ub3cc\\ud30c\\ud560 \\uac83\\uc73c\\ub85c \\uc608\\uc0c1\\ub418\\uae30 \\ub54c\\ubb38\\uc774\\ub2e4. \\uc2dc\\uac00\\ucd1d\\uc561 1\\uc704\\uc778 \\uc0bc\\uc131\\uc804\\uc790\\uac00 \\uc5f0\\uc77c \\uc0ac\\uc0c1 \\ucd5c\\uace0\\uac00\\ub97c \\uae30\\ub85d\\ud558\\uace0 \\uc788\\ub294 \\uac83\\ub3c4 \\ucf54\\uc2a4\\ud53c \\ub808\\ubca8\\uc5c5\\uc758 \\uc8fc\\uc694\\ud55c \\uc6d0\\uc778\\uc774\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uc62c\\ud574 \\uae5c\\uc9dd \\uc2e4\\uc801\\uc744 \\uae30\\ub85d\\ud560 \\uac83\\uc73c\\ub85c \\uc804\\ub9dd\\ub418\\uba74\\uc11c \\uc9c0\\ub09c\\ud574 \\uc774\\ud6c4 \\uc8fc\\uac00\\uac00 30% \\uc62c\\ub790\\ub2e4. \\uc5ec\\uae30\\uc5d0 \\uc790\\uc0ac\\uc8fc \\uc18c\\uac01\\uc744 \\ube44\\ub86f\\ud55c \\uc8fc\\uc8fc \\ud658\\uc6d0\\ucc45\\uae4c\\uc9c0 \\ub0b4\\ub193\\uc790 \\uc0bc\\uc131\\uc804\\uc790 \\uc8fc\\uac00\\ub294 227\\ub9cc\\uc6d0\\uc120\\uc744 \\ub3cc\\ud30c\\ud55c \\uc0c1\\ud0dc\\ub2e4. \\ucd5c\\ud604\\ub9cc \\ubbf8\\ub798\\uc5d0\\uc14b\\ub300\\uc6b0 \\uc218\\uc11d\\ubd80\\ud68c\\uc7a5\\uc740 \\\"\\uc218\\ucd9c\\uacfc \\ud22c\\uc790\\ub97c \\uc911\\uc2ec\\uc73c\\ub85c \\ud55c \\uacbd\\uae30\\uc9c0\\ud45c\\uac00 \\uac1c\\uc120\\ub418\\uace0 \\uc788\\ub294 \\ub370\\ub2e4 \\uc0c1\\uc7a5 \\uae30\\uc5c5\\ub4e4\\uc758 \\uc21c\\uc774\\uc775\\ub3c4 \\uc0ac\\uc0c1 \\ucd5c\\ub300\\ub97c \\ub118\\uc5b4\\uc124 \\uac83\\uc73c\\ub85c \\uc608\\uc0c1\\ub418\\uba74\\uc11c \\uc62c 2\\ubd84\\uae30 \\uc774\\ud6c4 \\ucf54\\uc2a4\\ud53c \\uc804\\ub9dd\\uc740 \\ub354\\uc6b1 \\ubc1d\\ub2e4\\\"\\uace0 \\ub0b4\\ub2e4\\ubd24\\ub2e4.\",\n          \"\\uc0bc\\uc131\\uc804\\uc790\\uac00 26\\uc77c \\ubbf8\\uad6d '\\uc0bc\\uc131 \\uac1c\\ubc1c\\uc790 \\ucf58\\ud37c\\ub7f0\\uc2a4 202121'\\uc744 \\uc5f4\\uace0 \\ud601\\uc2e0\\uc801\\uc778 \\uace0\\uac1d \\uacbd\\ud5d8\\uc744 \\uac00\\ub2a5\\ud558\\uac8c \\ud558\\ub294 \\ucc28\\uc138\\ub300 \\uae30\\uc220\\ub4e4\\uc744 \\ub17c\\uc758\\ud588\\ub2e4. 2019\\ub144 \\uc774\\ud6c4 2\\ub144 \\ub9cc\\uc5d0 \\uc5f4\\ub9b0 \\ud589\\uc0ac\\ub294 \\uc628\\ub77c\\uc778\\uc73c\\ub85c \\uac1c\\ucd5c\\ub410\\ub2e4. \\uc774\\ub0a0 \\ud589\\uc0ac\\uc5d0\\uc11c\\ub294 \\u25b2\\uc778\\uacf5\\uc9c0\\ub2a5\\uc0ac\\ubb3c\\uc778\\ud130\\ub137 \\ubcf4\\uc548 \\ud50c\\ub7ab\\ud3fc \\ud601\\uc2e0 \\u25b2\\ud0c0\\uc774\\uc820 \\uae30\\ubc18\\uc758 \\uc2a4\\ud06c\\ub9b0 \\uacbd\\ud5d8 \\ud655\\uc7a5 \\u25b2\\uac24\\ub7ed\\uc2dc \\uc0dd\\ud0dc\\uacc4 \\uac15\\ud654 \\ubc29\\uc548\\uc5d0 \\ub300\\ud574 \\uc804 \\uc138\\uacc4 \\uac1c\\ubc1c\\uc790, \\ud30c\\ud2b8\\ub108\\uc0ac, \\uc18c\\ube44\\uc790\\ub4e4\\uc5d0\\uac8c \\uacf5\\uac1c\\ud588\\ub2e4. \\ud2b9\\ud788, \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uc18c\\ube44\\uc790 \\uc911\\uc2ec \\ud601\\uc2e0\\uc758 \\uc911\\uc694\\uc131\\uc744 \\uac15\\uc870\\ud558\\uba70, \\ube45\\uc2a4\\ube44, \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4, \\uc0bc\\uc131 \\ub179\\uc2a4 \\ud50c\\ub7ab\\ud3fc\\uc758 \\ubc1c\\uc804\\uc774 \\uc18c\\ube44\\uc790\\ub4e4\\uc758 \\uc2e4\\uc0dd\\ud65c\\uc5d0 \\ub3c4\\uc6c0\\uc744 \\uc904 \\uc218 \\uc788\\ub294 \\ubc29\\uc548\\ub4e4\\uc744 \\uc120 \\ubcf4\\uc600\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790 \\uace0\\ub3d9\\uc9c4 \\ub300\\ud45c\\uc774\\uc0ac \\uc0ac\\uc7a5\\uc740 \\ub0a0 \\uae30\\uc870\\uc5f0\\uc124\\uc5d0\\uc11c \\\"\\uc0bc\\uc131\\uc804\\uc790\\uac00 \\uc804 \\uc138\\uacc4 \\ucc3d\\uc758\\uc801\\uc778 \\uac1c\\ubc1c\\uc790\\ub4e4\\uacfc \\ud611\\ub825\\ud574 \\uc0c8\\ub85c\\uc6b4 \\uc2dc\\ub300\\ub97c \\uc5f4\\uc5b4 \\ub098\\uac08 \\uc218 \\uc788\\uc5b4 \\uc790\\ubd80\\uc2ec\\uc744 \\ub290\\ub080\\ub2e4\\\"\\uba70 \\\"\\uc218\\ub9ce\\uc740 \\uae30\\uae30\\uc640 \\ub124\\ud2b8\\uc6cc\\ud06c\\ub97c \\ud1b5\\ud574 \\uc628 \\uc138\\uc0c1\\uc774 \\ubc00 \\uc811\\ud558\\uac8c \\uc5f0\\uacb0\\ub418\\uace0 \\uc788\\ub294 \\ubaa8\\ub450\\uc758 \\uc0b6\\uc744 \\ub354 \\ud3b8\\ub9ac\\ud558\\uace0 \\uc2a4\\ub9c8\\ud2b8\\ud558\\uac8c \\ub9cc\\ub4e4 \\uc218 \\uc788\\ub3c4\\ub85d \\ud601\\uc2e0\\uc744 \\uc9c0\\uc18d\\ud558\\uaca0\\ub2e4\\\"\\ub77c\\uace0 \\ub9d0\\ud588\\ub2e4. \\uc18c\\ube44\\uc790 \\uacbd\\ud5d8 \\ud5a5\\uc0c1 \\uac1c\\ubc1c\\uc790 \\uc791\\uc5c5 \\uc9c0\\uc6d0 \\ud50c\\ub7ab\\ud3fc \\uac15\\ud654 \\ubc29\\uc548 \\uacf5\\uac1c \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\ubc88 \\ucf58\\ud37c\\ub7f0\\uc2a4\\uc5d0\\uc11c \\uc18c\\ube44\\uc790 \\uacbd\\ud5d8\\uc744 \\ud5a5\\uc0c1\\uc2dc\\ud0a4\\uace0 \\uac1c\\ubc1c\\uc790\\ub4e4\\uc758 \\uc791\\uc5c5\\uc744 \\uc9c0\\uc6d0\\ud558\\uae30 \\uc704\\ud55c \\ube45\\uc2a4\\ube44, \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4, \\uc0bc\\uc131 \\ub179\\uc2a4 \\ud50c\\ub7ab\\ud3fc \\uac15\\ud654 \\ubc29\\uc548\\uc744 \\uacf5\\uac1c\\ud588\\ub2e4. 3\\uc5b5 \\ub300 \\uc774\\uc0c1\\uc758 \\uc0bc\\uc131 \\uae30\\uae30\\uc5d0 \\uc801\\uc6a9\\ub41c \\ud50c\\ub7ab\\ud3fc \\ube45\\uc2a4\\ube44\\ub294 \\uc131\\ub2a5\\uc774 \\ub354\\uc6b1 \\uac15\\ud654\\ub41c\\ub2e4. \\ube45\\uc2a4\\ube44\\ub294 \\ucd5c\\uc2e0 \\uc2a4\\ub9c8\\ud2b8\\ud3f0\\uc5d0\\uc11c \\uc628\\ub514\\ubc14\\uc774\\uc2a4 \\ud65c\\uc6a9\\ud574 \\uae30\\uae30 \\uc790\\uccb4\\uc5d0\\uc11c \\uba85\\ub839\\uc744 \\uc2e4\\ud589\\ud568\\uc73c\\ub85c\\uc368 \\ubc18\\uc751 \\uc18d\\ub3c4\\uac00 \\ucd5c\\ub300 35% \\ube68\\ub77c\\uc84c\\ub2e4. \\ud50c\\ub7ab\\ud3fc\\uc778 \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4\\uc758 \\uc5f0\\uacb0\\uc131\\uacfc \\ud638\\ud658 \\uae30\\uae30\\ub3c4 \\ub354\\uc6b1 \\ud655\\ub300\\ub41c\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\ub294 '\\uc2a4\\ub9c8\\ud2b8\\uc2f1\\uc2a4 \\ud5c8\\ube0c ' \\uc18c\\ud504\\ud2b8\\uc6e8\\uc5b4\\ub97c \\ud328\\ubc00\\ub9ac \\ud5c8\\ube0c \\ub0c9\\uc7a5\\uace0 \\uc0bc\\uc131 \\uc81c\\ud488\\uc5d0 \\ud0d1\\uc7ac\\ud574, \\ud574\\ub2f9 \\uc81c\\ud488\\uc5d0\\uc11c \\uae30\\uae30\\uc640 \\uc5f0\\uacb0\\ud560 \\uc218 \\uc788\\ub3c4\\ub85d \\uc9c0\\uc6d0\\ud560 \\uacc4\\ud68d\\uc774\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uac1c\\ubc1c\\uc790\\ub4e4\\uc774 \\ube45\\uc2a4\\ube44\\uc640 \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4 \\uac04 \\uc5f0\\uacc4\\ub97c \\uac15\\ud654\\ud560 \\uc218 \\uc788\\ub294 '\\ube45\\uc2a4\\ube44 \\ud648' \\ud50c\\ub7ab\\ud3fc\\ub3c4 \\uc120\\ubcf4\\uc600\\ub2e4. '\\ube45\\uc2a4\\ube44 \\ud648' \\ud50c\\ub7ab\\ud3fc\\uc740 \\ud604\\uc7ac \\uc5f0\\uacb0\\ub3fc \\uc788\\ub294 \\uc81c\\ud488\\ub4e4\\uc758 \\uc0c1\\ud0dc\\ub97c \\uace0\\ub824\\ud574 \\uc74c\\uc131 \\uba85\\ub839\\uc744 \\uc218\\ud589\\ud560 \\uc218. \\\"\\uc601\\ud654 \\uc7ac\\uc0dd\\ud574\\uc918\\\"\\ub77c\\uace0 \\ub9d0\\ud558\\uba74 \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4\\uc5d0 \\uc5f0\\uacb0\\ub41c , \\uc0ac\\uc6b4\\ub4dc\\ubc14, \\uc870\\uba85 \\uac19\\uc740 \\uae30\\uae30\\ub4e4\\uc774 \\uc791\\ub3d9\\ud574 \\uc2a4\\ub9c8\\ud2b8\\ud648 \\uacbd\\ud5d8\\uc744 \\ub354\\uc6b1 \\ud3b8\\ub9ac\\ud558\\uac8c \\ud574\\uc900\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4\\uc5d0 \\uc5c5\\uacc4 \\ucd5c\\uc2e0 \\ud1b5\\uc2e0 \\uaddc\\uaca9\\uc778 '\\ub9e4\\ud130'\\ub97c \\uc801\\uc6a9\\ud574, \\ud30c\\ud2b8\\ub108\\uc0ac\\ub97c \\ud655\\ub300\\ud558\\uace0 \\ud638\\ud658 \\uae30\\uae30\\ub4e4\\uc774 \\ub354 \\uc27d\\uac8c \\uc5f0\\uacb0\\ub418\\ub3c4\\ub85d \\ud560 \\uacc4\\ud68d\\uc774\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\ubcf4\\uc548\\uc5d0 \\uc788\\uc5b4\\uc11c \\u25b2\\ubcf4\\ud638 \\u25b2\\uc120\\ud0dd \\u25b2\\ud22c\\uba85\\uc131\\uc758 3\\ub300 \\uc6d0\\uce59\\uc744 \\uc900\\uc218\\ud558\\uace0 \\uc18c\\ube44\\uc790, \\uac1c\\ubc1c\\uc790, \\ud30c\\ud2b8\\ub108\\uc0ac\\uc5d0\\uac8c \\ub370\\uc774\\ud130 \\uad00\\ub9ac \\ubc29\\ubc95\\uc744 \\uacb0\\uc815\\ud558\\uae30 \\uc704\\ud574 \\ud544\\uc694\\ud55c \\uc815\\ubcf4\\ub97c \\uc81c\\uacf5\\ud558\\uace0. \\ub610, \\ubcf4\\uc548 \\uae30\\uc220\\uc744 \\uc704\\ud55c \\ud611\\uc5c5\\uacfc \\uac1c\\ubc29\\ud615 \\ud601\\uc2e0 \\ucca0\\ud559\\uc744 \\uc911\\uc2dc\\ud558\\uba70, \\ubcf4\\uc548 \\ubd84\\uc11d \\ub3c4\\uad6c\\ub97c \\uc624\\ud508\\uc18c\\uc2a4 \\ud504\\ub85c\\uc81d\\ud2b8\\ub85c \\uc9c0\\uc18d \\ucd9c\\uc2dc\\ud560 \\uacc4\\ud68d\\uc774\\ub2e4. '\\uc2a4\\ud06c\\ub9b0 \\ud3ec \\uc62c' \\uc0ac\\uc6a9 \\uacbd\\ud5d8 \\ud655\\uc7a5\\uc0c8\\ub85c\\uc6b4 \\uae30\\ub2a5 \\uc18c\\uac1c '\\uc2a4\\ud06c\\ub9b0 \\ud3ec \\uc62c ' \\ube44\\uc804\\uc744 \\ubc14\\ud0d5\\uc73c\\ub85c \\uc138\\uacc4 \\uc2dc\\uc7a5\\uc744 \\uc120\\ub3c4 \\ud558\\ub294 \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uc0ac\\uc6a9 \\uacbd\\ud5d8\\uc744 \\ud655\\uc7a5\\ud558\\ub294 \\uc0c8\\ub85c\\uc6b4 \\uae30\\ub2a5\\ub4e4\\uc744 \\uc18c\\uac1c\\ud588\\ub2e4. \\ud654\\uc0c1\\ud1b5\\ud654\\uc758 \\uacbd\\uc6b0, \\uad6c\\uae00\\uacfc\\uc758 \\ud611\\ub825\\uc744 \\ud1b5\\ud574 \\uae30\\ubc18\\uc73c\\ub85c \\uc778\\ubb3c\\uc744 \\ud3ec\\ucee4\\uc2a4\\ud558\\uac70\\ub098 \\uc90c\\uc778\\ud558\\ub294 \\uae30\\ub2a5\\uc744 \\uc0ac\\uc6a9\\ud560 \\uc218 \\uc788\\uc5b4 \\ud3b8\\uc758\\uc131\\uc774 \\uac15\\ud654\\ub41c\\ub2e4. \\uc6a9 \\uc0bc\\uc131 \\ud5ec\\uc2a4\\uc5d0\\uc11c\\ub294 \\uc6f9\\ucea0, \\ubaa8\\ubc14\\uc77c \\uae30\\uae30\\uc640 \\uc5f0\\uacb0\\ud558\\uba74 \\uc2a4\\ub9c8\\ud2b8 \\ud2b8\\ub808\\uc774\\ub108\\uac00 \\uc0ac\\uc6a9\\uc790\\uc758 \\uc6b4\\ub3d9\\uc744 \\ubd84\\uc11d\\ud558\\uace0 \\uc2e4\\uc2dc\\uac04 \\ud53c\\ub4dc\\ubc31\\uc744 \\uc81c\\uacf5\\ud560 \\uc218. \\ucc28\\uc138\\ub300 \\ud654\\uc9c8 \\uae30\\uc220\\uc778 10 \\ud45c\\uc900\\uc774 \\uac8c\\uc784\\uc73c\\ub85c\\ub3c4 \\ud655\\ub300 \\uc801\\uc6a9\\ub3fc \\ub354 \\uc2e4\\uc801\\uc774\\uace0 \\ubab0\\uc785\\uac10 \\uc788\\ub294 \\uac8c\\uc784\\uc744 \\uc990\\uae38 \\uc218. \\uc0bc\\uc131\\uc804\\uc790\\ub294 2 \\uc11c\\ube44\\uc2a4\\uc778 '\\uae30\\uc5c5\\uc6a9 \\ud0c0\\uc774\\uc820 '\\uc744 \\uc120 \\ubcf4\\uc600\\ub2e4. \\uc11c\\ube44\\uc2a4\\ub97c \\ud1b5\\ud574 \\uc5ec\\ub7ec \\uae30\\uae30\\ub97c \\uc2e4\\uc2dc\\uac04\\uc73c\\ub85c \\ubaa8\\ub2c8\\ud130\\ub9c1\\ud558\\uace0 \\uc6d0\\uaca9 \\uc81c\\uc5b4\\ud560 \\uc218 \\uc788\\uc5b4 \\uae30\\uc5c5\\uae30\\uad00\\ub4e4\\uc740 \\ud559\\uad50\\uc758 \\uce60\\ud310, \\uc1fc\\ud551\\ubab0 \\ud0a4\\uc624\\uc2a4\\ud06c\\uc5d0\\uc11c \\uc9c0\\ud558\\ucca0\\uacfc \\uacf5\\ud56d\\uc758 \\uc2a4\\ud06c\\ub9b0\\uae4c\\uc9c0 \\ub2e4\\uc591\\ud55c \\ub514\\uc2a4\\ud50c\\ub808\\uc774\\ub97c \\ud3b8\\ub9ac\\ud558\\uac8c \\uad00\\ub9ac \\uac00\\ub2a5\\ud558\\ub2e4. '\\ud3f4\\ub354\\ube14\\ud3f0 \\uc6e8\\uc5b4\\ub7ec\\ube14 \\uace0\\uc720 \\uc0ac\\uc6a9\\uc790 \\uacbd\\ud5d8\\uc73c\\ub85c \\uac24\\ub7ed\\uc2dc \\uc0dd\\ud0dc\\uacc4 \\uac15\\ud654 \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uac24\\ub7ed\\uc2dc \\ubaa8\\ubc14\\uc77c \\uae30\\uae30\\uc758 \\ucd5c\\uc2e0 \\uc0ac\\uc6a9\\uc790 \\uacbd\\ud5d8\\uc778 '\\uc6d0 4 4'\\ub97c \\uacf5\\uac1c\\ud588\\ub2e4. \\uc6d0 4\\ub294 \\uc0ac\\uc6a9\\uc790\\uac00 \\uc790\\uc2e0\\ub9cc\\uc758 \\uacbd\\ud5d8\\uc744 \\ub9cc\\ub4e4 \\uc218 \\uc788\\ub3c4\\ub85d \\uc0c8\\ub85c\\uc6b4 \\ucc28\\uc6d0\\uc758 \\ub9de\\ucda4\\ud615 \\uae30\\ub2a5\\uc744 \\uc81c\\uacf5\\ud558\\uba70, \\ubcf4\\uc548 \\uba74\\uc5d0\\uc11c\\ub3c4 \\uc0ac\\uc6a9\\uc790\\uac00 \\uc815\\ubcf4 \\uc774\\uc6a9 \\uad8c\\ud55c\\uc744 \\uc870\\uc815\\ud574 \\uc790\\uc2e0\\uc758 \\ub370\\uc774\\ud130\\ub97c \\uad00\\ub9ac\\ud560 \\uc218 \\uc788\\ub3c4\\ub85d \\ud55c\\ub2e4. \\ub610\\ud55c, '\\uac24\\ub7ed\\uc2dc \\ud3f4\\ub4dc3' '\\uac24\\ub7ed\\uc2dc \\ud50c\\ub9bd3' \\ud3f4\\ub354\\ube14\\ud3f0\\uc73c\\ub85c \\ud3fc\\ud329\\ud130\\uac00 \\ud655\\uc7a5\\ub428\\uc5d0 \\uac1c\\ubc1c\\uc790\\ub4e4\\uc740 \\ud50c\\ub809\\uc2a4 \\ubaa8\\ub4dc, \\uba40\\ud2f0 \\uc561\\ud2f0\\ube0c \\uc708\\ub3c4\\uc6b0 \\uc0bc\\uc131 \\ud3f4\\ub354\\ube14\\ud3f0 \\uace0\\uc720\\uc758 \\uc0ac\\uc6a9 \\uacbd\\ud5d8\\uc744 \\ud65c\\uc6a9\\ud560 \\uc218. \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\ub0a0 \\ucf58\\ud37c\\ub7f0\\uc2a4\\uc5d0\\uc11c '\\uac24\\ub7ed\\uc2dc \\uc6cc\\uce584' \\uc2dc\\ub9ac\\uc988\\uc5d0 \\ud0d1\\uc7ac\\ub41c \\uc0bc\\uc131\\uc758 \\ub3c5\\uc790\\uc801\\uc778 \\uc0ac\\uc6a9\\uc790 \\uacbd\\ud5d8\\uc778 '\\uc6d0 \\uc6cc\\uce58', \\uad6c\\uae00\\uacfc \\uacf5\\ub3d9 \\uac1c\\ubc1c\\ud55c \\uc2e0\\uaddc \\uc6e8\\uc5b4\\ub7ec\\ube14 \\ud1b5\\ud569 \\ud50c\\ub7ab\\ud3fc\\ub3c4 \\uc18c\\uac1c\\ud588\\ub2e4. \\ud589\\uc0ac\\uc5d0 \\ub300\\ud55c \\uc790\\uc138\\ud55c \\ub0b4\\uc6a9\\uc740 \\uc0bc\\uc131 \\uac1c\\ubc1c\\uc790 \\ucf58\\ud37c\\ub7f0\\uc2a4 \\uacf5\\uc2dd \\uc6f9\\uc0ac\\uc774\\ud2b8\\uc5d0\\uc11c \\ud655\\uc778\\ud560 \\uc218.\",\n          \"\\uc0bc\\uc131\\uc804\\uc790 \\ubd80\\ubb38\\uc740 \\uacbd\\uae30\\uc0ac\\ud68c\\ubcf5\\uc9c0\\uacf5\\ub3d9\\ubaa8\\uae08\\ud68c\\uc640 \\uacbd\\uae30 \\ud654\\uc131\\uc6a9\\uc778 \\ud3c9\\ud0dd \\uc9c0\\uc5ed\\uc5d0\\uc11c '\\ud589\\ubcf5 \\ubaa8\\uc790\\uc774\\ud06c \\ud504\\ub85c\\uc81d\\ud2b8'\\ub97c \\uc2dc\\ud589\\ud55c\\ub2e4\\uace0 4\\uc77c \\ubc1d\\ud614\\ub2e4. \\ud589\\ubcf5 \\ubaa8\\uc790\\uc774\\ud06c\\ub294 \\uc9c0\\uc5ed\\uc0ac\\ud68c\\uc758 \\uc18c\\uc678\\ub41c \\uc774\\uc6c3\\uc774 \\ud589\\ubcf5\\ud560 \\uc218 \\uc788\\ub3c4\\ub85d \\ucc38\\uc2e0\\ud55c \\ud504\\ub85c\\uadf8\\ub7a8\\uc744 \\uc9c0\\uc5ed \\uc804\\ubb38\\uac00\\ub4e4\\uc774 \\ubc1c\\uad74\\ud558\\uba74, \\uc0bc\\uc131\\uc804\\uc790 \\ubd80\\ubb38 \\uc784\\uc9c1\\uc6d0\\ub4e4\\uc758 \\uae30\\ubd80\\uae08\\uc73c\\ub85c \\ud504\\ub85c\\uadf8\\ub7a8\\uc744 \\uc9c4\\ud589\\ud558\\ub294 \\uc0ac\\ud68c \\uacf5\\ud5cc\\uc0ac\\uc5c5\\uc774\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\uac00 \\uc9c0\\ub09c 4\\uc6d4 \\ud55c \\ub2ec\\uac04 \\uc9c0\\uc5ed\\uc0ac\\ud68c 700\\uc5ec \\uac1c \\uc0ac\\ud68c\\ubcf5\\uc9c0\\uae30\\uad00\\uc744 \\ub300\\uc0c1\\uc73c\\ub85c \\uacf5\\ubaa8\\ud55c \\ub4a4 \\uad50\\uc218\\uc640 \\uacf5\\ubb34\\uc6d0, \\uc0ac\\ud68c\\uacf5\\ud5cc \\uc804\\ubb38 \\ucee8\\uc124\\ud134\\ud2b8\\ub85c \\uad6c\\uc131\\ub41c \\uc2ec\\uc0ac\\ub2e8\\uc744 \\ud1b5\\ud574 13\\uac1c \\ud504\\ub85c\\uc81d\\ud2b8\\ub97c \\ud655\\uc815\\ud588\\ub2e4. \\ud504\\ub85c\\uc81d\\ud2b8\\ub294 \\uae30\\ud68d \\uc8fc\\uccb4\\uc778 \\ubcf5\\uc9c0\\uae30\\uad00\\uc5d0\\uc11c \\ub0b4\\ub144 6\\uc6d4\\uae4c\\uc9c0 1\\ub144\\uac04 \\uc6b4\\uc601\\ud558\\uac8c \\ub41c\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790 \\ubd80\\ubb38 \\uc0ac\\ud68c \\ubd09\\uc0ac\\ub2e8\\uc7a5 \\uae40\\uc120\\uc2dd \\uc804\\ubb34\\ub294 \\\"\\uc608\\uc0c1\\ubcf4\\ub2e4 \\ub9ce\\uc740 \\uc544\\uc774\\ub514\\uc5b4\\uac00 \\uc811\\uc218\\ub410\\ub2e4\\\"\\uba74\\uc11c \\\"\\ud504\\ub85c\\uc81d\\ud2b8 \\uc120\\uc815 \\uae30\\uad00\\ub4e4\\uc774 \\uc9c0\\uc5ed \\ubb38\\uc81c \\ud574\\uacb0\\uc5d0 \\ud070 \\uc5f4\\uc758\\ub97c \\ubcf4\\uc5ec \\uc88b\\uc740 \\uc131\\uacfc\\uac00 \\uae30\\ub300\\ub41c\\ub2e4\\\"\\uace0 \\ub9d0\\ud588\\ub2e4.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ub77c\\ubca8\\ub9c1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "result.head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "BNcTnNh8hPAE"
      },
      "outputs": [],
      "source": [
        "# 기존 데이터셋에 label feature 추가\n",
        "final_매일경제['label'] = preds1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_매일경제"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FiI1zWVEmOOH",
        "outputId": "94097f78-d5d4-442e-8049-5e582c4f5e23"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            index                                               link  \\\n",
              "0      2015000001  https://n.news.naver.com/mnews/article/009/000...   \n",
              "1      2015000002  https://n.news.naver.com/mnews/article/009/000...   \n",
              "2      2015000003  https://n.news.naver.com/mnews/article/009/000...   \n",
              "3      2015000005  https://n.news.naver.com/mnews/article/009/000...   \n",
              "4      2015000006  https://m.entertain.naver.com/article/009/0003...   \n",
              "...           ...                                                ...   \n",
              "42911  2024003350  https://n.news.naver.com/mnews/article/009/000...   \n",
              "42912  2024003351  https://n.news.naver.com/mnews/article/009/000...   \n",
              "42913  2024003352  https://n.news.naver.com/mnews/article/009/000...   \n",
              "42914  2024003353  https://n.news.naver.com/mnews/article/009/000...   \n",
              "42915  2024003354  https://n.news.naver.com/mnews/article/009/000...   \n",
              "\n",
              "                      date                     title  \\\n",
              "0      2015-01-01 04:03:00   CES 놀라게 할 Made In Korea   \n",
              "1      2015-01-01 04:03:00  삼성전자 바닥 치고 2분기부터 실적 개선될듯   \n",
              "2      2015-01-01 04:03:00  리서치센터장 5인이 보는 새해 증시 투자전략   \n",
              "3      2015-01-01 15:31:00      삼성전자 타이젠 TV CES에서 공개   \n",
              "4      2015-01-01 17:07:00          [매경e신문] 오늘의 프리미엄   \n",
              "...                    ...                       ...   \n",
              "42911  2024-08-14 17:53:00    AI 특화 스마트폰 구글도 본격 뛰어든다   \n",
              "42912  2024-08-14 18:02:00    돌아온 엔비디아…반도체·소부장 '기지개'   \n",
              "42913  2024-08-14 18:02:00   삼성전자 소액주주 1년새 142만명 줄었다   \n",
              "42914  2024-08-14 18:03:00       삼성전자 2분기 R&D에 8조 썼다   \n",
              "42915  2024-08-14 18:05:00       美 법무부 칼날에 구글 쪼개질 신세   \n",
              "\n",
              "                                                    body  year  month  day  \\\n",
              "0      미국 라스베이거스에서 열리는 세계 최대 가전전시회 CES 2015를 깜짝 놀라게 할...  2015      1    1   \n",
              "1      ◆ 2015 업종별 유망주 ① IT·전자 ◆‘반도체 호황의 지속, 대형 패널 수요의...  2015      1    1   \n",
              "2      ‘상저하고(上低下高·주가가 상반기에 부진하고 하반기에는 올라가는 것 ), 지배구조·...  2015      1    1   \n",
              "3      삼성전자는 타이젠 운영체제(OS)를 기반으로 한 스마트TV를 6일 미국 라스베이거스...  2015      1    1   \n",
              "4      ◆ PC시장서 재기 노리는 삼성전자삼성전자가 올해 초 ‘노트북9 2015년 에디션’...  2015      1    1   \n",
              "...                                                  ...   ...    ...  ...   \n",
              "42911  제미나이 탑재 픽셀9 공개애플보다 출시 두달 앞당겨OS동맹 삼성과 휴대폰 경쟁갤S2...  2024      8   14   \n",
              "42912  美증시 AI관련주 강세에엔비디아 이달 17% 상승이턴 등 전력인프라도 '쑥'외국인,...  2024      8   14   \n",
              "42913  올들어 주가 급등락하자566만명서 424만명으로'국민주' 삼성전자의 소액주주가 45...  2024      8   14   \n",
              "42914  작년동기보다 12% 늘어 최대반도체 시설투자도 10조 달해삼성전자가 인공지능(AI)...  2024      8   14   \n",
              "42915  정부, 구글 반독점 정조준크롬·안드로이드 분리 요청MS 기업 분할 실패 이후20년만...  2024      8   14   \n",
              "\n",
              "       hour  minute               cleaned_title  \\\n",
              "0         4       3                      놀라게 할.   \n",
              "1         4       3  삼성전자 바닥 치고 2분기부터 실적 개선될 듯.   \n",
              "2         4       3   리서치센터장 5인이 보는 새해 증시 투자전략.   \n",
              "3        15      31                삼성전자 타이젠 공개.   \n",
              "4        17       7                   오늘의 프리미엄.   \n",
              "...     ...     ...                         ...   \n",
              "42911    17      53        특화 스마트폰 구글도 본격 뛰어든다.   \n",
              "42912    18       2     돌아온 엔비디아 반도체 소부장 '기지개'.   \n",
              "42913    18       2   삼성전자 소액주주 1년 새 142만명 줄었다.   \n",
              "42914    18       3             삼성전자 2분기 8조 썼다.   \n",
              "42915    18       5        미 법무부 칼날에 구글 쪼개질 신세.   \n",
              "\n",
              "                                            cleaned_body  label  \n",
              "0      미국 라스베이거스에서 열리는 세계 최대 가전 전시회 2015를 깜짝 놀라게 할 제품...      1  \n",
              "1      반도체 호황의 지속, 대형 패널 수요의 증가, 스마트폰의 반등 전문가들이 말하는 새...      1  \n",
              "2      상저하고 상저하고 주가가 상반기에 부진하고 하반기에는 올라가는 , 지배구조 배당 관...      2  \n",
              "3      삼성전자는 타이젠 운영체제를 기반으로 한 스마트를 6일 미국 라스베이거스에서 열리는...      0  \n",
              "4      ◆ 시장서 재기 노리는 삼성전자 삼성전자가 올해 초 노트북 9 2015년 에디션 올...      2  \n",
              "...                                                  ...    ...  \n",
              "42911  제미나이 탑재 픽셀 9 공개 애플보다 출 시 두 달 앞당겨 동맹 삼성과 휴대폰 경쟁...      0  \n",
              "42912  미 증시 관련주 강세에 엔비디아 달 17% 상승이 턴 전력 인프라도 '쑥' 외국인,...      1  \n",
              "42913  올 들어 주가 급등락하자 566만명 서 424만명으로 '국민주' 삼성전자의 소액주주...      2  \n",
              "42914  작년 동기보다 12% 늘어 최대 반도체 시설 투자도 10조 달해 삼성전자가 인공지능...      1  \n",
              "42915  정부, 구글 반독점 정 조준 크롬 안드로이드 분리 요청 기업 분할 실패 이후 20년...      0  \n",
              "\n",
              "[42916 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-62d3dbfc-06b6-4ccf-b29d-28fef7f62c41\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>link</th>\n",
              "      <th>date</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>minute</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_body</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015000001</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2015-01-01 04:03:00</td>\n",
              "      <td>CES 놀라게 할 Made In Korea</td>\n",
              "      <td>미국 라스베이거스에서 열리는 세계 최대 가전전시회 CES 2015를 깜짝 놀라게 할...</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>놀라게 할.</td>\n",
              "      <td>미국 라스베이거스에서 열리는 세계 최대 가전 전시회 2015를 깜짝 놀라게 할 제품...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015000002</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2015-01-01 04:03:00</td>\n",
              "      <td>삼성전자 바닥 치고 2분기부터 실적 개선될듯</td>\n",
              "      <td>◆ 2015 업종별 유망주 ① IT·전자 ◆‘반도체 호황의 지속, 대형 패널 수요의...</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>삼성전자 바닥 치고 2분기부터 실적 개선될 듯.</td>\n",
              "      <td>반도체 호황의 지속, 대형 패널 수요의 증가, 스마트폰의 반등 전문가들이 말하는 새...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015000003</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2015-01-01 04:03:00</td>\n",
              "      <td>리서치센터장 5인이 보는 새해 증시 투자전략</td>\n",
              "      <td>‘상저하고(上低下高·주가가 상반기에 부진하고 하반기에는 올라가는 것 ), 지배구조·...</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>리서치센터장 5인이 보는 새해 증시 투자전략.</td>\n",
              "      <td>상저하고 상저하고 주가가 상반기에 부진하고 하반기에는 올라가는 , 지배구조 배당 관...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015000005</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2015-01-01 15:31:00</td>\n",
              "      <td>삼성전자 타이젠 TV CES에서 공개</td>\n",
              "      <td>삼성전자는 타이젠 운영체제(OS)를 기반으로 한 스마트TV를 6일 미국 라스베이거스...</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>31</td>\n",
              "      <td>삼성전자 타이젠 공개.</td>\n",
              "      <td>삼성전자는 타이젠 운영체제를 기반으로 한 스마트를 6일 미국 라스베이거스에서 열리는...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015000006</td>\n",
              "      <td>https://m.entertain.naver.com/article/009/0003...</td>\n",
              "      <td>2015-01-01 17:07:00</td>\n",
              "      <td>[매경e신문] 오늘의 프리미엄</td>\n",
              "      <td>◆ PC시장서 재기 노리는 삼성전자삼성전자가 올해 초 ‘노트북9 2015년 에디션’...</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>7</td>\n",
              "      <td>오늘의 프리미엄.</td>\n",
              "      <td>◆ 시장서 재기 노리는 삼성전자 삼성전자가 올해 초 노트북 9 2015년 에디션 올...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42911</th>\n",
              "      <td>2024003350</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2024-08-14 17:53:00</td>\n",
              "      <td>AI 특화 스마트폰 구글도 본격 뛰어든다</td>\n",
              "      <td>제미나이 탑재 픽셀9 공개애플보다 출시 두달 앞당겨OS동맹 삼성과 휴대폰 경쟁갤S2...</td>\n",
              "      <td>2024</td>\n",
              "      <td>8</td>\n",
              "      <td>14</td>\n",
              "      <td>17</td>\n",
              "      <td>53</td>\n",
              "      <td>특화 스마트폰 구글도 본격 뛰어든다.</td>\n",
              "      <td>제미나이 탑재 픽셀 9 공개 애플보다 출 시 두 달 앞당겨 동맹 삼성과 휴대폰 경쟁...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42912</th>\n",
              "      <td>2024003351</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2024-08-14 18:02:00</td>\n",
              "      <td>돌아온 엔비디아…반도체·소부장 '기지개'</td>\n",
              "      <td>美증시 AI관련주 강세에엔비디아 이달 17% 상승이턴 등 전력인프라도 '쑥'외국인,...</td>\n",
              "      <td>2024</td>\n",
              "      <td>8</td>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>돌아온 엔비디아 반도체 소부장 '기지개'.</td>\n",
              "      <td>미 증시 관련주 강세에 엔비디아 달 17% 상승이 턴 전력 인프라도 '쑥' 외국인,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42913</th>\n",
              "      <td>2024003352</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2024-08-14 18:02:00</td>\n",
              "      <td>삼성전자 소액주주 1년새 142만명 줄었다</td>\n",
              "      <td>올들어 주가 급등락하자566만명서 424만명으로'국민주' 삼성전자의 소액주주가 45...</td>\n",
              "      <td>2024</td>\n",
              "      <td>8</td>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>삼성전자 소액주주 1년 새 142만명 줄었다.</td>\n",
              "      <td>올 들어 주가 급등락하자 566만명 서 424만명으로 '국민주' 삼성전자의 소액주주...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42914</th>\n",
              "      <td>2024003353</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2024-08-14 18:03:00</td>\n",
              "      <td>삼성전자 2분기 R&amp;D에 8조 썼다</td>\n",
              "      <td>작년동기보다 12% 늘어 최대반도체 시설투자도 10조 달해삼성전자가 인공지능(AI)...</td>\n",
              "      <td>2024</td>\n",
              "      <td>8</td>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "      <td>삼성전자 2분기 8조 썼다.</td>\n",
              "      <td>작년 동기보다 12% 늘어 최대 반도체 시설 투자도 10조 달해 삼성전자가 인공지능...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42915</th>\n",
              "      <td>2024003354</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
              "      <td>2024-08-14 18:05:00</td>\n",
              "      <td>美 법무부 칼날에 구글 쪼개질 신세</td>\n",
              "      <td>정부, 구글 반독점 정조준크롬·안드로이드 분리 요청MS 기업 분할 실패 이후20년만...</td>\n",
              "      <td>2024</td>\n",
              "      <td>8</td>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "      <td>미 법무부 칼날에 구글 쪼개질 신세.</td>\n",
              "      <td>정부, 구글 반독점 정 조준 크롬 안드로이드 분리 요청 기업 분할 실패 이후 20년...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>42916 rows × 13 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62d3dbfc-06b6-4ccf-b29d-28fef7f62c41')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-62d3dbfc-06b6-4ccf-b29d-28fef7f62c41 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-62d3dbfc-06b6-4ccf-b29d-28fef7f62c41');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5c533bb8-f544-43c2-9fd8-e068029e3b1d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5c533bb8-f544-43c2-9fd8-e068029e3b1d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5c533bb8-f544-43c2-9fd8-e068029e3b1d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f29c951c-a244-4764-9d5f-1db029daa96f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('final_매일경제')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f29c951c-a244-4764-9d5f-1db029daa96f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('final_매일경제');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "final_매일경제",
              "summary": "{\n  \"name\": \"final_\\ub9e4\\uc77c\\uacbd\\uc81c\",\n  \"rows\": 42916,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2822057,\n        \"min\": 2015000001,\n        \"max\": 2024003354,\n        \"num_unique_values\": 42909,\n        \"samples\": [\n          2017002671,\n          2015003286,\n          2020001132\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42870,\n        \"samples\": [\n          \"https://n.news.naver.com/mnews/article/009/0003942059?sid=101\",\n          \"https://n.news.naver.com/mnews/article/009/0003873438?sid=101\",\n          \"https://n.news.naver.com/mnews/article/009/0005337246?sid=101\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39163,\n        \"samples\": [\n          \"2015-08-07 13:56:00\",\n          \"2019-01-03 17:56:00\",\n          \"2016-12-20 20:48:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42532,\n        \"samples\": [\n          \"\\u2018\\uac24\\ub7ed\\uc2dc\\ub178\\ud2b87\\u2019 \\uacf5\\uac1c D-1\\u2026\\ud504\\ub9ac\\ubbf8\\uc5c4 \\uc2a4\\ub9c8\\ud2b8\\ud3f0 \\ub300\\uc804 \\ud3ec\\ubb38\",\n          \"[Hot-Line] \\\"\\uc0bc\\uc131\\uc804\\uae30, 3\\ubd84\\uae30 \\uc2e4\\uc801 \\uc2dc\\uc7a5\\ucd94\\uc815\\uce58 \\ub300\\ud3ed \\uc6c3\\ub3cc\\uc544\\\"\",\n          \"\\uc5f0\\uae30\\uae08, \\ubc18\\ub144\\ub9cc\\uc5d0 \\uc791\\ub144 \\uc2e4\\uc801 \\ub118\\uc5b4\\uc130\\ub2e4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42792,\n        \"samples\": [\n          \"\\ub9e4\\uacbd\\uc774 \\uc804\\ud558\\ub294 \\uc138\\uc0c1\\uc758 \\uc9c0\\uc2dd(\\ub9e4-\\uc138-\\uc9c0, 9\\uc6d4 21\\uc77c)1. \\ubb38\\uc7ac\\uc778 \\uc815\\ubd80\\uc758 '\\uc18c\\ub4dd\\uc8fc\\ub3c4 \\uc131\\uc7a5'\\uc774 \\uc5ed\\uc124\\uc801\\uc73c\\ub85c \\uc800\\uc18c\\ub4dd\\uce35\\uc758 \\uc77c\\uc790\\ub9ac\\ub97c \\ube7c\\uc557\\ub294 \\uc0ac\\ub840\\uac00 \\uc18d\\ucd9c\\ud568. \\uc815\\ubd80\\uc758 \\uc784\\uae08\\uc778\\uc0c1 \\uc555\\ubc15\\uc5d0 \\uc0dd\\uacc4\\uac00 \\uc5b4\\ub824\\uc6b4 \\uc790\\uc601\\uc5c5\\uc790\\ub4e4\\uc774 \\uace0\\uc6a9\\uc744 \\uc904\\uc774\\ub294 \\ubc29\\uc2dd\\uc73c\\ub85c \\ub300\\uc751\\ud558\\uace0 \\uc788\\uae30 \\ub54c\\ubb38\\uc784. \\uc800\\uc18c\\ub4dd \\uadfc\\ub85c\\uc790\\uac00 \\uc9d1\\uc911\\ub41c \\uc219\\ubc15\\u00b7\\uc74c\\uc2dd\\uc5c5 \\uace0\\uc6a9\\uc740 6\\uc6d4 \\uc774\\ud6c4 \\uc11d\\ub2ec \\uc5f0\\uc18d \\uac10\\uc18c\\ud568. \\u25b6\\ubc14\\ub85c\\uac00\\uae302. SK\\ud558\\uc774\\ub2c9\\uc2a4\\uac00 \\ucc38\\uc5ec\\ud55c '\\ud55c\\ubbf8\\uc77c \\uc5f0\\ud569'\\uc774 \\ub3c4\\uc2dc\\ubc14 \\ubc18\\ub3c4\\uccb4 \\uc0ac\\uc5c5 \\ubd80\\ubb38 \\uc778\\uc218\\uc790\\ub85c \\uacb0\\uc815\\ub428. \\ud55c\\ubbf8\\uc77c \\uc5f0\\ud569\\uc774 \\uc9c0\\ub09c 6\\uc6d4 \\uc6b0\\uc120\\ud611\\uc0c1\\ub300\\uc0c1\\uc790\\ub85c \\uc120\\uc815\\ub41c \\ud6c4 3\\uac1c\\uc6d4 \\ub9cc\\uc784. \\uc778\\uc218\\uac00\\uc561 \\uc57d 2\\uc870\\uc5d4 \\uc678\\uc5d0 \\uc124\\ube44\\ud22c\\uc790 \\uba85\\ubaa9\\uc73c\\ub85c \\uc5f0 4000\\uc5b5\\uc5d4\\uc744 \\ucd94\\uac00 \\uc81c\\uacf5\\ud558\\ub294 \\uc870\\uac74\\uc73c\\ub85c \\uc2b9\\uae30\\ub97c \\uc7a1\\uc74c. \\u25b6\\ubc14\\ub85c\\uac00\\uae30\\uae40\\uc0c1\\uc870 \\uacf5\\uc815\\uac70\\ub798\\uc704\\uc6d0\\uc7a5\\uc774 \\uc9c0\\ub09c 19\\uc77c \\uc624\\ud6c4 \\uc11c\\uc6b8 \\ucda9\\ubb34\\ub85c \\ub9e4\\uacbd\\ubbf8\\ub514\\uc5b4\\uc13c\\ud130\\uc5d0\\uc11c \\uc5f4\\ub9b0 '\\ub9e4\\uacbd \\uc774\\ucf54\\ub178\\ubbf8\\uc2a4\\ud2b8\\ud074\\ub7fd'\\uc5d0 \\ucc38\\uc11d\\ud574 \\uac15\\uc5f0\\uc744 \\ud558\\uace0 \\uc788\\ub2e4. \\uae40 \\uc704\\uc6d0\\uc7a5\\uc740 \\uc774\\ub0a0 \\uac15\\uc5f0\\uc5d0\\uc11c \\\"\\ub300\\uae30\\uc5c5\\uacfc 1\\ucc28 \\ud611\\ub825\\uc5c5\\uccb4 \\uac04 \\ubd88\\uacf5\\uc815\\uac70\\ub798\\ub294 \\ub9ce\\uc774 \\uac1c\\uc120\\ub410\\uc9c0\\ub9cc, 2\\ucc28\\u00b73\\ucc28 \\ud611\\ub825\\uc5c5\\uccb4\\uc5d0 \\ub300\\ud55c \\uac11\\uc9c8\\uc740 \\uac1c\\uc120\\ub418\\uc9c0 \\uc54a\\uace0 \\uc788\\uace0 \\ubc95\\ub960\\ub85c \\uac15\\uc81c\\ud558\\uae30\\ub3c4 \\uc5b4\\ub824\\uc6b4 \\ubb38\\uc81c\\\"\\ub77c\\uba70 \\\"\\uc6d0\\uc0ac\\uc5c5\\uc790\\uc778 \\ub300\\uae30\\uc5c5\\uc774 \\uc774 \\ubb38\\uc81c\\ub97c \\uac1c\\uc120\\ud558\\ub294 \\ubaa8\\ub378\\uc744 \\ub9cc\\ub4e4\\uc5b4\\uc92c\\uc73c\\uba74 \\ud55c\\ub2e4\\\"\\uace0 \\ub9d0\\ud588\\ub2e4. /\\uc0ac\\uc9c4=\\uae40\\ud638\\uc601 \\uae30\\uc7903. \\uae40\\uc0c1\\uc870 \\uacf5\\uc815\\uc704\\uc6d0\\uc7a5\\uc740 \\ub9e4\\uacbd \\uc774\\ucf54\\ub178\\ubbf8\\uc2a4\\ud2b8\\ud074\\ub7fd \\uac15\\uc5f0\\uc5d0\\uc11c \\\"\\ud604 \\uc815\\ubd80\\uac00 \\uacfc\\uac70\\ubcf4\\ub2e4 \\ud070 \\uc815\\ubd80\\uc778 \\uac83\\uc740 \\ub9de\\uc9c0\\ub9cc \\ubcc0\\ud654 \\ucd08\\uae30\\ub2e8\\uacc4\\uc758 \\uc624\\ubc84\\uc288\\ud305\\\"\\uc774\\ub77c\\uace0 \\ubc1d\\ud798. \\ub610 \\\"\\uc77c\\uc815 \\uc2dc\\uc810 \\uc774\\ud6c4 \\uc801\\uc808\\ud55c \\uade0\\ud615\\uc810\\uc744 \\ucc3e\\uc544\\uac08 \\uac83\\\"\\uc774\\ub77c\\uba70 \\\"\\uc2dc\\uc810\\uacfc \\uacbd\\ub85c\\ub97c \\uc798 \\uc124\\uc815\\ud574 \\uc2e4\\ud328\\ud558\\uc9c0 \\uc54a\\ub3c4\\ub85d \\ub178\\ub825\\ud558\\uaca0\\ub2e4\\\"\\uace0 \\uac15\\uc870\\ud568. \\u25b6\\ubc14\\ub85c\\uac00\\uae304. \\ubbfc\\uc8fc\\ub2f9\\uacfc \\uc815\\ubd80\\ub294 20\\uc77c \\ucd5c\\uc800\\uc784\\uae08 \\uc778\\uc0c1\\uc5d0 \\ub530\\ub978 \\uc18c\\uc0c1\\uacf5\\uc778\\u00b7\\uc790\\uc601\\uc5c5\\uc790 \\uacbd\\uc601\\ub09c \\uc644\\ud654 \\ubc29\\uc548\\uc744 \\uc758\\ub17c\\ud568. \\ud604\\uc7ac 9%\\uc778 \\uc0c1\\uac00 \\uc784\\ub300\\ub8cc \\uc778\\uc0c1\\ub960 \\uc0c1\\ud55c\\uc744 \\ub0ae\\ucd94\\uace0 \\uc0c1\\uac00 \\uc784\\ucc28\\uc778\\uc758 \\uacc4\\uc57d\\uac31\\uc2e0\\uccad\\uad6c\\uad8c \\ud589\\uc0ac\\uae30\\uac04\\uc744 10\\ub144\\uc73c\\ub85c \\uc5f0\\uc7a5\\ud558\\ub294 \\ubc29\\uc548 \\ub4f1\\uc774 \\uac70\\ub860\\ub428. \\u25b6\\ubc14\\ub85c\\uac00\\uae305. \\ubb38\\uc7ac\\uc778 \\ub300\\ud1b5\\ub839\\uc774 \\uc6d4\\uac00 \\uae08\\uc735\\uc804\\ubb38\\uac00 200\\uc5ec\\uba85\\uc744 \\ucd08\\uccad\\ud574 \\uc124\\uba85\\ud68c\\ub97c \\uac1c\\ucd5c\\ud568. \\ubb38 \\ub300\\ud1b5\\ub839\\uc740 \\\"\\ubd81\\ud575\\ub9ac\\uc2a4\\ud06c\\uc5d0\\ub3c4 \\ud55c\\uad6d \\uacbd\\uc81c\\uc640 \\uae08\\uc735\\uc2dc\\uc7a5\\uc740 \\uc548\\uc815\\uc801\\\"\\uc774\\ub77c\\uba70 \\ubcc0\\ud568\\uc5c6\\ub294 \\uc2e0\\ub8b0\\uc640 \\ud22c\\uc790\\ub97c \\uc694\\uccad\\ud568. \\ubb38 \\ub300\\ud1b5\\ub839\\uc740 21\\uc77c \\ud55c\\ubbf8\\uc77c \\uc815\\uc0c1\\ud68c\\ub2f4\\uc744 \\ub05d\\uc73c\\ub85c \\ub274\\uc695 \\uc21c\\ubc29 \\uc77c\\uc815\\uc744 \\ub9c8\\uce58\\uac8c \\ub428. \\u25b6\\ubc14\\ub85c\\uac00\\uae306. \\uae40\\uba85\\uc218 \\ub300\\ubc95\\uc6d0\\uc7a5 \\ud6c4\\ubcf4\\uc758 \\uc784\\uba85\\ub3d9\\uc758\\uc548 \\ud45c\\uacb0\\uc744 \\ud558\\ub8e8 \\uc55e\\ub454 20\\uc77c \\ubbfc\\uc8fc\\ub2f9 \\uc758\\uc6d0 121\\uba85 \\uc804\\uc6d0\\uc774 \\ud22c\\uc785\\ub3fc \\uc57c\\ub2f9\\uc744 \\uc124\\ub4dd\\ud568. \\uad6d\\ubbfc\\uc758\\ub2f9\\uc5d0\\uc11c 30\\uba85\\uc758 \\ucc2c\\uc131\\ud45c\\ub97c \\uc5bb\\uc5b4\\uc57c \\uc548\\uc815\\uc801 \\uc758\\uacb0\\uc815\\uc871\\uc218\\ub97c \\ud655\\ubcf4\\ud560 \\uc804\\ub9dd\\uc784. \\uc790\\uc720\\ud55c\\uad6d\\ub2f9\\uc740 \\uae40 \\ud6c4\\ubcf4\\uc790 \\uc784\\uba85\\ub3d9\\uc758\\uc548 \\ubd80\\uacb0\\uc744 \\ub2f9\\ub860\\uc73c\\ub85c \\ucc44\\ud0dd\\ud568. \\u25b6\\ubc14\\ub85c\\uac00\\uae30\\uae40\\uba85\\uc218 \\ub300\\ubc95\\uc6d0\\uc7a5 \\ud6c4\\ubcf4\\uc790\\uac00 \\uc784\\uba85\\ub3d9\\uc758\\uc548 \\uad6d\\ud68c \\ubcf8\\ud68c\\uc758 \\ud45c\\uacb0\\uc744 \\ud558\\ub8e8 \\uc55e\\ub454 20\\uc77c \\uc624\\uc804 \\uc11c\\uc6b8 \\uc11c\\ucd08\\uad6c \\uc0ac\\ubc95\\ubc1c\\uc804\\uc7ac\\ub2e8\\uc5d0 \\ub9c8\\ub828\\ub41c \\uc0ac\\ubb34\\uc2e4\\ub85c \\ub4e4\\uc5b4\\uac00\\uace0 \\uc788\\ub2e4. /\\uc0ac\\uc9c4=\\uc5f0\\ud569\\ub274\\uc2a47. \\uc0bc\\uc131\\uc804\\uc790 \\uc218\\ub1cc\\ubd80\\uac00 \\ucd9c\\ub3d9\\ud574 \\uc2e4\\ub9ac\\ucf58\\ubc38\\ub9ac\\uc5d0\\uc11c '\\ud14c\\ud06c\\ud3ec\\ub7fc 2017'\\uc744 \\uc5f4\\uace0 \\ud604\\uc9c0 \\uc778\\uc7ac\\ub4e4\\uacfc IT\\uc758 \\ubbf8\\ub798\\uc5d0 \\ub300\\ud574 \\ud1a0\\ub860\\ud568. \\uad6c\\uae00\\u00b7\\uc560\\ud50c \\ub4f1 \\ud604\\uc9c0 IT\\uae30\\uc5c5 \\uc5d4\\uc9c0\\ub2c8\\uc5b4\\uc640 \\ub514\\uc790\\uc774\\ub108 \\ub4f1 100\\uc5ec\\uba85\\uc774 \\ucc38\\uc11d\\ud574 \\uc778\\uacf5\\uc9c0\\ub2a5\\u00b7\\uc0ac\\ubb3c\\uc778\\ud130\\ub137\\u00b7\\uc804\\uc7a5\\uae30\\uc220 \\ub4f1\\uc5d0 \\ub300\\ud574 \\uc0bc\\uc131 \\uc0ac\\uc7a5\\ub2e8\\uacfc \\uc9c8\\uc758\\uc751\\ub2f5 \\uc2dc\\uac04\\uc744 \\uac00\\uc9d0. \\u25b6\\ubc14\\ub85c\\uac00\\uae308. \\uc9c0\\ub09c 7\\uc77c 100\\ub144\\ub9cc\\uc758 \\ucd5c\\ub300\\uaddc\\ubaa8 \\uc9c0\\uc9c4\\uc774 \\ubc1c\\uc0dd\\ud55c \\uba55\\uc2dc\\ucf54\\uc5d0\\uc11c \\ub2e4\\uc2dc 7.1\\uaddc\\ubaa8 \\uac15\\uc9c4\\uc774 \\ubc1c\\uc0dd\\ud574 200\\uba85 \\uc774\\uc0c1\\uc774 \\uc0ac\\ub9dd\\ud568. \\uc0ac\\uc0c1\\uc790 \\uc218\\uac00 \\uc810\\uc810 \\ub298\\uc5b4\\ub098\\uba70 \\ud53c\\ud574\\uaddc\\ubaa8\\uac00 \\ucee4\\uc9c0\\uace0 \\uc788\\uace0 \\uc5ec\\uc9c4\\uc5d0 \\ub300\\ud55c \\ubd88\\uc548\\uac10\\uc73c\\ub85c \\uba55\\uc2dc\\ucf54\\ub294 \\ud328\\ub2c9\\uc5d0 \\ube60\\uc9d0. \\ud55c\\uad6d\\uc778 1\\uba85\\uc774 \\uc0ac\\ub9dd\\ud55c \\uac83\\uc73c\\ub85c \\ud655\\uc778\\ub428. \\u25b6\\ubc14\\ub85c\\uac00\\uae30[\\ub9e4\\uc77c\\uacbd\\uc81c \\uacf5\\uc2dd \\ud398\\uc774\\uc2a4\\ubd81] [\\uc624\\ub298\\uc758 \\uc778\\uae30\\ub274\\uc2a4] [\\ub9e4\\uacbd \\ud504\\ub9ac\\ubbf8\\uc5c4][\\u24d2 \\ub9e4\\uc77c\\uacbd\\uc81c & mk.co.kr, \\ubb34\\ub2e8\\uc804\\uc7ac \\ubc0f \\uc7ac\\ubc30\\ud3ec \\uae08\\uc9c0]\",\n          \"6\\uac1c \\ubd84\\uc57c 12\\uac1c \\uacfc\\uc81c \\uc120\\uc815\\ud574\\uc62c\\ud574 152\\uc5b5\\uc6d0 \\uc5f0\\uad6c\\ube44 \\uc9c0\\uc6d0\\uc0bc\\uc131\\uc804\\uc790\\uac00 \\uad6d\\uac00\\uc801\\uc73c\\ub85c \\uc5f0\\uad6c\\uac00 \\ud544\\uc694\\ud55c \\ubbf8\\ub798 \\uacfc\\ud559\\uae30\\uc220 \\ubd84\\uc57c\\ub97c \\ubc1c\\uc804\\uc2dc\\ud0a4\\uae30 \\uc704\\ud574 \\uc5b4\\ub4dc\\ubc34\\uc2a4\\ud2b8 \\uc778\\uacf5\\uc9c0\\ub2a5(AI), \\ucc28\\uc138\\ub300 \\uc554\\ud638 \\uc2dc\\uc2a4\\ud15c \\ub4f1 6\\uac1c \\ubd84\\uc57c\\uc5d0 \\ub300\\ud55c \\uc9c0\\uc6d0\\uc5d0 \\ub098\\uc120\\ub2e4. 15\\uc77c \\uc0bc\\uc131\\uc804\\uc790\\ub294 2021\\ub144 '\\uc0bc\\uc131 \\ubbf8\\ub798\\uae30\\uc220 \\uc721\\uc131\\uc0ac\\uc5c5' \\uc9c0\\uc815 \\ud14c\\ub9c8 \\uc5f0\\uad6c \\uc9c0\\uc6d0 \\uacfc\\uc81c 12\\uac1c\\uc5d0 \\uc5f0\\uad6c\\ube44 \\ucd1d 152\\uc5b51000\\ub9cc\\uc6d0\\uc744 \\uc9c0\\uc6d0\\ud55c\\ub2e4\\uace0 \\ubc1d\\ud614\\ub2e4. \\uc0bc\\uc131 \\ubbf8\\ub798\\uae30\\uc220 \\uc721\\uc131\\uc0ac\\uc5c5\\uc740 \\uc6b0\\ub9ac\\ub098\\ub77c \\ubbf8\\ub798\\ub97c \\ucc45\\uc784\\uc9c8 \\uacfc\\ud559\\uae30\\uc220 \\uc721\\uc131\\u00b7\\uc9c0\\uc6d0\\uc744 \\ubaa9\\ud45c\\ub85c \\uc0bc\\uc131\\uc804\\uc790\\uac00 2013\\ub144\\ubd80\\ud130 1\\uc8705000\\uc5b5\\uc6d0\\uc744 \\ucd9c\\uc5f0\\ud574 \\uc2dc\\ud589\\ud558\\uace0 \\uc788\\ub294 \\uc5f0\\uad6c \\uc9c0\\uc6d0 \\uacf5\\uc775\\uc0ac\\uc5c5\\uc774\\ub2e4.\\ub9e4\\ub144 \\uc0c1\\u00b7\\ud558\\ubc18\\uae30\\uc5d0 \\uae30\\ucd08\\uacfc\\ud559, \\uc18c\\uc7ac, \\uc815\\ubcf4\\ud1b5\\uc2e0\\uae30\\uc220 \\ubd84\\uc57c\\uc5d0\\uc11c \\uc9c0\\uc6d0\\ud560 \\uacfc\\uc81c\\ub97c \\uc120\\uc815\\ud55c\\ub2e4. \\ucd94\\uac00\\ub85c 2014\\ub144\\ubd80\\ud130\\ub294 \\ub9e4\\ub144 \\ud55c \\ucc28\\ub840 '\\uc9c0\\uc815 \\ud14c\\ub9c8 \\uacfc\\uc81c \\uacf5\\ubaa8'\\ub97c \\ud1b5\\ud574 \\uad6d\\uac00\\uc801\\uc73c\\ub85c \\ud544\\uc694\\ud55c \\ubbf8\\ub798 \\uae30\\uc220 \\ubd84\\uc57c\\ub97c \\uc9c0\\uc815\\ud574 \\uc5f0\\uad6c\\ub97c \\uc9c0\\uc6d0\\ud558\\uace0 \\uc788\\ub2e4.\\uc62c\\ud574 \\uc9c0\\uc815\\ub41c \\ud14c\\ub9c8 \\uc5f0\\uad6c \\uc9c0\\uc6d0 \\uacfc\\uc81c\\ub294 '\\uc5b4\\ub4dc\\ubc34\\uc2a4\\ud2b8 AI' '\\ucc28\\uc138\\ub300 \\uc554\\ud638 \\uc2dc\\uc2a4\\ud15c' 'B(Beyond)5G&6G' '\\ub85c\\ubd07' '\\ucc28\\uc138\\ub300 \\ub514\\uc2a4\\ud50c\\ub808\\uc774' '\\ubc18\\ub3c4\\uccb4 \\uc18c\\uc790 \\ubc0f \\uacf5\\uc815' \\ub4f1 \\ucd1d 6\\uac1c \\ubd84\\uc57c\\uc5d0\\uc11c 12\\uac1c\\ub97c \\uc120\\uc815\\ud588\\ub2e4. \\uc5b4\\ub4dc\\ubc34\\uc2a4\\ud2b8 AI\\uc640 \\ucc28\\uc138\\ub300 \\uc554\\ud638 \\uc2dc\\uc2a4\\ud15c\\uc740 \\uc774\\ubc88\\uc5d0 \\uc0c8\\ub86d\\uac8c \\uc120\\uc815\\ub41c \\ubd84\\uc57c\\ub2e4. \\uc5b4\\ub4dc\\ubc34\\uc2a4\\ud2b8 AI\\ub294 \\uace0\\ucc28\\uc6d0\\uc758 \\ub17c\\ub9ac \\ucd94\\ub860\\uae4c\\uc9c0 \\uac00\\ub2a5\\ud558\\uac8c \\ud558\\ub294 AI\\ub85c \\uc774 \\ubd84\\uc57c\\uc5d0\\uc11c\\ub294 \\ud669\\ub3c4\\uc2dd \\uc5f0\\uc138\\ub300 \\uc804\\uae30\\uc804\\uc790\\uacf5\\ud559\\ubd80 \\uad50\\uc218\\uc758 '\\uc21c\\ud658 \\ucd94\\ub860\\ud615 \\uc778\\uacf5\\uc9c0\\ub2a5-\\uc790\\uae30 \\uc9c8\\uc758\\uc751\\ub2f5 \\uae30\\ubc18 \\uc790\\ub3d9 \\uc758\\ub8cc\\uc9c4\\ub2e8 \\uae30\\uc220' \\ub4f1 \\ucd1d 2\\uac1c \\uacfc\\uc81c\\uac00 \\uc120\\uc815\\ub410\\ub2e4. \\ucc28\\uc138\\ub300 \\uc554\\ud638 \\uc2dc\\uc2a4\\ud15c \\ubd84\\uc57c\\uc5d0\\uc11c\\ub294 \\uc1a1\\uc6a9\\uc218 \\uc11c\\uc6b8\\ub300 \\ucef4\\ud4e8\\ud130\\uacf5\\ud559\\ubd80 \\uad50\\uc218\\uc758 '\\ub2e4\\uc790\\uac04 \\uadfc\\uc0ac\\uacc4\\uc0b0 \\uc554\\ud638 \\uc6d0\\ucc9c\\uae30\\uc220 \\uac1c\\ubc1c' \\uacfc\\uc81c\\uac00 \\uc120\\uc815\\ub410\\ub2e4. \\ud074\\ub77c\\uc6b0\\ub4dc\\uc5d0 \\ubcf4\\uad00\\ub41c \\uc790\\ub8cc\\uc758 \\ube44\\ubc00\\uc131\\uc740 \\uc720\\uc9c0\\ud558\\uba74\\uc11c \\ub370\\uc774\\ud130\\ub97c \\ubd84\\uc11d\\ud560 \\uc218 \\uc788\\ub294 \\uae30\\uc220\\uc774\\ub2e4.[\\ubc15\\uc7ac\\uc601 \\uae30\\uc790]\",\n          \"\\uc62c\\ud574 1\\ubd84\\uae30 \\uc2e4\\uc801\\uc744 \\ubc1c\\ud45c\\ud55c \\uc0c1\\uc7a5\\uc0ac \\uc911 \\uc2dc\\uc7a5\\uc804\\ub9dd\\uce58(\\ucee8\\uc13c\\uc11c\\uc2a4)\\ub97c \\ud6e8\\uc52c \\ub6f0\\uc5b4\\ub118\\ub294 \\u2018\\uc5b4\\ub2dd \\uc11c\\ud504\\ub77c\\uc774\\uc988\\u2019\\ub97c \\uae30\\ub85d\\ud55c \\uacf3\\uc774 40%\\uc5d0 \\ub2ec\\ud558\\ub294 \\uac83\\uc73c\\ub85c \\uc870\\uc0ac\\ub410\\ub2e4. \\ubd80\\uc815\\ud655\\ud55c \\uc2e4\\uc801 \\ucd94\\uc815\\uc73c\\ub85c \\uc5b4\\ub2dd \\uc1fc\\ud06c\\u00b7\\uc11c\\ud504\\ub77c\\uc774\\uc988\\uac00 \\ubc1c\\uc0dd\\ud588\\ub358 \\uc774\\uc804 \\uc2e4\\uc801\\uc2dc\\uc98c\\uacfc \\ube44\\uad50\\ud574 \\ub69c\\ub837\\ud55c \\uc774\\uc775\\uc2f8\\uc774\\ud074\\uc758 \\uac1c\\uc120\\uc774 \\ub098\\ud0c0\\ub098\\uace0 \\uc788\\uc5b4 \\uae0d\\uc815\\uc801\\uc774\\ub77c\\ub294 \\ubd84\\uc11d\\uc774\\ub2e4.29\\uc77c \\ub9e4\\uc77c\\uacbd\\uc81c\\uac00 \\uc5d0\\ud504\\uc564\\uac00\\uc774\\ub4dc\\uc5d0 \\uc758\\ub8b0\\ud574 \\uc774\\ub0a0\\uae4c\\uc9c0 \\uc2e4\\uc801\\uc744 \\ubc1c\\ud45c\\ud55c \\uc0c1\\uc7a5\\uc0ac\\ub4e4 \\uc911 \\uc99d\\uad8c\\uc0ac\\ub4e4\\uc758 \\ub9e4\\ucd9c\\uc561\\uacfc \\uc601\\uc5c5\\uc774\\uc775 \\ucd94\\uc815\\uce58\\uac00 3\\uac1c \\uc774\\uc0c1 \\uc874\\uc7ac\\ud558\\ub294 70\\uacf3\\uc758 \\uc2e4\\uc801\\uc744 \\ubd84\\uc11d\\ud55c \\uacb0\\uacfc\\ub2e4. \\ucd94\\uc815\\uce58\\uc640 \\uc2e4\\uc801\\ubc1c\\ud45c\\uce58\\ub97c \\ubd84\\uc11d\\ud55c \\uacb0\\uacfc \\uc5b4\\ub2dd \\uc11c\\ud504\\ub77c\\uc774\\uc988\\ub294 28\\uacf3, \\uc5b4\\ub2dd \\uc1fc\\ud06c\\ub97c \\ub0b8 \\uae30\\uc5c5\\uc740 14\\uacf3\\uc73c\\ub85c \\uac01\\uac01 \\uc9d1\\uacc4\\ub410\\ub2e4. \\uc5b4\\ub2dd \\uc11c\\ud504\\ub77c\\uc774\\uc988 \\uae30\\uc5c5 \\ube44\\uc911\\uc740 40.0%\\ub97c \\ucc28\\uc9c0\\ud55c \\ubc18\\uba74 \\uc5b4\\ub2dd \\uc1fc\\ud06c \\uae30\\uc5c5 \\ube44\\uc911\\uc740 20%\\uc5d0 \\ubd88\\uacfc\\ud574 \\ub450\\ubc30 \\ucc28\\uc774\\uac00 \\ub0ac\\ub2e4.\\ub300\\uac1c \\uc99d\\uad8c\\uc0ac\\ub4e4\\uc774 \\ub0b4\\ub193\\ub294 \\uc2e4\\uc801 \\uc804\\ub9dd\\uce58 \\ud3c9\\uade0(\\ucee8\\uc13c\\uc11c\\uc2a4)\\ubcf4\\ub2e4 \\uae30\\uc5c5\\uc758 \\uc2e4\\uc81c \\uc601\\uc5c5\\uc774\\uc775\\uc774 10% \\uc774\\uc0c1 \\ub9ce\\uc73c\\uba74 \\uc5b4\\ub2dd \\uc11c\\ud504\\ub77c\\uc774\\uc988\\ub85c, 10% \\uc774\\uc0c1 \\uc801\\uc73c\\uba74 \\uc5b4\\ub2dd \\uc1fc\\ud06c\\ub85c \\uac01\\uac01 \\ubd84\\ub958\\ud55c\\ub2e4. \\ud751\\uc790\\u00b7\\uc801\\uc790 \\uc804\\ud658\\ud558\\uac70\\ub098 \\uc801\\uc790\\ud3ed\\uc774 \\ud655\\ub300\\ub41c \\uacbd\\uc6b0\\ub3c4 \\uc5b4\\ub2dd\\uc1fc\\ud06c\\ub85c \\ubd84\\ub958\\ub418\\ub294 \\uac8c \\uc77c\\ubc18\\uc801\\uc774\\ub2e4. \\uc544\\uc9c1 \\uc2e4\\uc801\\uc2dc\\uc98c \\ucd08\\ubc18\\uc774\\uc9c0\\ub9cc \\uc0bc\\uc131\\uc804\\uc790, LG\\uc804\\uc790 \\ub4f1 \\uc5c5\\uc885 \\ub300\\ud45c\\uc8fc\\ub4e4\\uc758 \\uc2e4\\uc801\\uc774 \\ub300\\ubd80\\ubd84 \\uacf5\\uac1c\\ub3fc \\uc0c1\\uc7a5\\uc0ac\\ub4e4\\uc758 1\\ubd84\\uae30 \\uc2e4\\uc801\\uc744 \\uc5b4\\ub290\\uc815\\ub3c4 \\uac00\\ub984\\ud560 \\uc218 \\uc788\\ub2e4.1\\ubd84\\uae30 \\uac00\\uc7a5 \\uc5b4\\ub2dd \\uc11c\\ud504\\ub77c\\uc774\\uc988\\ub97c \\uae30\\ub85d\\ud55c \\uc885\\ubaa9 \\uc911 \\ub208\\uc5d0 \\ub744\\ub294 \\uc885\\ubaa9\\uc740 LG\\uc0dd\\uba85\\uacfc\\ud559\\uc774\\ub2e4. LG\\uc0dd\\uba85\\uacfc\\ud559\\uc758 1\\ubd84\\uae30 \\uc601\\uc5c5\\uc774\\uc775\\uc740 171\\uc5b5\\uc6d0\\uc73c\\ub85c \\ucee8\\uc13c\\uc11c\\uc2a4 37\\uc5b5\\uc6d0\\ubcf4\\ub2e4 354.5% \\ub192\\uc558\\ub2e4. \\ubc30\\uae30\\ub2ec \\uc2e0\\ud55c\\uae08\\uc735\\ud22c\\uc790 \\uc5f0\\uad6c\\uc6d0\\uc740 \\u201c1\\ubd84\\uae30 \\uc5b4\\ub2dd \\uc11c\\ud504\\ub77c\\uc774\\uc988\\ub294 \\uae30\\uc220 \\uc218\\ucd9c\\ub8cc(150\\uc5b5\\uc6d0) \\uc720\\uc785\\uacfc \\uc8fc\\ub825 \\ud488\\ubaa9 \\ub9e4\\ucd9c \\ud638\\uc870 \\ub355\\ubd84\\u201d\\uc774\\ub77c\\uba70 \\u201c\\ub2f9\\ub1e8\\ubcd1 \\uce58\\ub8cc\\uc81c \\u2018\\uc81c\\ubbf8\\uae00\\ub85c\\u2019 \\ud310\\ub9e4\\ub7c9\\uc774 \\uc804\\ub144\\ub3d9\\uae30\\ub300\\ube44 121.3% \\uc99d\\uac00\\ud558\\ub294 \\ub4f1 \\ub192\\uc740 \\uc131\\uc7a5\\uc744 \\uc9c0\\uc18d\\ud558\\uace0 \\uc788\\ub2e4\\u201d\\uace0 \\ubd84\\uc11d\\ud588\\ub2e4. \\uc2e0\\ud55c\\uae08\\uc735\\ud22c\\uc790\\ub294 LG\\uc0dd\\uba85\\uacfc\\ud559\\uc758 \\ubaa9\\ud45c\\uc8fc\\uac00\\ub97c \\uae30\\uc874 8\\ub9cc\\uc6d0\\uc5d0\\uc11c 8\\ub9cc5000\\uc6d0\\uc73c\\ub85c \\uc0c1\\ud5a5\\ud588\\ub2e4.\\ud48d\\uc0b0\\ub3c4 393\\uc5b5\\uc6d0\\uc758 \\uc601\\uc5c5\\uc774\\uc775\\uc744 \\ub0b4\\uba70 \\ucee8\\uc13c\\uc11c\\uc2a4(182\\uc5b5\\uc6d0)\\ub97c 116.1% \\uc0c1\\ud68c\\ud588\\ub2e4. \\uc804\\ub144\\ub3d9\\uae30 \\uc601\\uc5c5\\uc774\\uc775\\uacfc \\ube44\\uad50\\ud558\\uba74 588% \\uc99d\\uac00\\ud55c \\uae08\\uc561\\uc774\\ub2e4. HMC\\ud22c\\uc790\\uc99d\\uad8c\\uc740 \\ud48d\\uc0b0\\uc758 2\\ubd84\\uae30 \\uc601\\uc5c5\\uc774\\uc775\\uc744 482\\uc5b5\\uc6d0\\uc73c\\ub85c \\ucd94\\uc815\\ud558\\uba70 1\\ubd84\\uae30 \\uc0c1\\uc2b9\\uc138\\ub97c \\uc774\\uc5b4\\ub098\\uac08 \\uac83\\uc73c\\ub85c \\uc608\\uce21\\ud588\\ub2e4. 2\\ubd84\\uae30 \\uc131\\uc218\\uae30\\uc5d0 \\uc811\\uc5b4\\ub4e4\\uc5b4 \\ud310\\ub9e4\\ub7c9\\uc774 \\uc99d\\uac00\\ud558\\uace0 \\uad6c\\ub9ac\\uac00\\uaca9 \\uc0c1\\uc2b9\\uc73c\\ub85c \\uc778\\ud55c \\uc774\\uc775\\uc774 \\uc608\\uc0c1\\ub41c\\ub2e4\\ub294 \\uac83\\uc774\\ub2e4.\\uc870\\uc120\\uc5c5\\uacc4 \\uad6c\\uc870\\uc870\\uc815 \\ubd88\\uc548\\uac10\\uc774 \\uac00\\uc911\\ub410\\ub358 \\ud604\\ub300\\uc911\\uacf5\\uc5c5\\uc740 \\uc608\\uc0c1\\uc678\\ub85c \\ucee8\\uc13c\\uc11c\\uc2a4(1427\\uc5b5\\uc6d0)\\uc744 127.9% \\uc6c3\\ub3c4\\ub294 3252\\uc5b5\\uc6d0\\uc758 \\uc601\\uc5c5\\uc774\\uc775\\uc744 \\ub0c8\\ub2e4. \\ud604\\ub300\\ubbf8\\ud3ec\\uc870\\uc120\\ub3c4 576\\uc5b5\\uc6d0\\uc758 \\uc601\\uc5c5\\uc774\\uc775\\uc744 \\uae30\\ub85d\\ud574 \\uc99d\\uad8c\\uac00 \\uc804\\ub9dd\\uce58 214\\uc5b5\\uc6d0\\uc744 \\ub450 \\ubc30\\uc774\\uc0c1 \\uc55e\\uc9c8\\ub800\\ub2e4. \\ud604\\ub300\\ub85c\\ud15c\\uc740 \\ucee8\\uc13c\\uc11c\\uc2a4\\ub97c 77.5% \\uc6c3\\ub3cc\\uc558\\uace0 KT&G\\ub294 39.6%, SK\\uc774\\ub178\\ubca0\\uc774\\uc158\\uc740 25.9%, \\uae30\\uc544\\ucc28\\ub294 19.2%, \\ub300\\ub9bc\\uc0b0\\uc5c5\\uc740 16.9% \\uc6c3\\ub3cc\\uc558\\ub2e4.\\ubc18\\uba74 \\uc2e4\\uc801\\uac1c\\uc120 \\ud750\\ub984\\uc18d\\uc5d0\\uc11c\\ub3c4 \\uc5b4\\ub2dd\\uc1fc\\ud06c\\ub97c \\ub0b8 \\uae30\\uc5c5\\ub4e4\\uc774 \\ub354\\ub7ec \\uc788\\ub2e4. 727\\uc5b5\\uc6d0\\uc758 \\uc601\\uc5c5\\uc774\\uc775\\uc774 \\uc608\\uc0c1\\ub410\\ub358 \\uc0bc\\uc131\\ubb3c\\uc0b0\\uc740 \\uc624\\ud788\\ub824 4348\\uc5b5\\uc6d0 \\uc601\\uc5c5\\uc190\\uc2e4\\uc744 \\uae30\\ub85d\\ud588\\ub2e4. \\uc724\\ud0dc\\ud638 \\ud55c\\uad6d\\ud22c\\uc790\\uc99d\\uad8c \\uc5f0\\uad6c\\uc6d0\\uc740 \\u201c\\uc190\\uc2e4\\uc744 \\ubc18\\uc601\\ud55c \\ud574\\uc678\\ud504\\ub85c\\uc81d\\ud2b8 \\uc0c1\\ub2f9\\uc218\\ub294 \\uc644\\uacf5 \\uc608\\uc815\\uc77c\\uc774 2017~2018\\ub144\\uc5d0 \\uac78\\uccd0 \\uc788\\uc5b4 \\ud5a5\\ud6c4 \\ub098\\ub220 \\ubc18\\uc601\\ud560 \\uc218 \\uc788\\uc5c8\\ub294 \\ub370 \\uc774\\ubc88 1\\ubd84\\uae30 \\uc2e4\\uc801\\uc740 \\uc758\\uc678\\uc758 \\uacb0\\uacfc\\u201d\\ub77c\\uba70 \\u201c\\ud5a5\\ud6c4 \\uac01 \\uc0ac\\uc5c5\\ubd80 \\uac1c\\ud3b8\\uacfc \\uc9c0\\uc8fc \\uc804\\ud658 \\uc774\\uc804 \\uac01 \\uc0ac\\uc5c5\\ubd80\\ubcc4 \\ubd80\\ub2f4\\uc774 \\ub418\\ub294 \\uc7a0\\uc7ac\\uc190\\uc2e4\\uc744 \\uc801\\uadf9\\uc801\\uc73c\\ub85c \\uc120\\ubc18\\uc601\\ud55c \\uac83\\uc73c\\ub85c \\ud310\\ub2e8\\ub41c\\ub2e4\\u201d\\uace0 \\ub9d0\\ud588\\ub2e4.\\ud604\\ub300\\uc704\\uc544\\ub294 800\\uc5b5\\uc6d0\\uc758 \\uc601\\uc5c5\\uc774\\uc775\\uc744 \\uae30\\ub85d\\ud574 \\ucee8\\uc13c\\uc11c\\uc2a4(1149\\uc5b5\\uc6d0)\\ub97c 30.4% \\ud558\\ud68c\\ud588\\uace0, \\uc0bc\\uc131\\uc804\\uae30\\ub294 34.4%, \\ud604\\ub300\\uc81c\\ucca0\\uc740 18.1% \\uc529 \\ucee8\\uc13c\\uc11c\\uc2a4\\ub97c \\uac01\\uac01 \\ubc11\\ub3cc\\uc558\\ub2e4. \\ud55c\\ubbf8\\uc57d\\ud488\\uc740 \\uc804\\ub9dd\\uce58(854\\uc5b5\\uc6d0)\\ub97c 73.6% \\ud558\\ud68c\\ud588\\ub294\\ub370, \\ud68c\\uacc4\\uc778\\uc2dd \\uc0c1\\uc758 \\ubb38\\uc81c\\ub97c \\uc81c\\uc678\\ud558\\uba74 1\\ubd84\\uae30 \\uc601\\uc5c5\\uc774\\uc775 226\\uc5b5\\uc6d0\\uc740 \\ub300\\uccb4\\ub85c \\uc804\\ub9dd\\uacfc \\uc77c\\uce58\\ud55c\\ub2e4\\ub294 \\ud3c9\\uac00\\uac00 \\uc9c0\\ubc30\\uc801\\uc774\\ub2e4.\\uc791\\ub144 11\\uc6d4 \\uc0ac\\ub178\\ud53c\\uc5d0 \\uae30\\uc220 \\uc218\\ucd9c\\ub85c \\uc720\\uc785\\ub41c \\uacc4\\uc57d\\uae08 5200\\uc5b5\\uc6d0 \\uc911 \\uc791\\ub144 4\\ubd84\\uae30\\uc5d0 \\uc778\\uc2dd(2556\\uc5b5)\\ub418\\uace0 \\ub0a8\\uc740 2644\\uc5b5\\uc6d0\\uc744 \\uc62c\\ud574 \\uc804\\ubd80 \\uc778\\uc2dd\\ud558\\uc9c0 \\uc54a\\uace0 3\\ub144 \\ub3d9\\uc548 \\ubd84\\ud560 \\uc778\\uc2dd\\ud588\\uae30 \\ub54c\\ubb38\\uc774\\ub2e4. \\ud55c\\ud3b8 1\\ubd84\\uae30 \\uc2e4\\uc801\\ubc1c\\ud45c\\ub294 \\ub2e4\\ub978 \\ubd84\\uae30\\uc5d0 \\ube44\\ud574 \\uc815\\ud655\\ub3c4\\uac00 \\ub192\\uc740 \\ud3b8\\uc774\\uc5b4\\uc11c \\uc62c 1\\ubd84\\uae30 \\uc5b4\\ub2dd\\uc2dc\\uc98c\\uc774 \\ud2b9\\ud788 \\uae0d\\uc815\\uc801\\uc774\\ub77c\\ub294 \\ubd84\\uc11d\\ub3c4 \\ub098\\uc654\\ub2e4. \\uc720\\uc548\\ud0c0\\uc99d\\uad8c\\uc5d0 \\ub530\\ub974\\uba74 2011~2015\\ub144 \\ub3d9\\uc548 1\\ubd84\\uae30 \\uc804\\ub9dd\\uce58 \\ub2ec\\uc131\\ub960\\uc740 94.7%\\ub85c 4\\ubd84\\uae30(77.0%) \\ub4f1 \\ub2e4\\ub978 \\ubd84\\uae30\\ubcf4\\ub2e4 \\uc801\\uc911\\ub960\\uc774 \\ub192\\uc740 \\uac83\\uc73c\\ub85c \\ub098\\ud0c0\\ub0ac\\ub2e4.[\\uae40\\ud0dc\\uc900 \\uae30\\uc790][\\u24d2 \\ub9e4\\uc77c\\uacbd\\uc81c & mk.co.kr, \\ubb34\\ub2e8\\uc804\\uc7ac \\ubc0f \\uc7ac\\ubc30\\ud3ec \\uae08\\uc9c0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 2015,\n        \"max\": 2024,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          2023,\n          2016,\n          2020\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          11,\n          10,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1,\n        \"max\": 31,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          28,\n          16,\n          24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hour\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 23,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          0,\n          6,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"minute\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18,\n        \"min\": 0,\n        \"max\": 59,\n        \"num_unique_values\": 60,\n        \"samples\": [\n          3,\n          46,\n          44\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42423,\n        \"samples\": [\n          \"\\ub9c8\\ub8e8\\uce74\\ud3ab \\ucd5c\\uc801\\uc758 \\uccad\\uc18c \\ubaa8\\ub4dc \\uc2a4\\uc2a4\\ub85c \\ucc3e\\uc544.\",\n          \"\\ud30c\\uc778\\ud14d 4~5\\uc77c \\uacf5\\ubaa8.\",\n          \"\\ub77c\\uc628\\ud14c\\ud06c, \\uae30\\uc220\\uc131 \\ud3c9\\uac00 \\ub3cc\\uc785 \\ucf54\\uc2a4\\ub2e5 \\uc804 \\ubcf8\\uaca9\\ud654.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_body\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42777,\n        \"samples\": [\n          \"\\uc678\\uad6d\\uc778 \\uc21c\\ub9e4\\uc218 \\ubab0\\ub9ac\\uba70 6\\ub144 \\ub9cc\\uc5d0 \\uc0ac\\uc0c1 \\ucd5c\\uace0\\uce58 \\uacbd\\uc2e0 \\ucf54\\uc2a4\\ud53c\\uac00 2240\\uc120\\uc744 \\ubc1f\\uc73c\\uba70 \\uc0ac\\uc0c1 \\ucd5c\\uace0\\uce58 \\uae30\\ub85d\\uc744 \\ub2e4\\uc2dc \\uc37c\\ub2e4. 4\\uc77c \\uc720\\uac00 \\uc99d\\uad8c\\uc2dc\\uc7a5\\uc5d0\\uc11c \\ucf54\\uc2a4\\ud53c\\ub294 \\uc804\\ub0a0\\ubcf4\\ub2e4 21. 57\\ud3ec\\uc778\\ud2b80. 97% \\uc624\\ub978 2241. 24\\ub97c \\uae30\\ub85d\\ud558\\uba70 \\uc7a5\\uc744 \\ub9c8\\ucce4\\ub2e4. \\uc885\\uac00 \\uae30\\uc900 \\uc774\\uc804 \\ucd5c\\uace0\\uce58\\uc600\\ub358 2228. 962011\\ub144 5\\uc6d4 2\\uc77c\\uc744 \\ub3cc\\ud30c\\ud55c \\uac83\\uc740 \\ubb3c\\ub860\\uc774\\uace0 \\uae30\\uc874 \\uc7a5 \\uc911 \\ucd5c\\uace0\\uce58 \\uae30\\ub85d 2011\\ub144 4\\uc6d4 27\\uc77c 2231. 94\\ub3c4 \\ub118\\uc5b4\\uc120 \\uac83\\uc774\\ub2e4. \\uc774\\ub85c\\uc368 \\ud55c\\uad6d \\uc720\\uac00\\uc99d\\uad8c\\uc2dc\\uc7a5\\uc740 '\\ubc15\\uc2a4\\ud53c\\ubc15\\uc2a4\\uad8c\\uc5d0 \\uac16\\ud78c \\ucf54\\uc2a4\\ud53c'\\ub77c\\ub294 \\uc624\\uba85\\uc744 6\\ub144 \\ub9cc\\uc5d0 \\ub5bc\\uac8c \\ub410\\ub2e4. \\uc774\\ub0a0 \\ud55c\\uad6d\\uc99d\\uc2dc\\uc758 \\uc2dc\\uac00 \\ucd1d\\uc561 \\uae30\\ub85d\\ub3c4 \\uacbd\\uc2e0\\ub410\\ub2e4. \\ucf54\\uc2a4\\ud53c \\uc2dc\\uac00\\ucd1d\\uc561\\uc740 \\ub0a0 \\uc885\\uac00 1454\\uc8705775\\uc5b5\\uc6d0\\uc73c\\ub85c \\uc9d1\\uacc4\\ub410\\ub2e4. \\uc804\\ub0a0 1441\\uc870 1544\\uc5b5\\uc6d0\\uc774\\uc5c8\\ub358 \\uc2dc\\uac00\\ucd1d\\uc561\\uc740 \\ub0a0 13\\uc870\\uc6d0 \\uac00\\ub7c9 \\ub298\\uc5b4\\ub098\\uba74\\uc11c \\ucc98\\uc74c\\uc73c\\ub85c 1450\\uc870\\uc6d0\\uc744 \\ub118\\uc5b4\\uc11c\\uac8c \\ub41c \\uac83\\uc774\\ub2e4. \\uc99d\\uc2dc \\uc804\\ubb38\\uac00\\ub4e4\\uc740 \\uae00\\ub85c\\ubc8c \\uacbd\\uae30\\uac00 \\ud638\\uc870\\ub97c \\ubcf4\\uc774\\uace0 \\uae30\\uc5c5 \\uc2e4\\uc801\\uc774 \\uc774\\ub97c \\ub4b7\\ubc1b\\uce68\\ud558\\uba74\\uc11c \\uc678\\uad6d\\uc778 \\uc790\\uae08\\uc774 \\ucf54\\uc2a4\\ud53c\\ub97c \\ub808\\ubca8\\uc5c5 \\uc2dc\\ucf30\\ub2e4\\uace0 \\ubd84\\uc11d\\ud588\\ub2e4. \\uc774\\ub0a0\\ub3c4 \\ucf54\\uc2a4\\ud53c \\uae09\\ub4f1\\uc758 1 \\uacf5\\uc2e0\\uc740 \\uc678\\uad6d\\uc778 \\ud22c\\uc790\\uc790\\uc600\\ub2e4. \\uc678\\uad6d\\uc778\\uc740 \\ub0a0 \\uc720\\uac00\\uc99d\\uad8c\\uc2dc\\uc7a5\\uc5d0\\uc11c 3614\\uc5b5\\uc6d0 \\uc5b4\\uce58\\ub97c \\uc21c\\ub9e4\\uc218\\ud558\\uba74\\uc11c \\uc9c0\\uc218 \\uc0c1\\uc2b9\\uc744 \\ub04c\\uc5c8\\ub2e4. \\uae30\\uad00\\uacfc \\uac1c\\uc778 \\ud22c\\uc790\\uc790\\ub4e4\\uc774 3336\\uc5b5\\uc6d0, 709\\uc5b5\\uc6d0 \\uc5b4\\uce58\\ub97c \\uc21c\\ub9e4\\ub3c4\\ud588\\uc9c0\\ub9cc \\uc678\\uad6d\\uc778\\uc740 \\uc774\\ud2c0\\uc9f8 '\\ubc14\\uc774\\ucf54\\ub9ac\\uc544'\\ub97c \\uc774\\uc5b4\\uac14\\ub2e4. \\uc678\\uad6d\\uc778\\uc740 \\ud55c\\ubc18\\ub3c4\\ub97c \\ub458\\ub7ec\\uc2fc \\uc9c0\\uc815\\ud559\\uc801 \\uc704\\ud5d8\\uc774 \\ucee4\\uc84c\\uc74c\\uc5d0\\ub3c4 \\uc9c0\\ub09c\\ub2ec 20\\uc77c\\ubd80\\ud130 \\ucf54\\uc2a4\\ud53c\\uc5d0\\uc11c \\ub9e4\\uc218 \\uae30\\uc870\\ub97c \\uc720\\uc9c0\\ud574\\uc624\\uba74\\uc11c \\ub0a0\\uae4c\\uc9c0 2\\uc8701000\\uc5b5\\uc6d0 \\uc5b4\\uce58\\ub97c \\uc21c\\ub9e4\\uc218\\ud588\\ub2e4. \\uadf8\\ub9cc\\ud07c \\ud55c\\uad6d \\uae30\\uc5c5 \\uc2e4\\uc801 \\ud638\\uc870\\uc640 \\uae30\\uc5c5\\uc9c0\\ubc30\\uad6c\\uc870 \\uac1c\\uc120\\uc5d0 \\ub300\\ud55c \\ubbff\\uc74c\\uc774 \\ud070 \\uac83\\uc73c\\ub85c \\ud480\\uc774\\ub41c\\ub2e4. \\ucf54\\uc2a4\\ud53c\\uac00 \\ub9ce\\uc774 \\uc62c\\ub790\\ub2e4\\uace0 \\uc99d\\uc2dc\\uc5d0\\uc11c\\ub294 \\uc2e4\\uc81c \\uc2e4\\uc801 \\uac1c\\uc120\\uc5d0 \\ub35c \\uc624\\ub978 \\uac83\\uc73c\\ub85c \\ud3c9\\uac00\\uac00 \\ub098\\uc624\\uace0. \\ud55c\\uad6d \\uc8fc\\uc2dd\\uc774 \\uc77c\\ubcf8\\uc774\\ub098 \\ub300\\ub9cc\\uc5d0 \\ube44\\ud574 \\uc2f8\\ub2e4\\ub294 \\ubd84\\uc11d\\uc774 \\ub098\\uc62c \\uc815\\ub3c4\\ub2e4. \\ucf54\\uc2a4\\ud53c \\uc0c1\\uc7a5\\uc0ac\\uc758 \\uc5f0\\uac04 \\uc21c\\uc774\\uc775\\uc740 \\uc9c0\\ub09c\\ud574 94\\uc870\\uc6d0\\uc744 \\uae30\\ub85d\\ud55c \\uc774\\ud6c4 \\uc62c\\ud574\\ub294 120\\uc870\\uc6d0\\uc744 \\ub3cc\\ud30c\\ud560 \\uac83\\uc73c\\ub85c \\uc608\\uc0c1\\ub418\\uae30 \\ub54c\\ubb38\\uc774\\ub2e4. \\uc2dc\\uac00\\ucd1d\\uc561 1\\uc704\\uc778 \\uc0bc\\uc131\\uc804\\uc790\\uac00 \\uc5f0\\uc77c \\uc0ac\\uc0c1 \\ucd5c\\uace0\\uac00\\ub97c \\uae30\\ub85d\\ud558\\uace0 \\uc788\\ub294 \\uac83\\ub3c4 \\ucf54\\uc2a4\\ud53c \\ub808\\ubca8\\uc5c5\\uc758 \\uc8fc\\uc694\\ud55c \\uc6d0\\uc778\\uc774\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uc62c\\ud574 \\uae5c\\uc9dd \\uc2e4\\uc801\\uc744 \\uae30\\ub85d\\ud560 \\uac83\\uc73c\\ub85c \\uc804\\ub9dd\\ub418\\uba74\\uc11c \\uc9c0\\ub09c\\ud574 \\uc774\\ud6c4 \\uc8fc\\uac00\\uac00 30% \\uc62c\\ub790\\ub2e4. \\uc5ec\\uae30\\uc5d0 \\uc790\\uc0ac\\uc8fc \\uc18c\\uac01\\uc744 \\ube44\\ub86f\\ud55c \\uc8fc\\uc8fc \\ud658\\uc6d0\\ucc45\\uae4c\\uc9c0 \\ub0b4\\ub193\\uc790 \\uc0bc\\uc131\\uc804\\uc790 \\uc8fc\\uac00\\ub294 227\\ub9cc\\uc6d0\\uc120\\uc744 \\ub3cc\\ud30c\\ud55c \\uc0c1\\ud0dc\\ub2e4. \\ucd5c\\ud604\\ub9cc \\ubbf8\\ub798\\uc5d0\\uc14b\\ub300\\uc6b0 \\uc218\\uc11d\\ubd80\\ud68c\\uc7a5\\uc740 \\\"\\uc218\\ucd9c\\uacfc \\ud22c\\uc790\\ub97c \\uc911\\uc2ec\\uc73c\\ub85c \\ud55c \\uacbd\\uae30\\uc9c0\\ud45c\\uac00 \\uac1c\\uc120\\ub418\\uace0 \\uc788\\ub294 \\ub370\\ub2e4 \\uc0c1\\uc7a5 \\uae30\\uc5c5\\ub4e4\\uc758 \\uc21c\\uc774\\uc775\\ub3c4 \\uc0ac\\uc0c1 \\ucd5c\\ub300\\ub97c \\ub118\\uc5b4\\uc124 \\uac83\\uc73c\\ub85c \\uc608\\uc0c1\\ub418\\uba74\\uc11c \\uc62c 2\\ubd84\\uae30 \\uc774\\ud6c4 \\ucf54\\uc2a4\\ud53c \\uc804\\ub9dd\\uc740 \\ub354\\uc6b1 \\ubc1d\\ub2e4\\\"\\uace0 \\ub0b4\\ub2e4\\ubd24\\ub2e4.\",\n          \"\\uc0bc\\uc131\\uc804\\uc790\\uac00 26\\uc77c \\ubbf8\\uad6d '\\uc0bc\\uc131 \\uac1c\\ubc1c\\uc790 \\ucf58\\ud37c\\ub7f0\\uc2a4 202121'\\uc744 \\uc5f4\\uace0 \\ud601\\uc2e0\\uc801\\uc778 \\uace0\\uac1d \\uacbd\\ud5d8\\uc744 \\uac00\\ub2a5\\ud558\\uac8c \\ud558\\ub294 \\ucc28\\uc138\\ub300 \\uae30\\uc220\\ub4e4\\uc744 \\ub17c\\uc758\\ud588\\ub2e4. 2019\\ub144 \\uc774\\ud6c4 2\\ub144 \\ub9cc\\uc5d0 \\uc5f4\\ub9b0 \\ud589\\uc0ac\\ub294 \\uc628\\ub77c\\uc778\\uc73c\\ub85c \\uac1c\\ucd5c\\ub410\\ub2e4. \\uc774\\ub0a0 \\ud589\\uc0ac\\uc5d0\\uc11c\\ub294 \\u25b2\\uc778\\uacf5\\uc9c0\\ub2a5\\uc0ac\\ubb3c\\uc778\\ud130\\ub137 \\ubcf4\\uc548 \\ud50c\\ub7ab\\ud3fc \\ud601\\uc2e0 \\u25b2\\ud0c0\\uc774\\uc820 \\uae30\\ubc18\\uc758 \\uc2a4\\ud06c\\ub9b0 \\uacbd\\ud5d8 \\ud655\\uc7a5 \\u25b2\\uac24\\ub7ed\\uc2dc \\uc0dd\\ud0dc\\uacc4 \\uac15\\ud654 \\ubc29\\uc548\\uc5d0 \\ub300\\ud574 \\uc804 \\uc138\\uacc4 \\uac1c\\ubc1c\\uc790, \\ud30c\\ud2b8\\ub108\\uc0ac, \\uc18c\\ube44\\uc790\\ub4e4\\uc5d0\\uac8c \\uacf5\\uac1c\\ud588\\ub2e4. \\ud2b9\\ud788, \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uc18c\\ube44\\uc790 \\uc911\\uc2ec \\ud601\\uc2e0\\uc758 \\uc911\\uc694\\uc131\\uc744 \\uac15\\uc870\\ud558\\uba70, \\ube45\\uc2a4\\ube44, \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4, \\uc0bc\\uc131 \\ub179\\uc2a4 \\ud50c\\ub7ab\\ud3fc\\uc758 \\ubc1c\\uc804\\uc774 \\uc18c\\ube44\\uc790\\ub4e4\\uc758 \\uc2e4\\uc0dd\\ud65c\\uc5d0 \\ub3c4\\uc6c0\\uc744 \\uc904 \\uc218 \\uc788\\ub294 \\ubc29\\uc548\\ub4e4\\uc744 \\uc120 \\ubcf4\\uc600\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790 \\uace0\\ub3d9\\uc9c4 \\ub300\\ud45c\\uc774\\uc0ac \\uc0ac\\uc7a5\\uc740 \\ub0a0 \\uae30\\uc870\\uc5f0\\uc124\\uc5d0\\uc11c \\\"\\uc0bc\\uc131\\uc804\\uc790\\uac00 \\uc804 \\uc138\\uacc4 \\ucc3d\\uc758\\uc801\\uc778 \\uac1c\\ubc1c\\uc790\\ub4e4\\uacfc \\ud611\\ub825\\ud574 \\uc0c8\\ub85c\\uc6b4 \\uc2dc\\ub300\\ub97c \\uc5f4\\uc5b4 \\ub098\\uac08 \\uc218 \\uc788\\uc5b4 \\uc790\\ubd80\\uc2ec\\uc744 \\ub290\\ub080\\ub2e4\\\"\\uba70 \\\"\\uc218\\ub9ce\\uc740 \\uae30\\uae30\\uc640 \\ub124\\ud2b8\\uc6cc\\ud06c\\ub97c \\ud1b5\\ud574 \\uc628 \\uc138\\uc0c1\\uc774 \\ubc00 \\uc811\\ud558\\uac8c \\uc5f0\\uacb0\\ub418\\uace0 \\uc788\\ub294 \\ubaa8\\ub450\\uc758 \\uc0b6\\uc744 \\ub354 \\ud3b8\\ub9ac\\ud558\\uace0 \\uc2a4\\ub9c8\\ud2b8\\ud558\\uac8c \\ub9cc\\ub4e4 \\uc218 \\uc788\\ub3c4\\ub85d \\ud601\\uc2e0\\uc744 \\uc9c0\\uc18d\\ud558\\uaca0\\ub2e4\\\"\\ub77c\\uace0 \\ub9d0\\ud588\\ub2e4. \\uc18c\\ube44\\uc790 \\uacbd\\ud5d8 \\ud5a5\\uc0c1 \\uac1c\\ubc1c\\uc790 \\uc791\\uc5c5 \\uc9c0\\uc6d0 \\ud50c\\ub7ab\\ud3fc \\uac15\\ud654 \\ubc29\\uc548 \\uacf5\\uac1c \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\ubc88 \\ucf58\\ud37c\\ub7f0\\uc2a4\\uc5d0\\uc11c \\uc18c\\ube44\\uc790 \\uacbd\\ud5d8\\uc744 \\ud5a5\\uc0c1\\uc2dc\\ud0a4\\uace0 \\uac1c\\ubc1c\\uc790\\ub4e4\\uc758 \\uc791\\uc5c5\\uc744 \\uc9c0\\uc6d0\\ud558\\uae30 \\uc704\\ud55c \\ube45\\uc2a4\\ube44, \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4, \\uc0bc\\uc131 \\ub179\\uc2a4 \\ud50c\\ub7ab\\ud3fc \\uac15\\ud654 \\ubc29\\uc548\\uc744 \\uacf5\\uac1c\\ud588\\ub2e4. 3\\uc5b5 \\ub300 \\uc774\\uc0c1\\uc758 \\uc0bc\\uc131 \\uae30\\uae30\\uc5d0 \\uc801\\uc6a9\\ub41c \\ud50c\\ub7ab\\ud3fc \\ube45\\uc2a4\\ube44\\ub294 \\uc131\\ub2a5\\uc774 \\ub354\\uc6b1 \\uac15\\ud654\\ub41c\\ub2e4. \\ube45\\uc2a4\\ube44\\ub294 \\ucd5c\\uc2e0 \\uc2a4\\ub9c8\\ud2b8\\ud3f0\\uc5d0\\uc11c \\uc628\\ub514\\ubc14\\uc774\\uc2a4 \\ud65c\\uc6a9\\ud574 \\uae30\\uae30 \\uc790\\uccb4\\uc5d0\\uc11c \\uba85\\ub839\\uc744 \\uc2e4\\ud589\\ud568\\uc73c\\ub85c\\uc368 \\ubc18\\uc751 \\uc18d\\ub3c4\\uac00 \\ucd5c\\ub300 35% \\ube68\\ub77c\\uc84c\\ub2e4. \\ud50c\\ub7ab\\ud3fc\\uc778 \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4\\uc758 \\uc5f0\\uacb0\\uc131\\uacfc \\ud638\\ud658 \\uae30\\uae30\\ub3c4 \\ub354\\uc6b1 \\ud655\\ub300\\ub41c\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\ub294 '\\uc2a4\\ub9c8\\ud2b8\\uc2f1\\uc2a4 \\ud5c8\\ube0c ' \\uc18c\\ud504\\ud2b8\\uc6e8\\uc5b4\\ub97c \\ud328\\ubc00\\ub9ac \\ud5c8\\ube0c \\ub0c9\\uc7a5\\uace0 \\uc0bc\\uc131 \\uc81c\\ud488\\uc5d0 \\ud0d1\\uc7ac\\ud574, \\ud574\\ub2f9 \\uc81c\\ud488\\uc5d0\\uc11c \\uae30\\uae30\\uc640 \\uc5f0\\uacb0\\ud560 \\uc218 \\uc788\\ub3c4\\ub85d \\uc9c0\\uc6d0\\ud560 \\uacc4\\ud68d\\uc774\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uac1c\\ubc1c\\uc790\\ub4e4\\uc774 \\ube45\\uc2a4\\ube44\\uc640 \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4 \\uac04 \\uc5f0\\uacc4\\ub97c \\uac15\\ud654\\ud560 \\uc218 \\uc788\\ub294 '\\ube45\\uc2a4\\ube44 \\ud648' \\ud50c\\ub7ab\\ud3fc\\ub3c4 \\uc120\\ubcf4\\uc600\\ub2e4. '\\ube45\\uc2a4\\ube44 \\ud648' \\ud50c\\ub7ab\\ud3fc\\uc740 \\ud604\\uc7ac \\uc5f0\\uacb0\\ub3fc \\uc788\\ub294 \\uc81c\\ud488\\ub4e4\\uc758 \\uc0c1\\ud0dc\\ub97c \\uace0\\ub824\\ud574 \\uc74c\\uc131 \\uba85\\ub839\\uc744 \\uc218\\ud589\\ud560 \\uc218. \\\"\\uc601\\ud654 \\uc7ac\\uc0dd\\ud574\\uc918\\\"\\ub77c\\uace0 \\ub9d0\\ud558\\uba74 \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4\\uc5d0 \\uc5f0\\uacb0\\ub41c , \\uc0ac\\uc6b4\\ub4dc\\ubc14, \\uc870\\uba85 \\uac19\\uc740 \\uae30\\uae30\\ub4e4\\uc774 \\uc791\\ub3d9\\ud574 \\uc2a4\\ub9c8\\ud2b8\\ud648 \\uacbd\\ud5d8\\uc744 \\ub354\\uc6b1 \\ud3b8\\ub9ac\\ud558\\uac8c \\ud574\\uc900\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uc2a4\\ub9c8\\ud2b8 \\uc2f1\\uc2a4\\uc5d0 \\uc5c5\\uacc4 \\ucd5c\\uc2e0 \\ud1b5\\uc2e0 \\uaddc\\uaca9\\uc778 '\\ub9e4\\ud130'\\ub97c \\uc801\\uc6a9\\ud574, \\ud30c\\ud2b8\\ub108\\uc0ac\\ub97c \\ud655\\ub300\\ud558\\uace0 \\ud638\\ud658 \\uae30\\uae30\\ub4e4\\uc774 \\ub354 \\uc27d\\uac8c \\uc5f0\\uacb0\\ub418\\ub3c4\\ub85d \\ud560 \\uacc4\\ud68d\\uc774\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\ubcf4\\uc548\\uc5d0 \\uc788\\uc5b4\\uc11c \\u25b2\\ubcf4\\ud638 \\u25b2\\uc120\\ud0dd \\u25b2\\ud22c\\uba85\\uc131\\uc758 3\\ub300 \\uc6d0\\uce59\\uc744 \\uc900\\uc218\\ud558\\uace0 \\uc18c\\ube44\\uc790, \\uac1c\\ubc1c\\uc790, \\ud30c\\ud2b8\\ub108\\uc0ac\\uc5d0\\uac8c \\ub370\\uc774\\ud130 \\uad00\\ub9ac \\ubc29\\ubc95\\uc744 \\uacb0\\uc815\\ud558\\uae30 \\uc704\\ud574 \\ud544\\uc694\\ud55c \\uc815\\ubcf4\\ub97c \\uc81c\\uacf5\\ud558\\uace0. \\ub610, \\ubcf4\\uc548 \\uae30\\uc220\\uc744 \\uc704\\ud55c \\ud611\\uc5c5\\uacfc \\uac1c\\ubc29\\ud615 \\ud601\\uc2e0 \\ucca0\\ud559\\uc744 \\uc911\\uc2dc\\ud558\\uba70, \\ubcf4\\uc548 \\ubd84\\uc11d \\ub3c4\\uad6c\\ub97c \\uc624\\ud508\\uc18c\\uc2a4 \\ud504\\ub85c\\uc81d\\ud2b8\\ub85c \\uc9c0\\uc18d \\ucd9c\\uc2dc\\ud560 \\uacc4\\ud68d\\uc774\\ub2e4. '\\uc2a4\\ud06c\\ub9b0 \\ud3ec \\uc62c' \\uc0ac\\uc6a9 \\uacbd\\ud5d8 \\ud655\\uc7a5\\uc0c8\\ub85c\\uc6b4 \\uae30\\ub2a5 \\uc18c\\uac1c '\\uc2a4\\ud06c\\ub9b0 \\ud3ec \\uc62c ' \\ube44\\uc804\\uc744 \\ubc14\\ud0d5\\uc73c\\ub85c \\uc138\\uacc4 \\uc2dc\\uc7a5\\uc744 \\uc120\\ub3c4 \\ud558\\ub294 \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uc0ac\\uc6a9 \\uacbd\\ud5d8\\uc744 \\ud655\\uc7a5\\ud558\\ub294 \\uc0c8\\ub85c\\uc6b4 \\uae30\\ub2a5\\ub4e4\\uc744 \\uc18c\\uac1c\\ud588\\ub2e4. \\ud654\\uc0c1\\ud1b5\\ud654\\uc758 \\uacbd\\uc6b0, \\uad6c\\uae00\\uacfc\\uc758 \\ud611\\ub825\\uc744 \\ud1b5\\ud574 \\uae30\\ubc18\\uc73c\\ub85c \\uc778\\ubb3c\\uc744 \\ud3ec\\ucee4\\uc2a4\\ud558\\uac70\\ub098 \\uc90c\\uc778\\ud558\\ub294 \\uae30\\ub2a5\\uc744 \\uc0ac\\uc6a9\\ud560 \\uc218 \\uc788\\uc5b4 \\ud3b8\\uc758\\uc131\\uc774 \\uac15\\ud654\\ub41c\\ub2e4. \\uc6a9 \\uc0bc\\uc131 \\ud5ec\\uc2a4\\uc5d0\\uc11c\\ub294 \\uc6f9\\ucea0, \\ubaa8\\ubc14\\uc77c \\uae30\\uae30\\uc640 \\uc5f0\\uacb0\\ud558\\uba74 \\uc2a4\\ub9c8\\ud2b8 \\ud2b8\\ub808\\uc774\\ub108\\uac00 \\uc0ac\\uc6a9\\uc790\\uc758 \\uc6b4\\ub3d9\\uc744 \\ubd84\\uc11d\\ud558\\uace0 \\uc2e4\\uc2dc\\uac04 \\ud53c\\ub4dc\\ubc31\\uc744 \\uc81c\\uacf5\\ud560 \\uc218. \\ucc28\\uc138\\ub300 \\ud654\\uc9c8 \\uae30\\uc220\\uc778 10 \\ud45c\\uc900\\uc774 \\uac8c\\uc784\\uc73c\\ub85c\\ub3c4 \\ud655\\ub300 \\uc801\\uc6a9\\ub3fc \\ub354 \\uc2e4\\uc801\\uc774\\uace0 \\ubab0\\uc785\\uac10 \\uc788\\ub294 \\uac8c\\uc784\\uc744 \\uc990\\uae38 \\uc218. \\uc0bc\\uc131\\uc804\\uc790\\ub294 2 \\uc11c\\ube44\\uc2a4\\uc778 '\\uae30\\uc5c5\\uc6a9 \\ud0c0\\uc774\\uc820 '\\uc744 \\uc120 \\ubcf4\\uc600\\ub2e4. \\uc11c\\ube44\\uc2a4\\ub97c \\ud1b5\\ud574 \\uc5ec\\ub7ec \\uae30\\uae30\\ub97c \\uc2e4\\uc2dc\\uac04\\uc73c\\ub85c \\ubaa8\\ub2c8\\ud130\\ub9c1\\ud558\\uace0 \\uc6d0\\uaca9 \\uc81c\\uc5b4\\ud560 \\uc218 \\uc788\\uc5b4 \\uae30\\uc5c5\\uae30\\uad00\\ub4e4\\uc740 \\ud559\\uad50\\uc758 \\uce60\\ud310, \\uc1fc\\ud551\\ubab0 \\ud0a4\\uc624\\uc2a4\\ud06c\\uc5d0\\uc11c \\uc9c0\\ud558\\ucca0\\uacfc \\uacf5\\ud56d\\uc758 \\uc2a4\\ud06c\\ub9b0\\uae4c\\uc9c0 \\ub2e4\\uc591\\ud55c \\ub514\\uc2a4\\ud50c\\ub808\\uc774\\ub97c \\ud3b8\\ub9ac\\ud558\\uac8c \\uad00\\ub9ac \\uac00\\ub2a5\\ud558\\ub2e4. '\\ud3f4\\ub354\\ube14\\ud3f0 \\uc6e8\\uc5b4\\ub7ec\\ube14 \\uace0\\uc720 \\uc0ac\\uc6a9\\uc790 \\uacbd\\ud5d8\\uc73c\\ub85c \\uac24\\ub7ed\\uc2dc \\uc0dd\\ud0dc\\uacc4 \\uac15\\ud654 \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\uac24\\ub7ed\\uc2dc \\ubaa8\\ubc14\\uc77c \\uae30\\uae30\\uc758 \\ucd5c\\uc2e0 \\uc0ac\\uc6a9\\uc790 \\uacbd\\ud5d8\\uc778 '\\uc6d0 4 4'\\ub97c \\uacf5\\uac1c\\ud588\\ub2e4. \\uc6d0 4\\ub294 \\uc0ac\\uc6a9\\uc790\\uac00 \\uc790\\uc2e0\\ub9cc\\uc758 \\uacbd\\ud5d8\\uc744 \\ub9cc\\ub4e4 \\uc218 \\uc788\\ub3c4\\ub85d \\uc0c8\\ub85c\\uc6b4 \\ucc28\\uc6d0\\uc758 \\ub9de\\ucda4\\ud615 \\uae30\\ub2a5\\uc744 \\uc81c\\uacf5\\ud558\\uba70, \\ubcf4\\uc548 \\uba74\\uc5d0\\uc11c\\ub3c4 \\uc0ac\\uc6a9\\uc790\\uac00 \\uc815\\ubcf4 \\uc774\\uc6a9 \\uad8c\\ud55c\\uc744 \\uc870\\uc815\\ud574 \\uc790\\uc2e0\\uc758 \\ub370\\uc774\\ud130\\ub97c \\uad00\\ub9ac\\ud560 \\uc218 \\uc788\\ub3c4\\ub85d \\ud55c\\ub2e4. \\ub610\\ud55c, '\\uac24\\ub7ed\\uc2dc \\ud3f4\\ub4dc3' '\\uac24\\ub7ed\\uc2dc \\ud50c\\ub9bd3' \\ud3f4\\ub354\\ube14\\ud3f0\\uc73c\\ub85c \\ud3fc\\ud329\\ud130\\uac00 \\ud655\\uc7a5\\ub428\\uc5d0 \\uac1c\\ubc1c\\uc790\\ub4e4\\uc740 \\ud50c\\ub809\\uc2a4 \\ubaa8\\ub4dc, \\uba40\\ud2f0 \\uc561\\ud2f0\\ube0c \\uc708\\ub3c4\\uc6b0 \\uc0bc\\uc131 \\ud3f4\\ub354\\ube14\\ud3f0 \\uace0\\uc720\\uc758 \\uc0ac\\uc6a9 \\uacbd\\ud5d8\\uc744 \\ud65c\\uc6a9\\ud560 \\uc218. \\uc0bc\\uc131\\uc804\\uc790\\ub294 \\ub0a0 \\ucf58\\ud37c\\ub7f0\\uc2a4\\uc5d0\\uc11c '\\uac24\\ub7ed\\uc2dc \\uc6cc\\uce584' \\uc2dc\\ub9ac\\uc988\\uc5d0 \\ud0d1\\uc7ac\\ub41c \\uc0bc\\uc131\\uc758 \\ub3c5\\uc790\\uc801\\uc778 \\uc0ac\\uc6a9\\uc790 \\uacbd\\ud5d8\\uc778 '\\uc6d0 \\uc6cc\\uce58', \\uad6c\\uae00\\uacfc \\uacf5\\ub3d9 \\uac1c\\ubc1c\\ud55c \\uc2e0\\uaddc \\uc6e8\\uc5b4\\ub7ec\\ube14 \\ud1b5\\ud569 \\ud50c\\ub7ab\\ud3fc\\ub3c4 \\uc18c\\uac1c\\ud588\\ub2e4. \\ud589\\uc0ac\\uc5d0 \\ub300\\ud55c \\uc790\\uc138\\ud55c \\ub0b4\\uc6a9\\uc740 \\uc0bc\\uc131 \\uac1c\\ubc1c\\uc790 \\ucf58\\ud37c\\ub7f0\\uc2a4 \\uacf5\\uc2dd \\uc6f9\\uc0ac\\uc774\\ud2b8\\uc5d0\\uc11c \\ud655\\uc778\\ud560 \\uc218.\",\n          \"\\uc0bc\\uc131\\uc804\\uc790 \\ubd80\\ubb38\\uc740 \\uacbd\\uae30\\uc0ac\\ud68c\\ubcf5\\uc9c0\\uacf5\\ub3d9\\ubaa8\\uae08\\ud68c\\uc640 \\uacbd\\uae30 \\ud654\\uc131\\uc6a9\\uc778 \\ud3c9\\ud0dd \\uc9c0\\uc5ed\\uc5d0\\uc11c '\\ud589\\ubcf5 \\ubaa8\\uc790\\uc774\\ud06c \\ud504\\ub85c\\uc81d\\ud2b8'\\ub97c \\uc2dc\\ud589\\ud55c\\ub2e4\\uace0 4\\uc77c \\ubc1d\\ud614\\ub2e4. \\ud589\\ubcf5 \\ubaa8\\uc790\\uc774\\ud06c\\ub294 \\uc9c0\\uc5ed\\uc0ac\\ud68c\\uc758 \\uc18c\\uc678\\ub41c \\uc774\\uc6c3\\uc774 \\ud589\\ubcf5\\ud560 \\uc218 \\uc788\\ub3c4\\ub85d \\ucc38\\uc2e0\\ud55c \\ud504\\ub85c\\uadf8\\ub7a8\\uc744 \\uc9c0\\uc5ed \\uc804\\ubb38\\uac00\\ub4e4\\uc774 \\ubc1c\\uad74\\ud558\\uba74, \\uc0bc\\uc131\\uc804\\uc790 \\ubd80\\ubb38 \\uc784\\uc9c1\\uc6d0\\ub4e4\\uc758 \\uae30\\ubd80\\uae08\\uc73c\\ub85c \\ud504\\ub85c\\uadf8\\ub7a8\\uc744 \\uc9c4\\ud589\\ud558\\ub294 \\uc0ac\\ud68c \\uacf5\\ud5cc\\uc0ac\\uc5c5\\uc774\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790\\uac00 \\uc9c0\\ub09c 4\\uc6d4 \\ud55c \\ub2ec\\uac04 \\uc9c0\\uc5ed\\uc0ac\\ud68c 700\\uc5ec \\uac1c \\uc0ac\\ud68c\\ubcf5\\uc9c0\\uae30\\uad00\\uc744 \\ub300\\uc0c1\\uc73c\\ub85c \\uacf5\\ubaa8\\ud55c \\ub4a4 \\uad50\\uc218\\uc640 \\uacf5\\ubb34\\uc6d0, \\uc0ac\\ud68c\\uacf5\\ud5cc \\uc804\\ubb38 \\ucee8\\uc124\\ud134\\ud2b8\\ub85c \\uad6c\\uc131\\ub41c \\uc2ec\\uc0ac\\ub2e8\\uc744 \\ud1b5\\ud574 13\\uac1c \\ud504\\ub85c\\uc81d\\ud2b8\\ub97c \\ud655\\uc815\\ud588\\ub2e4. \\ud504\\ub85c\\uc81d\\ud2b8\\ub294 \\uae30\\ud68d \\uc8fc\\uccb4\\uc778 \\ubcf5\\uc9c0\\uae30\\uad00\\uc5d0\\uc11c \\ub0b4\\ub144 6\\uc6d4\\uae4c\\uc9c0 1\\ub144\\uac04 \\uc6b4\\uc601\\ud558\\uac8c \\ub41c\\ub2e4. \\uc0bc\\uc131\\uc804\\uc790 \\ubd80\\ubb38 \\uc0ac\\ud68c \\ubd09\\uc0ac\\ub2e8\\uc7a5 \\uae40\\uc120\\uc2dd \\uc804\\ubb34\\ub294 \\\"\\uc608\\uc0c1\\ubcf4\\ub2e4 \\ub9ce\\uc740 \\uc544\\uc774\\ub514\\uc5b4\\uac00 \\uc811\\uc218\\ub410\\ub2e4\\\"\\uba74\\uc11c \\\"\\ud504\\ub85c\\uc81d\\ud2b8 \\uc120\\uc815 \\uae30\\uad00\\ub4e4\\uc774 \\uc9c0\\uc5ed \\ubb38\\uc81c \\ud574\\uacb0\\uc5d0 \\ud070 \\uc5f4\\uc758\\ub97c \\ubcf4\\uc5ec \\uc88b\\uc740 \\uc131\\uacfc\\uac00 \\uae30\\ub300\\ub41c\\ub2e4\\\"\\uace0 \\ub9d0\\ud588\\ub2e4.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 CSV Save\n",
        "final_매일경제.to_csv('/content/drive/MyDrive/BOAZ/23기 분석 BASE/미니프로젝트/감성분석/final_매일경제.csv',encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "1HfCxzF-mP7r"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "La9CnVmKIlmT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a64edf1500b40d5aa6257e8a16199e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aff7f82e8c754ef782bad2b306210d16",
              "IPY_MODEL_63345ad270254aafa4d3884215ae2f6a",
              "IPY_MODEL_e5a996dcced342cfbaf7af4135335830"
            ],
            "layout": "IPY_MODEL_9a41b76a568042f6a447a2e7f4abfcde"
          }
        },
        "aff7f82e8c754ef782bad2b306210d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ef4c7d72ab54d89af1e6ba1e23dd7b8",
            "placeholder": "​",
            "style": "IPY_MODEL_545a33f5458f40cd822f406eef2c288a",
            "value": "100%"
          }
        },
        "63345ad270254aafa4d3884215ae2f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e63da71294ab453398a0ff173d0b45bc",
            "max": 121,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ecfcbb323e28411b9377ee27a154db8b",
            "value": 121
          }
        },
        "e5a996dcced342cfbaf7af4135335830": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaa3cec8724c450da3ae31c5d03d435a",
            "placeholder": "​",
            "style": "IPY_MODEL_799db078221e47c6974c9451364fddf7",
            "value": " 121/121 [02:58&lt;00:00,  1.35s/it]"
          }
        },
        "9a41b76a568042f6a447a2e7f4abfcde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ef4c7d72ab54d89af1e6ba1e23dd7b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "545a33f5458f40cd822f406eef2c288a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e63da71294ab453398a0ff173d0b45bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecfcbb323e28411b9377ee27a154db8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eaa3cec8724c450da3ae31c5d03d435a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "799db078221e47c6974c9451364fddf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ed4c35672fc48b385a8d98c784d3a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35bbbff88f194e5b8ab4cb869bf87a32",
              "IPY_MODEL_0962c5dd16544e84a2ea69b7156c63c1",
              "IPY_MODEL_8128901c619244c1a7e4f825ecef1365"
            ],
            "layout": "IPY_MODEL_6ade17158af441f8891e760974064cba"
          }
        },
        "35bbbff88f194e5b8ab4cb869bf87a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9c62e7a142f4405a5f5eb5b30fe5ddf",
            "placeholder": "​",
            "style": "IPY_MODEL_5574aa6ad8f54ae385fd2ca61813a463",
            "value": "100%"
          }
        },
        "0962c5dd16544e84a2ea69b7156c63c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef50b518b796417f8b9a43df9c40cac1",
            "max": 121,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dbf2270aecdc461ba8ace414848077fe",
            "value": 121
          }
        },
        "8128901c619244c1a7e4f825ecef1365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb3189ee615d4b9a9c291cbb30e48cdd",
            "placeholder": "​",
            "style": "IPY_MODEL_7f7f049f5ff84088967c305fd0b8daac",
            "value": " 121/121 [02:59&lt;00:00,  1.34s/it]"
          }
        },
        "6ade17158af441f8891e760974064cba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9c62e7a142f4405a5f5eb5b30fe5ddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5574aa6ad8f54ae385fd2ca61813a463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef50b518b796417f8b9a43df9c40cac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbf2270aecdc461ba8ace414848077fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb3189ee615d4b9a9c291cbb30e48cdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f7f049f5ff84088967c305fd0b8daac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e922fbc4a62c47de82b71f048bec507e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4ce8e60b1b54ab3b9eb0a05eb077bc4",
              "IPY_MODEL_ef68bf630e034bec9d464709d9fc4da6",
              "IPY_MODEL_ec81b84e201743b7b606f7c648d047ae"
            ],
            "layout": "IPY_MODEL_0af2c4729b78470086a2f7f2aa6744b3"
          }
        },
        "e4ce8e60b1b54ab3b9eb0a05eb077bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b0ea2fdecf74f8d8ce2bc08c081a0c1",
            "placeholder": "​",
            "style": "IPY_MODEL_2d9c5c6f20d94745ac0a54764e49d811",
            "value": "100%"
          }
        },
        "ef68bf630e034bec9d464709d9fc4da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b000534b4ee646b0a86aed136892bcc8",
            "max": 121,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db16c1b814e747dbada283538dc5d8c4",
            "value": 121
          }
        },
        "ec81b84e201743b7b606f7c648d047ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b0154de48d84dc497c4608b1b6c8458",
            "placeholder": "​",
            "style": "IPY_MODEL_bd0a4894934044d3964aef97ba587846",
            "value": " 121/121 [02:59&lt;00:00,  1.34s/it]"
          }
        },
        "0af2c4729b78470086a2f7f2aa6744b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b0ea2fdecf74f8d8ce2bc08c081a0c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d9c5c6f20d94745ac0a54764e49d811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b000534b4ee646b0a86aed136892bcc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db16c1b814e747dbada283538dc5d8c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b0154de48d84dc497c4608b1b6c8458": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd0a4894934044d3964aef97ba587846": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef0f3fc941854b0fa24210fe22453409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46934d94668d414380d74723870ba04d",
              "IPY_MODEL_9021b3725f5d48e289475445dadc1278",
              "IPY_MODEL_f3c3bcd10cfe45efae4ed5701e0fe5c5"
            ],
            "layout": "IPY_MODEL_afcb81cc9525414686a847c14e10c0b8"
          }
        },
        "46934d94668d414380d74723870ba04d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e772f67354b548ff87ff2d7efa251a4b",
            "placeholder": "​",
            "style": "IPY_MODEL_f29e183b401b490c9b8833f097ea339b",
            "value": "100%"
          }
        },
        "9021b3725f5d48e289475445dadc1278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e2e8822a97140299c1ee84094a6ae03",
            "max": 121,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3475490566404627943a659e22332129",
            "value": 121
          }
        },
        "f3c3bcd10cfe45efae4ed5701e0fe5c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_526025ffb89d4f6792d121abe024ec23",
            "placeholder": "​",
            "style": "IPY_MODEL_4fec4df44f664e1db9638706f2ccf19e",
            "value": " 121/121 [02:59&lt;00:00,  1.34s/it]"
          }
        },
        "afcb81cc9525414686a847c14e10c0b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e772f67354b548ff87ff2d7efa251a4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f29e183b401b490c9b8833f097ea339b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e2e8822a97140299c1ee84094a6ae03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3475490566404627943a659e22332129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "526025ffb89d4f6792d121abe024ec23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fec4df44f664e1db9638706f2ccf19e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "642a20dee0644e81876e2abc8d8122d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_129242362d3246dd8975968d39a1651c",
              "IPY_MODEL_d8fc273082a6430898338f19e39a81a0",
              "IPY_MODEL_a15d4c3075064571a2b73cfec45ca3e1"
            ],
            "layout": "IPY_MODEL_4d375086f71e4d499276ca41a52a5986"
          }
        },
        "129242362d3246dd8975968d39a1651c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce46895eba8444779158cd516be41b23",
            "placeholder": "​",
            "style": "IPY_MODEL_ecf45a26c78a489b8d4bb2034cd3389e",
            "value": "100%"
          }
        },
        "d8fc273082a6430898338f19e39a81a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a844598428664500b5067d7b593905f2",
            "max": 121,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f42e9b0dbf24a2a83c0311745a71122",
            "value": 121
          }
        },
        "a15d4c3075064571a2b73cfec45ca3e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_696fe8e39eaf4982b2e9214c23cf1243",
            "placeholder": "​",
            "style": "IPY_MODEL_3ab70c1a8f5b48dc94d0bf9713020a49",
            "value": " 121/121 [02:59&lt;00:00,  1.34s/it]"
          }
        },
        "4d375086f71e4d499276ca41a52a5986": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce46895eba8444779158cd516be41b23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecf45a26c78a489b8d4bb2034cd3389e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a844598428664500b5067d7b593905f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f42e9b0dbf24a2a83c0311745a71122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "696fe8e39eaf4982b2e9214c23cf1243": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ab70c1a8f5b48dc94d0bf9713020a49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "595388fb1fa948d0b580a68f40e50879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e69fb5b943eb47cfb432c7385804e42d",
              "IPY_MODEL_2d232a0379b14d86b4239481c3f8f692",
              "IPY_MODEL_ffb6b447b8b945638a38e268514ec06d"
            ],
            "layout": "IPY_MODEL_9cf58fd1e74c402d8f4c9c345bc76282"
          }
        },
        "e69fb5b943eb47cfb432c7385804e42d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a77d4c1298a24ff5b319cef2a71275a3",
            "placeholder": "​",
            "style": "IPY_MODEL_d966dfda02284dad99c5edbc1aede483",
            "value": "100%"
          }
        },
        "2d232a0379b14d86b4239481c3f8f692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1883d3d58afb4dcaa069450418c0a90f",
            "max": 121,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8632478077e446f58976faaea72e01d1",
            "value": 121
          }
        },
        "ffb6b447b8b945638a38e268514ec06d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f410797350c64b10a757f5c26a171a92",
            "placeholder": "​",
            "style": "IPY_MODEL_fc2a7bb15e514dc8b9edb189d32fbe78",
            "value": " 121/121 [02:59&lt;00:00,  1.35s/it]"
          }
        },
        "9cf58fd1e74c402d8f4c9c345bc76282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a77d4c1298a24ff5b319cef2a71275a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d966dfda02284dad99c5edbc1aede483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1883d3d58afb4dcaa069450418c0a90f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8632478077e446f58976faaea72e01d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f410797350c64b10a757f5c26a171a92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc2a7bb15e514dc8b9edb189d32fbe78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8639f59e0b2449c95c2b63797e54c6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0bd0a5b9f70243539e434a86732580ab",
              "IPY_MODEL_8428fb2a0561425ea6ad9ecf316584bc",
              "IPY_MODEL_7ea91b322ce64ac7b4f77bc019e91330"
            ],
            "layout": "IPY_MODEL_126c29d7a5824a628bc22821ed80b63c"
          }
        },
        "0bd0a5b9f70243539e434a86732580ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e09dccf7216d430b84e42476436f871a",
            "placeholder": "​",
            "style": "IPY_MODEL_3242d6e329ba470abd2ffca2d07cbb9d",
            "value": "100%"
          }
        },
        "8428fb2a0561425ea6ad9ecf316584bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbe18b780fb74cdab90e360726f0a12e",
            "max": 31,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46bd84b677504ea397e6e62b6133ba51",
            "value": 31
          }
        },
        "7ea91b322ce64ac7b4f77bc019e91330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d6bd8f2de8f4ae8b305659b38ec6c7f",
            "placeholder": "​",
            "style": "IPY_MODEL_ca571f859d23487aa729ae25ade61437",
            "value": " 31/31 [00:16&lt;00:00,  1.88it/s]"
          }
        },
        "126c29d7a5824a628bc22821ed80b63c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e09dccf7216d430b84e42476436f871a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3242d6e329ba470abd2ffca2d07cbb9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbe18b780fb74cdab90e360726f0a12e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46bd84b677504ea397e6e62b6133ba51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d6bd8f2de8f4ae8b305659b38ec6c7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca571f859d23487aa729ae25ade61437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57ef3457eda54651a0f6b3814a3f90bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43f0357c6ff6460d8d73efa62acf252a",
              "IPY_MODEL_a2aa1e7159a2446f8c422bb9dd796fa0",
              "IPY_MODEL_b608f0b298624175b6e7b107b0ed2ba7"
            ],
            "layout": "IPY_MODEL_a8bfe8cd52424fe09f2dc1ef18621979"
          }
        },
        "43f0357c6ff6460d8d73efa62acf252a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3b7a0fd86eb4b58887c56efe8df707d",
            "placeholder": "​",
            "style": "IPY_MODEL_e0ffba37a4a34724917db913b99e2b3a",
            "value": "100%"
          }
        },
        "a2aa1e7159a2446f8c422bb9dd796fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17b013ffe2564527ab6beddaecdcb1a2",
            "max": 1342,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9923ae7e2ab4274ba558220b18d75bd",
            "value": 1342
          }
        },
        "b608f0b298624175b6e7b107b0ed2ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2acfd52feb34328acfc0820d13ca810",
            "placeholder": "​",
            "style": "IPY_MODEL_757086af102a4c1c994a7ca9f1d2877a",
            "value": " 1342/1342 [07:13&lt;00:00,  3.10it/s]"
          }
        },
        "a8bfe8cd52424fe09f2dc1ef18621979": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3b7a0fd86eb4b58887c56efe8df707d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0ffba37a4a34724917db913b99e2b3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17b013ffe2564527ab6beddaecdcb1a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9923ae7e2ab4274ba558220b18d75bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2acfd52feb34328acfc0820d13ca810": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "757086af102a4c1c994a7ca9f1d2877a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}