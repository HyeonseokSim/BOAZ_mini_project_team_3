{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfxuT5WPcnSN"
      },
      "source": [
        "# 패키지 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK6YnrjOcnTn"
      },
      "outputs": [],
      "source": [
        "# 필요한 패키지 설치\n",
        "!pip install hanja   # 한자를 한글로 변환하기 위한 패키지\n",
        "!pip install nltk   # 불용어 제거를 위한 패키지\n",
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git   # 띄어쓰기 바로 잡기 위한 패키지\n",
        "!pip install git+https://github.com/ssut/py-hanspell.git   # 오타 및 맞춤법을 수정해주는 패키지"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqMAXtbhcnT2"
      },
      "source": [
        "# 라이브러리 호출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QuUYltjcnT6"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 호출\n",
        "import pandas as pd\n",
        "import re\n",
        "import hanja\n",
        "from hanspell import spell_checker\n",
        "from nltk.corpus import stopwords\n",
        "from pykospacing import Spacing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPMY2Hu8cnT8"
      },
      "source": [
        "# 불용어 목록 파일 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrfTAhDhcnT9"
      },
      "outputs": [],
      "source": [
        "# NLTK 불용어 다운로드\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# stopwords 폴더 안에 korean 불용어 파일 만들고 노션에 있는거 복붙해서 넣기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToexR9wWcnT-"
      },
      "source": [
        "# 코랩 사용할 경우 파일 옮기는 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPN8LmlrcnUE"
      },
      "outputs": [],
      "source": [
        "# 주피터가 아니라 코랩 쓰는 경우에만 참고\n",
        "#지연) 구글 코랩으로 작성해서 직접 생성한 korean파일을 옮기는 코드가 필요했음.. 주피터로 하면 필요없을듯!\n",
        "# import shutil\n",
        "\n",
        "# # 파일 경로\n",
        "# src_path = '/content/korean'\n",
        "# dest_path = '/root/nltk_data/corpora/stopwords'\n",
        "\n",
        "# # 파일 이동\n",
        "# shutil.move(src_path, dest_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 정규표현식 테스트 .."
      ],
      "metadata": {
        "id": "fnXKcG6yyHJp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvDnrejqyBa4"
      },
      "outputs": [],
      "source": [
        "# # 정규표현식 테스트\n",
        "# patterns = [\n",
        "#     r'<.*?>|\\[.*?\\]|【.*?】|\\{.*?\\}',\n",
        "#     r'[^가-힣0-9\\s◆◇▲▼]'\n",
        "# ]\n",
        "\n",
        "# text = '안녕하세요! [안녕하세요] (안녕하세요★☆)'\n",
        "# for pattern in patterns:\n",
        "#     text = re.sub(pattern, '', text)\n",
        "# print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9Eq9ya9yBa5"
      },
      "outputs": [],
      "source": [
        "# text1 = '안녕하세요.이것은 테스트 문장입니다...그렇습니다.'\n",
        "# text2 = '안녕하세요.. 이것은 테스트 문장입니다. 그렇습니다.'\n",
        "\n",
        "# def split_text(article):   # split하는 함수 정의\n",
        "#     sentences = re.split(r'(?<!\\.)\\.(?!\\.)', article)  # 마침표를 기준으로 문장 나누기, 단 마침표가 2개 이상 연달아 있는 경우 제외\n",
        "#     sentences = [sentence.strip() for sentence in sentences if sentence.strip()]   # 공백 제거 및 빈 문장 필터링\n",
        "#     return sentences\n",
        "\n",
        "# print(split_text(text1))\n",
        "# print('=====')\n",
        "# print(split_text(text2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F46yVzccnUH"
      },
      "source": [
        "# 전처리 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbHR08aYcnUJ"
      },
      "outputs": [],
      "source": [
        "# 데이터 클렌징 함수 정의\n",
        "def clean_text(text):\n",
        "\n",
        "    # 0. 한자를 한글로 변환\n",
        "    # text = re.sub(r'\\([^()]*\\)', '', text)  # 괄호 안의 한자는 삭제\n",
        "    text = hanja.translate(text, 'substitution')  # 그 외 한자는 한글로 변환\n",
        "\n",
        "    # 1. 첫 번째 문장에서 `◆...◆` 패턴 제거\n",
        "    sentences = text.split('. ')  # 마침표와 공백을 기준으로 문장을 나누기\n",
        "    if sentences:  # 문장이 존재할 경우\n",
        "        sentences[0] = re.sub(r'◆.*?◆', '', sentences[0])  # 첫 번째 문장에서 패턴 제거\n",
        "    text = '. '.join(sentences)  # 문장들을 다시 결합\n",
        "\n",
        "    # 2. 제거할 단어 및 패턴 목록\n",
        "    # 기사 전체 제거, 패턴 제거 순으로 함수 실행하기\n",
        "    patterns = [\n",
        "        r'[^.]*※[^.]*\\.',            # '※'를 포함하는 문장 전체를 제거\n",
        "        r'[^.]*\\[사설\\][^.]*\\.',      # '[사설]'을 포함하는 문장 전체를 제거\n",
        "        r'[^.]*마켓PRO[^.]*\\.',       # '마켓PRO'를 포함하는 문장 전체를 제거\n",
        "        r'[^.]*한경유레카[^.]*\\.',   # '한경유레카'를 포함하는 문장 전체를 제거\n",
        "        r'[^.]*이 기사는[^.]*\\.',    # '이 기사는'을 포함하는 문장 전체를 제거\n",
        "        r'[^.]*@[^.]*\\.',            # '@'를 포함하는 문장 전체를 제거 / 이메일을 제거하기 위함\n",
        "        r'[^.]*#장면[^.]*\\.',          # '#장면'을 포함하는 문장 전체를 제거\n",
        "        r'[^.]*\\[기자\\][^.]*\\.',      # '[기자]'를 포함하는 문장 전체를 제거\n",
        "        r'[^.]*\\[저작권\\][^.]*\\.',    # '[저작권]'을 포함하는 문장 전체를 제거\n",
        "        r'[^.]*\\[퀴즈\\][^.]*\\.',       # '[퀴즈]'를 포함하는 문장 전체를 제거\n",
        "        r'[^.]*<용 어>[^.]*\\.',\n",
        "        r'지금은 임원시대',            # '지금은 임원시대' 라는 문자열만 제거\n",
        "        r'<.*?>|\\[.*?\\]|【.*?】|\\{.*?\\}', # ()제외, 온갖 종류의 괄호와 그 안의 내용 전부 제거\n",
        "        r'레이더M 기사 더보기>>>',\n",
        "        r'미래를 바꿀 혁신기술 12가지 - ',\n",
        "        r'MBA도 모바일로 공부한다.',\n",
        "        r'매경이 전하는 세상의 지식\\(매-세-지',\n",
        "        r'▶ 네이버 뉴스스탠드에서 매일경제를 MY뉴스로 구독하세요',\n",
        "        r'▶뉴스 이상의 무궁무진한 프리미엄 읽을거리',\n",
        "        r'▶아나운서가 직접 읽어주는 오늘의 주요 뉴스',\n",
        "        r'▶매경 뉴스레터 \\'매콤달콤\\'을 지금 구독하세요'\n",
        "        r'▶\\'지킬앤하이드\\' 조승우 공연 티켓 경품 이벤트',\n",
        "        r'▶네이버 메인에서 \\'매일경제\\'를 받아보세요',\n",
        "        r'▶뉴스레터 \\'매콤달콤\\' 구독',\n",
        "        r'▶무궁무진한 프리미엄 읽을거리',\n",
        "        r'▶네이버에서 \\'매일경제\\' 뉴스 구독하고 경품 받아가세요',\n",
        "        r'▶매경이 에어팟프로 쏩니다! \\'M코인\\'',\n",
        "        r'▶기사공유하고 코인적립하세요 \\'M코인\\'',\n",
        "        r'▶\\'M코인\\' 지금 가입하면 5000코인 드려요',\n",
        "        r'▶\\'매일경제\\' 바로가기',\n",
        "        r'▶매부리TV 구독하고 에어팟프로 득템!!',\n",
        "        r'▶제21회 세계지식포럼 : 팬데노믹스',\n",
        "        r'▶ 팬데믹위기 해법 찾는다! - 제21회 세계지식포럼',\n",
        "        r'▶ 궁금한 제조과정 영상으로 보세요. \\'이렇게 만들죠\\'',\n",
        "        r'▶ 데일리 뉴스 브리핑 \\'매경이 전하는 세상의 지식\\'',\n",
        "        r'▶ 매일매일 색다른 뉴스레터 \\'매콤달콤\\' 구독하세요',\n",
        "        r'▶ 아파트 살까 청약할까. 여기서 확인하세요. \\'매부리tv\\'',\n",
        "        r'▶ \\'경제 1위\\' 매일경제, 네이버에서 구독하세요'\n",
        "        r'▶ 이 제품은 \\'이렇게 만들죠\\' 영상으로 만나요',\n",
        "        r'▶ 부동산의 모든것 \\'매부리TV\\'가 펼칩니다',\n",
        "        r'▶ \\'경제 1위\\' 매일경제, 앱으로 편하게 보세요',\n",
        "        r'▶ 매일경제 지식레터 \\'매콤달콤\\' 받아보세요',\n",
        "        r'▶ 매경이 알려주는 \\'취업비법\\' 한달간 무료'\n",
        "    ]\n",
        "\n",
        "    # 단어 및 패턴을 제거하는 함수\n",
        "    def remove_patterns(text):\n",
        "        for pattern in patterns:\n",
        "            text = re.sub(pattern, '', text)\n",
        "        return text\n",
        "\n",
        "    # 패턴 제거\n",
        "    # text = remove_article(text)\n",
        "    text = remove_patterns(text)\n",
        "\n",
        "    # 3. 문장 단위로 split하여 리스트에 저장\n",
        "    def split_text(article):   # split하는 함수 정의\n",
        "        sentences = re.split(r'(?<!\\.)\\.(?!\\.)', article)  # 마침표를 기준으로 문장 나누기, 단 마침표가 2개 이상 연달아 있는 경우 제외\n",
        "        sentences = [sentence.strip() for sentence in sentences if sentence.strip()]   # 공백 제거 및 빈 문장 필터링\n",
        "        return sentences\n",
        "\n",
        "    # 각 문장들을 요소로 갖는 리스트인 text_list\n",
        "    text_list = split_text(text)\n",
        "\n",
        "    # 4. 특수 문자 제거 (◆, ◇, ▲, ▼ 제외)\n",
        "    text_list = [re.sub(r'[^가-힣0-9\\s◆◇▲▼~,.\\'\"%]', '', s) for s in text_list]  # 한글과 숫자, 공백, 그리고 ◆, ◇, ▲, ▼, ~, ', \", . , , % 만 남김\n",
        "\n",
        "    # 5. 띄어쓰기 교정\n",
        "    spacing = Spacing()\n",
        "    text_list = [spacing(s) for s in text_list]\n",
        "\n",
        "    # # 6. 오타 수정\n",
        "    # def correct_text(sentence):\n",
        "    #     result = spell_checker.check(sentence)\n",
        "    #     return result.checked   # 수정된 문장 반환\n",
        "    # # 직접 spell_checker 라이브러리 코드 열어서 payload의 key값에 passportKey와 _callback 추가, value는 네이버 맞춤법 검사기 개발자 도구 참고하기\n",
        "    # # data 변수도 바꿔줘야함 -> 자세한건 노션에 서술\n",
        "\n",
        "    # text_list = [correct_text(s) for s in text_list]\n",
        "\n",
        "    #아래 오류가 났을 때의 코드 -> API 응답이 뜨지 않고 오류 발생이라고 뜨면 라이브러리 코드를 수정 안했거나 잘못한 것\n",
        "    '''\n",
        "    def correct_typos(sentence):\n",
        "        if not isinstance(sentence, str):\n",
        "            print(\"입력값이 문자열이 아닙니다.\")\n",
        "            return sentence  # 문자열이 아닐 경우 원본 반환\n",
        "        try:\n",
        "            result = spell_checker.check(sentence)\n",
        "            if not result.result:\n",
        "                print(f\"수정 실패: {result.original}\")\n",
        "                return sentence  # 수정 실패 시 원본 문장 반환\n",
        "            return result.checked  # 수정된 문장 반환\n",
        "        except Exception as e:\n",
        "            print(f\"오류 발생: {e}\")\n",
        "            return sentence  # 오류 발생 시 원본 문장 반환\n",
        "\n",
        "    corrected_text_list = []\n",
        "    for sentence in text_list:\n",
        "        try:\n",
        "            result = spell_checker.check(sentence)\n",
        "            print(f\"API 응답: {result}\")  # API 응답 확인\n",
        "            corrected_text_list.append(result.checked)\n",
        "        except Exception as e:\n",
        "            print(f\"오류 발생: {e}\")\n",
        "            corrected_text_list.append(sentence)  # 오류 발생 시 원본 문장 반환\n",
        "\n",
        "    for original, corrected in zip(text_list, corrected_text_list):\n",
        "        print(f\"원본: {original}\\n수정: {corrected}\\n\")\n",
        "    '''\n",
        "\n",
        "    # 7. 불용어 제거\n",
        "    stop_words = set(stopwords.words('korean'))   # korean 불용어 파일 만들어져 있어야 함\n",
        "    def remove_stopwords(text):   # 불용어 제거 함수 정의\n",
        "        return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    text_list = [remove_stopwords(s) + '.' for s in text_list] # 문장 마지막에 다시 온점 붙여서 반환\n",
        "\n",
        "    # 8. 분리된 문장들 다시 하나의 텍스트로 합치기\n",
        "    text = ' '.join(text_list)   # 다시 하나의 문자열로 합쳐진걸 원하면 return text 하면 됨\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSImGnuLcnUN"
      },
      "source": [
        "# 특정 연도 csv 파일 하나만 돌릴 때 아래 코드 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEnAfYsbcnUO"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# CSV 파일을 데이터프레임으로 불러오기\n",
        "start = time.time()\n",
        "df = pd.read_csv('2005_매일경제.csv')\n",
        "\n",
        "# 데이터 정제 ) null값과 중복 제거\n",
        "df.dropna(subset=['body'], inplace=True)\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# 패턴 목록 정의\n",
        "patterns = [\n",
        "    r'삼성서울병원',\n",
        "    r'삼성 서울병원',\n",
        "    r'삼성안내견센터',\n",
        "    r'삼성홀',\n",
        "    r'삼성전자홀',\n",
        "    r'스타크래프트',\n",
        "    r'프로리그',\n",
        "    r'라이온즈',\n",
        "    r'매경TEST',\n",
        "    r'<표>',\n",
        "    r'\\[표\\]',\n",
        "    r'\\[포토\\]',\n",
        "    r'\\[MK 추천매물\\]'\n",
        "]\n",
        "\n",
        "# 패턴이 포함된 행을 삭제하기 위한 조건식 작성\n",
        "condition1 = df['title'].apply(lambda text: any(re.search(pattern, text) for pattern in patterns))\n",
        "df = df[~condition1]\n",
        "condition2 = df['body'].apply(lambda text: any(re.search(pattern, text) for pattern in patterns))\n",
        "df = df[~condition2]\n",
        "\n",
        "# 'title'과 'body' 컬럼에 클렌징 함수 적용\n",
        "df['cleaned_title'] = df['title'].apply(clean_text)\n",
        "df['cleaned_body'] = df['body'].apply(clean_text)\n",
        "\n",
        "# 결과 확인\n",
        "print(df[['body', 'cleaned_body']].head())\n",
        "\n",
        "# 클렌징된 데이터프레임 저장 (원하는 파일명으로 변경)\n",
        "df.to_csv('cleaned_2005_매일경제.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VbLaLfqcnUQ"
      },
      "source": [
        "# 여러 연도 csv 파일 반복문으로 돌릴 때 아래 코드 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-_75UBPcnUS"
      },
      "outputs": [],
      "source": [
        "# 파일 경로 및 범위 설정\n",
        "import os\n",
        "\n",
        "start_year = 2005\n",
        "end_year = 2024\n",
        "\n",
        "# 모든 연도의 CSV 파일을 처리\n",
        "for year in range(start_year, end_year + 1):\n",
        "    filename = f'/{year}_매일경제.csv'\n",
        "\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"Processing file: {filename}\")\n",
        "\n",
        "        # CSV 파일을 데이터프레임으로 불러오기\n",
        "        df = pd.read_csv(filename)\n",
        "\n",
        "        df.dropna(subset=['body'], inplace=True)\n",
        "        df.drop_duplicates(inplace=True)\n",
        "\n",
        "        # 패턴 목록 정의\n",
        "        patterns = [\n",
        "            r'삼성서울병원',\n",
        "            r'삼성 서울병원',\n",
        "            r'삼성안내견센터',\n",
        "            r'삼성홀',\n",
        "            r'삼성전자홀',\n",
        "            r'스타크래프트',\n",
        "            r'프로리그',\n",
        "            r'라이온즈',\n",
        "            r'매경TEST',\n",
        "            r'<표>',\n",
        "            r'\\[표\\]',\n",
        "            r'\\[포토\\]',\n",
        "            r'\\[MK 추천매물\\]'\n",
        "        ]\n",
        "\n",
        "        # 패턴이 포함된 행을 삭제하기 위한 조건식 작성\n",
        "        condition1 = df['title'].apply(lambda text: any(re.search(pattern, text) for pattern in patterns))\n",
        "        df = df[~condition1]\n",
        "        condition2 = df['body'].apply(lambda text: any(re.search(pattern, text) for pattern in patterns))\n",
        "        df = df[~condition2]\n",
        "\n",
        "        # 'body'와 'title' 컬럼에 클렌징 함수 적용\n",
        "        df['cleaned_title'] = df['title'].apply(clean_text)\n",
        "        df['cleaned_body'] = df['body'].apply(clean_text)\n",
        "\n",
        "        # 클렌징된 데이터프레임 저장\n",
        "        output_filename = f'cleaned_{year}_매일경제.csv'  # 파일명이 한국경제로 되어있길래 수정함\n",
        "        df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
        "\n",
        "        print(f\"Saved cleaned file: {output_filename}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"File not found: {filename}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}